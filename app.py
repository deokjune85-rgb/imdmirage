# ======================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.3 â€” Dual RAG + Relay Mechanism (Omega-Infinitum Core)
# ======================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time # ì†ë„ ì¡°ì ˆì„ ìœ„í•´ í•„ìš”í•¨

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol) ---
# í…Œë§ˆ ì„¤ì •: ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ ì‚¬ìš© (í° ë°”íƒ•/ê²€ì€ ê¸€ì”¨ ë˜ëŠ” ë‹¤í¬ ëª¨ë“œ ìë™ í˜¸í™˜)
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.3", page_icon="ğŸ›¡ï¸", layout="centered")

# CSS í•´í‚¹ (ì‹ ê¸°ë£¨ í”„ë¡œí† ì½œ) - [â˜…ìˆ˜ì •ë¨: ìƒ‰ìƒ ê°•ì œ ì œê±° ë° ìµœì í™”]
custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}

/* --- ê¸€ì ìŠ¤íƒ€ì¼ í†µì¼ (ìƒ‰ìƒ ê°•ì œ ì œê±°) --- */
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    /* color: #FFFFFF !important; <-- ì´ì „ ì½”ë“œì˜ í° ê¸€ì”¨ ë¬¸ì œ ì›ì¸ ì œê±°. */
    font-size: 16px !important;
    line-height: 1.7 !important;
}

/* --- íƒ€ì´í‹€ ìœ„ì¹˜ ì¡°ì • --- */
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
}

strong, b {
    font-weight: 700;
}

/* --- ë¶€ë“œëŸ¬ìš´ í…ìŠ¤íŠ¸ ë“±ì¥ (0.5s) --- */
.fadein {
    animation: fadeInText 0.5s ease-in-out forwards;
    opacity: 0;
}
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}

[data-testid="stChatMessageContent"] {
    font-size: 16px !important;
}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)


# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.3")
# ë¼ì´íŠ¸ ëª¨ë“œ ê°€ë…ì„±ì„ ìœ„í•´ warning ì‚¬ìš©
st.warning("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# --- 3. API í‚¤ ë° RAG ì—”ì§„ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
         raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
    st.stop()

# --- [RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜] (ê¸°ì¡´ ë‚´ìš© ìœ ì§€) ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text, task_type="retrieval_document"):
    try:
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text: return None
        result = genai.embed_content(model=EMBEDDING_MODEL_NAME, content=clean_text, task_type=task_type)
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}"); return None

@st.cache_data
def load_and_embed_data(file_path, separator_regex):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}"); return [], []
    try:
        with open(file_path, 'r', encoding='utf-8') as f: content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}"); return [], []
    if not content.strip(): return [], []
    chunks = re.split(separator_regex, content)
    data_items = [p.strip() for p in chunks if p and p.strip()]
    embeddings, valid_items = [], []
    for item in data_items:
        ebd = embed_text(item, task_type="retrieval_document")
        if ebd:
            embeddings.append(ebd); valid_items.append(item)
    print(f"[RAG] Loaded {len(valid_items)} items from {file_path}.")
    return valid_items, embeddings

def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.50):
    if not embeddings or not items: return []
    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None: return []
    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]
    results = []
    for i in idxs:
        if float(sims[i]) >= threshold:
            results.append({"similarity": float(sims[i]), "raw_text": items[i]})
    return results

def _parse_precedent_block(text: str) -> dict:
    # (ê¸°ì¡´ íŒŒì‹± í•¨ìˆ˜ ë‚´ìš© ìœ ì§€ - ìƒëµ)
    t = text.strip()
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    title = lines[0][:120] if lines else "ì œëª© ì—†ìŒ"
    m = re.search(r'\[(?P<court>[^ \[\]]+)\s+(?P<date>\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)\s*ì„ ê³ \s*(?P<caseno>\d{4}\s*[ê°€-í£]{1,2}\s*\d{3,6})\s*íŒê²°\]', t)
    court, date, caseno = (m.group('court'), m.group('date'), m.group('caseno').replace(" ", "")) if m else ("", "", "")
    if not caseno:
        m2 = re.search(r'(?P<caseno>\d{4}\s*[ê°€-í£]{1,2}\s*\d{3,6})', t)
        if m2: caseno = m2.group('caseno').replace(" ", "")
    holding = ""
    m2 = re.search(r'ã€íŒê²°ìš”ì§€ã€‘(.*?)(ã€|$)', t, re.S)
    if m2: holding = re.sub(r'\s+', ' ', m2.group(1)).strip()
    else:
        m3 = re.search(r'ã€íŒì‹œì‚¬í•­ã€‘(.*?)(ã€|$)', t, re.S)
        if m3: holding = re.sub(r'\s+', ' ', m3.group(1)).strip()
    if not holding: holding = re.sub(r'\s+', ' ', t)[:160].strip()
    excerpt = ""
    for key in ["ã€ì „ë¬¸ã€‘", "ã€ì´ ìœ ã€‘", "ã€ì´ìœ ã€‘", "ã€ë³¸ë¬¸ã€‘"]:
        pos = t.find(key)
        if pos != -1:
            excerpt = re.sub(r'\s+', ' ', t[pos:pos+300]).strip(); break
    if not excerpt: excerpt = re.sub(r'\s+', ' ', t)[:300].strip()
    if len(holding) > 130: holding = holding[:130].rstrip() + "â€¦"
    if len(excerpt) > 160: excerpt = excerpt[:160].rstrip() + "â€¦"
    return {"title": title, "court": court, "date": date, "case_no": caseno, "holding": holding, "excerpt": excerpt}


# (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ìœ ì§€)
def _is_menu_input(s: str) -> bool:
    if not s: return False
    return bool(re.fullmatch(r'\d+|[1-9]-\d+', s.strip()))

def _is_final_report(txt: str) -> bool:
    if not txt: return False
    # HTML íƒœê·¸ ì œê±° í›„ ë¶„ì„ (ë¦´ë ˆì´ ë©”ì»¤ë‹ˆì¦˜ ëŒ€ì‘ ê°•í™”)
    t = re.sub('<[^<]+?>', '', txt).replace(" ", "")
    hits = 0
    # ë³´ê³ ì„œ ì‹ë³„ í‚¤ì›Œë“œ ê°•í™”
    for key in ["ì „ëµë¸Œë¦¬í•‘ë³´ê³ ì„œ", "ë¦¬ìŠ¤í¬ì‹œë®¬ë ˆì´ì…˜ë¶„ì„", "ê¶Œì¥ë‹¤ìŒë‹¨ê³„", "ë©´ì±…ì¡°í•­", "ì‹œë®¬ë ˆì´ì…˜ì™„ë£Œ"]:
        if key in t: hits += 1
    return (hits >= 2) and (len(t) > 500)

def _query_title(prompt_text: str) -> str:
    if not prompt_text: return ""
    m = re.search(r'\[([^\]]+)\]', prompt_text)
    if m: return m.group(1).strip()
    first = prompt_text.strip().splitlines()[0].strip()
    return (first[:77] + "â€¦") if len(first) > 80 else first

# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì (Prime Genome) ë¡œë“œ ë° ì´ˆê¸°í™” ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if not SYSTEM_INSTRUCTION.strip():
        raise ValueError("System prompt file is empty.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì½”ì–´(system_prompt.txt) ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()


if "model" not in st.session_state:
    try:
        # ëª¨ë¸ëª… í™•ì¸: 'models/gemini-2.5-flash'
        st.session_state.model = genai.GenerativeModel("models/gemini-1.5-flash-latest",
                                                    system_instruction=SYSTEM_INSTRUCTION)
        
        # ë“€ì–¼ RAG ì´ˆê¸°í™”
        with st.spinner("ë¶„ì„ ì—”ì§„(Dual RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            # 1. íŒë¡€ ë°ì´í„° ë¡œë“œ (P-RAG)
            p_data, p_emb = load_and_embed_data('precedents_data.txt', r'\s*---END OF PRECEDENT---\s*')
            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb

            # 2. ë²•ë ¹ ë°ì´í„° ë¡œë“œ (S-RAG)
            s_data, s_emb = load_and_embed_data('statutes_data.txt', r'\s*---END OF STATUTE---\s*')
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

# --- 5. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state or not st.session_state.messages:
    if "model" in st.session_state:
        try:
            if "chat" not in st.session_state:
                st.session_state.chat = st.session_state.model.start_chat(history=[])

            if not st.session_state.messages:
                initial_prompt = "ê¸´ê¸‰ ëª…ë ¹: EPE í™œì„±í™”. ì¦‰ì‹œ <KnowledgeBase>ì˜ 'Phase 0: ë„ë©”ì¸ ì„ íƒ í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ê³  ë©”ë‰´ë¥¼ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í™•ì¸ì€ ìƒëµí•œë‹¤."
                response = st.session_state.chat.send_message(initial_prompt)
                if response and response.text:
                     st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{response.text}</div>"})
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (API í†µì‹  ì˜¤ë¥˜): {e}")

# --- 6. ëŒ€í™” ì¶œë ¥ ---
for message in st.session_state.messages:
    role = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role, avatar=avatar):
        st.markdown(message['content'], unsafe_allow_html=True)

# --- 7. ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…í•µì‹¬ ìˆ˜ì •: ë¦´ë ˆì´ ë©”ì»¤ë‹ˆì¦˜ íƒ‘ì¬â˜…) ---

# [â˜…ì‹ ì„¤â˜…] ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ë° ì €ì¥ í•¨ìˆ˜ (ì†ë„ ì œì–´ í¬í•¨)
def stream_and_store_response(chat_session, prompt_to_send, spinner_text="Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
    full_response = ""
    with st.spinner(spinner_text):
        try:
            response_stream = chat_session.send_message(prompt_to_send, stream=True)
            
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                placeholder = st.empty()
                word_buffer = ""
                try:
                    for chunk in response_stream:
                        if getattr(chunk, "text", None):
                            word_buffer += chunk.text
                            # ìŠ¤ë¬´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° + ì†ë„ ì œì–´ (0.01ì´ˆ ì§€ì—°ìœ¼ë¡œ ë©€ë¯¸ ë°©ì§€)
                            if re.search(r'[\s.,!?\n]', chunk.text):
                                full_response += word_buffer
                                word_buffer = ""
                                time.sleep(0.01) 
                                placeholder.markdown(
                                    f"<div class='fadein'>{full_response}â–Œ</div>",
                                    unsafe_allow_html=True
                                )
                except Exception as stream_error:
                     # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ë‹¨ ì‹œ (ê³¼ë¶€í•˜/íƒ€ì„ì•„ì›ƒ) ì˜¤ë¥˜ í‘œì‹œ
                     full_response += f"\n\n[âš ï¸ ì‹œìŠ¤í…œ ê³¼ë¶€í•˜ ê°ì§€: ì‘ë‹µ ìƒì„± ì¤‘ë‹¨ë¨. {stream_error} âš ï¸]"

                if word_buffer:
                    full_response += word_buffer
                
                placeholder.markdown(
                    f"<div class='fadein'>{full_response}</div>",
                    unsafe_allow_html=True
                )
            
            # ë©”ì‹œì§€ ì €ì¥ (HTML í¬í•¨)
            st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{full_response}</div>"})
            return full_response

        except Exception as e:
            err = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ (API í˜¸ì¶œ ì‹¤íŒ¨): {e}"
            st.error(err)
            st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{err}</div>"})
            return err

# ë©”ì¸ ì…ë ¥ ë£¨í”„
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": f"<div class='fadein'>{prompt}</div>"})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(f"<div class='fadein'>{prompt}</div>", unsafe_allow_html=True)

    # [â˜…í•µì‹¬ ìˆ˜ì • 1: Phase 2 ê°ì§€â˜…]
    is_phase2_data = False
    if st.session_state.messages:
        # ë§ˆì§€ë§‰ Architect ë©”ì‹œì§€ ì°¾ê¸° (HTML ì œê±° í›„ ë¶„ì„)
        last_architect_msg = ""
        for msg in reversed(st.session_state.messages):
            if msg['role'] == 'Architect':
                # HTML íƒœê·¸ ì œê±° í›„ ë¶„ì„
                last_architect_msg = re.sub('<[^<]+?>', '', msg['content'])
                break
        
        # ì´ì „ ë©”ì‹œì§€ê°€ Phase 2 ë°ì´í„° ìš”ì²­ì´ì—ˆëŠ”ì§€ í™•ì¸ (í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€)
        # (Phase 2 ìš”ì²­ ë¬¸êµ¬ëŠ” system_prompt.txtì— ì •ì˜ëœ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨)
        if "Phase 2:" in last_architect_msg and ("ë°ì´í„°ë¥¼ ì§€ê¸ˆ ì‹œìŠ¤í…œì— ì…ë ¥í•˜ì‹­ì‹œì˜¤" in last_architect_msg or "ì—”ì§„'ì„ ê°€ë™í•˜ì—¬" in last_architect_msg):
            is_phase2_data = True

    # [â˜…í•µì‹¬ ìˆ˜ì • 2: ë“€ì–¼ RAG ì‹¤í–‰â˜…]
    rag_context = ""
    similar_precedents = []
    
    if not _is_menu_input(prompt):
         with st.spinner("ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... (Dual RAG: íŒë¡€/ë²•ë ¹)..."):
            # 1. ë²•ë ¹ ê²€ìƒ‰ (S-RAG)
            if ("statutes" in st.session_state and st.session_state.statutes):
                similar_statutes = find_similar_items(prompt, st.session_state.statutes, st.session_state.s_embeddings, top_k=3, threshold=0.60)
                if similar_statutes:
                    s_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c['raw_text']}\n---\n" for c in similar_statutes]
                    rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n" + "\n".join(s_texts)

            # 2. íŒë¡€ ê²€ìƒ‰ (P-RAG)
            if ("precedents" in st.session_state and st.session_state.precedents):
                similar_precedents = find_similar_items(prompt, st.session_state.precedents, st.session_state.p_embeddings, top_k=5, threshold=0.50)
                if similar_precedents:
                    p_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c['raw_text']}\n---\n" for c in similar_precedents]
                    rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" + "\n".join(p_texts)

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    final_prompt = f"{prompt}\n{rag_context}"

    # ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± (Phase 1 ë˜ëŠ” Phase 2 ë¶„ì„)
    current_response = stream_and_store_response(st.session_state.chat, final_prompt)

    # [â˜…í•µì‹¬ ìˆ˜ì • 3: ë¦´ë ˆì´ ë©”ì»¤ë‹ˆì¦˜â˜…]
    # Phase 2 ë°ì´í„°ê°€ ì…ë ¥ë˜ì—ˆê³ , ë°©ê¸ˆ ìƒì„±ëœ ì‘ë‹µì´ ìµœì¢… ë³´ê³ ì„œê°€ ì•„ë‹ˆë¼ë©´ (ì¦‰, ë¶„ì„ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ê³  ë©ˆì·„ë‹¤ë©´)
    
    # ì‘ë‹µ í´ë¦°ì§• (HTML ì œê±°)
    clean_response = re.sub('<[^<]+?>', '', current_response)
    
    if is_phase2_data and not _is_final_report(clean_response):
        # ê°•ì œë¡œ Phase 3 ì‹¤í–‰ ëª…ë ¹ (Relay Prompt)
        # ì‹œìŠ¤í…œì—ê²Œ ëª…í™•í•˜ê²Œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§€ì‹œí•œë‹¤.
        relay_prompt = "[ì‹œìŠ¤í…œ ëª…ë ¹]: Phase 2 ë¶„ì„ ê²°ê³¼ í™•ì¸ ì™„ë£Œ. ì¦‰ì‹œ ì´ì–´ì„œ Phase 3(ìµœì¢… ë³´ê³ ì„œ ìƒì„±)ë¥¼ ì‹¤í–‰í•˜ë¼. ë°©ê¸ˆ ë¶„ì„í•œ ë‚´ìš©ê³¼ RAG ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ì „ì²´ë¥¼ ì™„ì„±í•˜ë¼. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í™•ì¸ì€ ìƒëµí•œë‹¤."
        
        # ë¦´ë ˆì´ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (Phase 3 ë³´ê³ ì„œ ìƒì„±)
        current_response = stream_and_store_response(st.session_state.chat, relay_prompt, spinner_text="ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘ (Phase 3 Relay)...")
        # ë¦´ë ˆì´ í›„ ì‘ë‹µ ë‹¤ì‹œ í´ë¦°ì§•
        clean_response = re.sub('<[^<]+?>', '', current_response)


    # [â˜…í•µì‹¬ ìˆ˜ì • 4: íŒë¡€ ì‹œê°í™”â˜…]
    # ìµœì¢… ì‘ë‹µ(ë¦´ë ˆì´ í¬í•¨)ì´ ë³´ê³ ì„œì´ê³ , P-RAG ê²°ê³¼ê°€ ìˆì„ ê²½ìš° í‘œì‹œ
    if _is_final_report(clean_response) and similar_precedents:
        q_title = _query_title(prompt)
        st.markdown("**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n* ê²€ìƒ‰ ì¿¼ë¦¬: `[" + q_title + "]`\n")

        for case_data in similar_precedents[:3]:
            case = _parse_precedent_block(case_data['raw_text'])
            sim_pct = int(round(case_data["similarity"] * 100))
            
            label = f"íŒë¡€ [{case.get('title','ì œëª© ì—†ìŒ')}]"
            if case.get("court") and case.get("case_no"):
                label += f" â€” {case['court']} {case['case_no']}"

            item_md = (
                f"* {label}  \n"
                f"  - ì„ ê³ : {case.get('date','').strip()} {case.get('court','').strip()} | ìœ ì‚¬ë„: {sim_pct}%  \n"
                f"  - íŒê²°ìš”ì§€: {case.get('holding','').strip()}  \n"
                f"  - ì „ë¬¸ ì¼ë¶€: \"{case.get('excerpt','').strip()}\""
            )
            st.markdown(item_md)
