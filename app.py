# ======================================================
# ğŸ›¡ï¸ Veritas Engine v8.5 â€” Gradient Edition (Gemini Style)
# ======================================================
import streamlit as st
import google.generativeai as genai
import requests, re, numpy as np, time

# ======================================================
# 1. SYSTEM CONFIG
# ======================================================
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ v8.5", page_icon="ğŸ›¡ï¸", layout="centered")

# âœ… ìŠ¤íƒ€ì¼ í†µí•© â€” í°ìƒ‰ í…ìŠ¤íŠ¸ + ê·¸ë¼ë°ì´ì…˜ í—¤ë” + ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
st.markdown("""
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}

html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    font-size: 16px !important;
    line-height: 1.7 !important;
    color: #FFFFFF !important;
}

[data-testid="stChatMessage"], [data-testid="stChatMessageContent"] {
    background-color: inherit !important;
    border: none !important;
}

/* âœ… ë¶€ë“œëŸ¬ìš´ í…ìŠ¤íŠ¸ í‘œì‹œ */
.lineblock {
    white-space: pre-wrap;
    line-height: 1.7;
    margin-bottom: 4px;
    color: #FFFFFF;
    font-size: 16px;
    opacity: 0;
    animation: fadeIn 0.8s forwards ease-in-out;
}
@keyframes fadeIn {
    from {opacity: 0;}
    to {opacity: 1;}
}

/* âœ… ë¦¬ìŠ¤íŠ¸ ê°„ê²© í†µì œ */
.option-list div {
    margin-bottom: 2px !important;
    line-height: 1.7 !important;
}

/* âœ… ê·¸ë¼ë°ì´ì…˜ ì œëª© íš¨ê³¼ (Gemini ëŠë‚Œ) */
.gradient-title {
    font-size: 22px;
    font-weight: 800;
    background: linear-gradient(90deg, #7EE8FA, #EEC0C6, #A993FF);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}
</style>
""", unsafe_allow_html=True)

# âœ… ìë™ ìŠ¤í¬ë¡¤
st.markdown("""
<script>
setInterval(() => {
  var chat = window.parent.document.querySelector('[data-testid="stVerticalBlock"]');
  if (chat) chat.scrollTo(0, chat.scrollHeight);
}, 400);
</script>
""", unsafe_allow_html=True)

st.markdown("<div class='gradient-title'>ğŸ›¡ï¸ Veritas Engine v8.5 â€” Gemini Smooth Render Edition</div>", unsafe_allow_html=True)
st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ======================================================
# 2. API CONFIG
# ======================================================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì‹œìŠ¤í…œ ì˜¤ë¥˜: 'GOOGLE_API_KEY' ëˆ„ë½. [Secrets] íƒ­ í™•ì¸ í•„ìš”.")
    st.stop()

genai.configure(api_key=API_KEY)
OC_KEY = st.secrets.get("LAW_API_KEY", "DEOKJUNE")

# ======================================================
# 3. íƒ„ì•½ê³  ë¡œë“œ & ì„ë² ë”©
# ======================================================
EMBED_MODEL = "models/text-embedding-004"

def embed_text(text, task_type="RETRIEVAL_DOCUMENT"):
    try:
        res = genai.embed_content(model=EMBED_MODEL, content=text, task_type=task_type)
        return np.array(res["embedding"], dtype=float)
    except:
        return None

@st.cache_data(show_spinner=False)
def load_and_embed_precedents(file_path):
    try:
        r = requests.get(file_path, timeout=10)
        content = r.text
    except:
        return [], np.array([])
    items = [p.strip() for p in content.split("---END OF PRECEDENT---") if p.strip()]
    emb_list, valid = [], []
    for p in items:
        emb = embed_text(p)
        if emb is not None:
            emb_list.append(emb)
            valid.append(p)
    return valid, np.vstack(emb_list) if emb_list else np.array([])

def find_similar_precedents(query, precedents, embeddings, top_k=5):
    if embeddings.size == 0:
        return []
    q_emb = embed_text(query, task_type="RETRIEVAL_QUERY")
    if q_emb is None:
        return []
    sims = np.dot(embeddings, q_emb) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(q_emb))
    idx = np.argsort(sims)[-top_k:][::-1]
    return [{"similarity": float(sims[i]), "text": precedents[i]} for i in idx if sims[i] > 0.6]

# ======================================================
# 4. SYSTEM PROMPT
# ======================================================
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
except:
    SYSTEM_INSTRUCTION = "ë‹¹ì‹ ì€ ë²•ë¥  AI ì‹œìŠ¤í…œ 'ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„'ì…ë‹ˆë‹¤."

RAW_URL = "https://raw.githubusercontent.com/deokjune85-rgb/imdmirage/main/precedents_data.txt"
if "precedents" not in st.session_state:
    st.session_state.precedents, st.session_state.embeddings = load_and_embed_precedents(RAW_URL)

if "model" not in st.session_state:
    st.session_state.model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=SYSTEM_INSTRUCTION)

if "chat" not in st.session_state:
    st.session_state.chat = st.session_state.model.start_chat(history=[])
    st.session_state.messages = []
    try:
        init = st.session_state.chat.send_message("ì‹œìŠ¤í…œ ê°€ë™. Phase 0 ì‹œì‘.")
        st.session_state.messages.append({"role": "Architect", "content": init.text})
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ======================================================
# 5. CHAT INTERFACE
# ======================================================
for msg in st.session_state.messages:
    avatar = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(f"<div class='lineblock'>{msg['content']}</div>", unsafe_allow_html=True)

if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(f"<div class='lineblock'>{prompt}</div>", unsafe_allow_html=True)

    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
        try:
            # âœ… ì „ì²´ ë‹¨ìœ„ ë Œë”ë§ (ë¶€ë“œëŸ½ê²Œ Fade-in)
            stream = st.session_state.chat.send_message(prompt, stream=True)
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                placeholder = st.empty()
                answer = ""
                for chunk in stream:
                    answer += chunk.text
                placeholder.markdown(f"<div class='lineblock'>{answer}</div>", unsafe_allow_html=True)

            # âœ… íŒë¡€ ìë™ ì²¨ë¶€
            docs = find_similar_precedents(prompt, st.session_state.precedents, st.session_state.embeddings)
            if docs:
                report = "### ğŸ§¾ ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (ìë™)\n\n"
                report += f"* ê²€ìƒ‰ ì¿¼ë¦¬: `{prompt}`\n\n"
                for d in docs:
                    sim = d["similarity"]
                    title = d["text"].split("\n")[0][:80]
                    excerpt = " ".join(d["text"].split("\n")[1:5])[:300]
                    report += f"* íŒë¡€ [{title}](#)\n  - ìœ ì‚¬ë„: {sim*100:.0f}%\n  - ì „ë¬¸ ì¼ë¶€: \"{excerpt}...\"\n\n"
                with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                    st.markdown(f"<div class='lineblock'>{report}</div>", unsafe_allow_html=True)

            st.session_state.messages.append({"role": "Architect", "content": answer})

        except Exception as e:
            st.error(f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: {e}")
