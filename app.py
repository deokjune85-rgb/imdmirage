# ======================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.1 â€” Fine-Tune Build (ìœ¤ì§„ ì»¤ìŠ¤í…€ ì™„ì„±ë³¸)
# ======================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol) ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.1", page_icon="ğŸ›¡ï¸", layout="centered")

# CSS í•´í‚¹ (ì‹ ê¸°ë£¨ í”„ë¡œí† ì½œ)
custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}

/* --- ê¸€ì ìŠ¤íƒ€ì¼ í†µì¼ --- */
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    color: #FFFFFF !important;
    font-size: 17px !important;
    line-height: 1.7 !important;
}

/* --- íƒ€ì´í‹€ ìœ„ì¹˜ ì¡°ì • (ì—¬ë°± ìµœì†Œí™”) --- */
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
    color: #FFFFFF !important;
}

/* --- ì¤‘ìš” ë¬¸ë‹¨ / í—¤ë“œë¼ì¸ ì»¬ëŸ¬ ê°•ì¡° --- */
strong, b {
    color: #5AB0FF !important; /* ì§„íŒŒë‘ í¬ì¸íŠ¸ */
}

/* --- ë¶€ë“œëŸ¬ìš´ í…ìŠ¤íŠ¸ ë“±ì¥ (ì œë¯¸ë‚˜ì´í˜• ì‹œê° íš¨ê³¼) --- */
.fadein {
    animation: fadeInText 0.8s ease-in-out forwards;
    opacity: 0;
}
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}

/* --- íŒë¡€/ê²°ê³¼ ì¶œë ¥ ì‹œ í…ìŠ¤íŠ¸ í†µì¼ --- */
[data-testid="stChatMessageContent"] {
    font-size: 17px !important;
    color: #FFFFFF !important;
}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)


# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.1")

st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# --- 3. API í‚¤ ë° ëª¨ë¸ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. (API Key ëˆ„ë½)")
    st.stop()

genai.configure(api_key=API_KEY)

# --- [ì‘ì „ëª…: íŠ¸ë¡œì´ ëª©ë§ˆ] ê²Œë¦´ë¼ RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜ ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text, task_type="RETRIEVAL_DOCUMENT"):
    try:
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text:
            return None
        # task_typeì€ "RETRIEVAL_DOCUMENT" / "RETRIEVAL_QUERY" ë§Œ ì‚¬ìš©
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type
        )
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}")
        return None


@st.cache_data
def load_and_embed_precedents(file_path='precedents_data.txt'):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return [], []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return [], []

    # ê²¬ê³ í•œ ìŠ¤í”Œë¦¿: ë§ˆì»¤ ë¼ì¸ì— ê³µë°±/ê°œí–‰ ìˆì–´ë„ ë¶„í• 
    import re
    chunks = re.split(r'\s*---END OF PRECEDENT---\s*', content)
    precedents = [p.strip() for p in chunks if p and p.strip()]

    embeddings, valid_precedents = [], []
    for p in precedents:
        ebd = embed_text(p, task_type="RETRIEVAL_DOCUMENT")
        if ebd:
            embeddings.append(ebd)
            valid_precedents.append(p)

    print(f"[RAG] precedents={len(valid_precedents)}")
    return valid_precedents, embeddings


def _parse_precedent_block(text: str) -> dict:
    """í”„ë¦¬í…ìŠ¤íŠ¸ íŒë¡€ ë¸”ë¡ì—ì„œ ì œëª©/ì„ ê³ /ìš”ì§€/ë°œì·Œë¥¼ ìµœëŒ€í•œ ë½‘ì•„ë‚¸ë‹¤(ë£°ë² ì´ìŠ¤)."""
    import re
    t = text.strip()

    # ì œëª©(ì²« ì¤„ ë˜ëŠ” ëŒ€ë²•ì›/ê³ ë“±ë²•ì› í—¤ë”)
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    title = lines[0][:120] if lines else "ì œëª© ì—†ìŒ"

    # [ëŒ€ë²•ì› 2024. 1. 18. ì„ ê³  ... íŒê²°] íŒ¨í„´ì—ì„œ ë²•ì›/ì„ ê³ ì¼ì ì¶”ì¶œ
    m = re.search(r'\[(?P<court>[^ \[\]]+)\s+(?P<date>\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)\s*ì„ ê³ .*?íŒê²°\]', t)
    court = m.group('court') if m else ""
    date  = m.group('date') if m else ""

    # ã€íŒê²°ìš”ì§€ã€‘ ë˜ëŠ” ã€íŒì‹œì‚¬í•­ã€‘ ì¼ë¶€ ì¶”ì¶œ
    holding = ""
    m2 = re.search(r'ã€íŒê²°ìš”ì§€ã€‘(.*?)(ã€|$)', t, re.S)
    if m2:
        holding = re.sub(r'\s+', ' ', m2.group(1)).strip()
    else:
        m3 = re.search(r'ã€íŒì‹œì‚¬í•­ã€‘(.*?)(ã€|$)', t, re.S)
        if m3:
            holding = re.sub(r'\s+', ' ', m3.group(1)).strip()

    if not holding:
        # ì—†ìœ¼ë©´ ë³¸ë¬¸ ì´ˆë°˜ 160ì ì •ë„ë¡œ ëŒ€ì²´
        holding = re.sub(r'\s+', ' ', t)[:160].strip()

    # ì „ë¬¸ ì¼ë¶€(ì „ë¬¸/ì´ìœ /ë³¸ë¬¸ ê·¼ì²˜ì—ì„œ 120~160ì)
    excerpt = ""
    for key in ["ã€ì „ë¬¸ã€‘", "ã€ì´ ìœ ã€‘", "ã€ì´ìœ ã€‘", "ã€ë³¸ë¬¸ã€‘"]:
        pos = t.find(key)
        if pos != -1:
            excerpt = re.sub(r'\s+', ' ', t[pos:pos+300]).strip()
            break
    if not excerpt:
        excerpt = re.sub(r'\s+', ' ', t)[:300].strip()

    # ì¢€ ì¤„ì—¬ì£¼ê¸°
    if len(holding) > 130: holding = holding[:130].rstrip() + "â€¦"
    if len(excerpt) > 160: excerpt = excerpt[:160].rstrip() + "â€¦"

    return {
        "title": title,
        "court": court,
        "date":  date,
        "holding": holding,
        "excerpt": excerpt,
    }


def find_similar_precedents(query_text, precedents, embeddings, top_k=3):
    """
    ê¸°ì¡´: ì»¤ë‹¤ë€ ì „ë¬¸ ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
    ë³€ê²½: ê¹”ë”í•œ ìš”ì•½ì¹´ë“œìš© dict ëª©ë¡ ë°˜í™˜
    """
    if not embeddings or not precedents:
        return []

    q_emb = embed_text(query_text, task_type="search_query")
    if q_emb is None:
        return []

    embeddings_np = np.array(embeddings)
    q_np = np.array(q_emb)
    sims = np.dot(embeddings_np, q_np)

    # ìƒìœ„ Kê°œ
    idxs = np.argsort(sims)[::-1][:top_k]

    results = []
    for i in idxs:
        sim = float(sims[i])
        # ì„ê³„ê°’ ë„ˆë¬´ ë†’ìœ¼ë©´ ì•ˆ ë‚˜ì˜¤ëŠ” ë¬¸ì œ â†’ ì‚´ì§ ì™„í™”(0.20)
        if sim < 0.20:
            continue

        parsed = _parse_precedent_block(precedents[i])
        results.append({
            "similarity": sim,  # 0~1
            **parsed
        })

    return results





# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì (Prime Genome) ---
with open("system_prompt.txt", "r", encoding="utf-8") as f:
    SYSTEM_INSTRUCTION = f.read()

if "model" not in st.session_state:
    st.session_state.model = genai.GenerativeModel("gemini-2.5-flash",
                                                   system_instruction=SYSTEM_INSTRUCTION)

# --- 5. ëŒ€í™” ì„¸ì…˜ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state:
    st.session_state.chat = st.session_state.model.start_chat(history=[])
    initial_prompt = "ì‹œìŠ¤í…œ ê°€ë™. 'ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ì—¬ Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
    try:
        response = st.session_state.chat.send_message(initial_prompt)
        st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{response.text}</div>"})
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# --- 6. ëŒ€í™” ì¶œë ¥ ---
for message in st.session_state.messages:
    role = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role, avatar=avatar):
        st.markdown(f"<div class='fadein'>{message['content']}</div>", unsafe_allow_html=True)

# --- 7. ì…ë ¥ ë° ë§ˆì§€ë§‰ Phaseì—ì„œë§Œ íŒë¡€ í˜¸ì¶œ (ë¸Œë¦¬í•‘ ë³´ê³ ì„œ íŠ¸ë¦¬ê±° ë²„ì „) ---
import re

def _is_menu_input(s: str) -> bool:
    if not s:
        return False
    s = s.strip()
    # ìˆ«ìë§Œ, ë˜ëŠ” 2-ìˆ«ì í˜•íƒœë§Œ (ë©”ë‰´ ì„ íƒ)
    return bool(re.fullmatch(r'\d+|2-\d+', s))

def _is_final_report(txt: str) -> bool:
    if not txt:
        return False
    t = txt.replace(" ", "")
    # 'ìµœì¢… ë³´ê³ ì„œ' í¬ë§·ì˜ í•µì‹¬ í‘œì§€ì–´ê°€ ìµœì†Œ 2ê°œ ì´ìƒ ì¡´ì¬ + ê¸¸ì´ ê¸°ì¤€
    hits = 0
    for key in ["ìœ ì‚¬ìˆ˜ì‹ /ì‚¬ê¸°ì „ëµë¸Œë¦¬í•‘ë³´ê³ ì„œ",
                "ë¦¬ìŠ¤í¬ì‹œë®¬ë ˆì´ì…˜ë¶„ì„",
                "ê¶Œì¥ë‹¤ìŒë‹¨ê³„",
                "ë©´ì±…ì¡°í•­",
                "ìµœì¢…ë³´ê³ ì„œ",
                "ë¸Œë¦¬í•‘ë³´ê³ ì„œ"]:
        if key in t:
            hits += 1
    return (hits >= 2) and (len(t) > 800)

if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(f"<div class='fadein'>{prompt}</div>", unsafe_allow_html=True)

    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
        try:
            response_stream = st.session_state.chat.send_message(prompt, stream=True)
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                placeholder = st.empty()
                full_response = ""
                for chunk in response_stream:
                    # ì¼ë¶€ ì‘ë‹µ ì¡°ê°ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°ê°€ ìˆì–´ ê°€ë“œ
                    if not getattr(chunk, "text", None):
                        continue
                    full_response += chunk.text
                    placeholder.markdown(
                        f"<div class='fadein'>{full_response}â–Œ</div>",
                        unsafe_allow_html=True
                    )
                placeholder.markdown(
                    f"<div class='fadein'>{full_response}</div>",
                    unsafe_allow_html=True
                )

            # ìŠ¤íŠ¸ë¦¼ì´ ë¹„ì–´ ìˆìœ¼ë©´ non-stream í´ë°±
            if not full_response.strip():
                non_stream = st.session_state.chat.send_message(prompt)
                txt = getattr(non_stream, "text", None)
                if txt:
                    full_response = txt
                    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                        st.markdown(f"<div class='fadein'>{full_response}</div>", unsafe_allow_html=True)

            st.session_state.messages.append({"role": "Architect", "content": full_response})

            # ğŸ”’ ì—¬ê¸°ì„œ 'ìµœì¢… ë³´ê³ ì„œ'ì¼ ë•Œë§Œ íŒë¡€ ë¶™ì„ (ë©”ë‰´ ì…ë ¥/ì¤‘ê°„ ë‹¨ê³„ì—ì„œëŠ” ì ˆëŒ€ ì•ˆ ë¶™ì„)
            if _is_final_report(full_response) and not _is_menu_input(prompt):
                precedents, embeddings = load_and_embed_precedents()
                if not precedents or not embeddings:
                    st.warning("âš ï¸ íŒë¡€ íƒ„ì•½ê³ ê°€ ë¹„ì—ˆê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨. 'precedents_data.txt' ìœ„ì¹˜/í˜•ì‹ í™•ì¸.")
                else:
                    similar_cases = find_similar_precedents(prompt, precedents, embeddings, top_k=5)
                    if similar_cases:
                        st.markdown("<br><b>ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„</b><br>", unsafe_allow_html=True)
                        # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ë°©ì§€
                            if similar_cases:
        # í—¤ë” + ê²€ìƒ‰ ì¿¼ë¦¬
        st.markdown("**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„**\n\n* ê²€ìƒ‰ ì¿¼ë¦¬: `" + prompt + "`\n")

        # ìƒìœ„ 3ê±´ë§Œ ì¹´ë“œí˜• ìš”ì•½ìœ¼ë¡œ ì¶œë ¥
        for case in similar_cases[:3]:
            sim_pct = int(round(case["similarity"] * 100))
            item_md = (
                f"* íŒë¡€ [{case.get('title','ì œëª© ì—†ìŒ')}]  \n"
                f"  - ì„ ê³ : {case.get('date','').strip()} {case.get('court','').strip()} | ìœ ì‚¬ë„: {sim_pct}%  \n"
                f"  - íŒê²°ìš”ì§€: {case.get('holding','').strip()}  \n"
                f"  - ì „ë¬¸ ì¼ë¶€: \"{case.get('excerpt','').strip()}\""
            )
            st.markdown(item_md)

                    else:
                        st.info("â„¹ï¸ ìµœì¢… ë³´ê³ ì„œ ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ëœ ìœ ì‚¬ íŒë¡€ê°€ ì—†ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.20)")

        except Exception as e:
            err = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(err)
            st.session_state.messages.append({"role": "Architect", "content": err})
