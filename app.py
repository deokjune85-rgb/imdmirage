# ======================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.6 â€” Contextual Dual RAG (JSONL/TXT Hybrid) + Relay Mechanism
# ======================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time
import json # â˜… JSONL ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • ë° CSS (ê¸°ì¡´ 7.5 ë²„ì „ ìœ ì§€) ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.6", page_icon="ğŸ›¡ï¸", layout="centered")

custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    font-size: 16px !important;
    line-height: 1.7 !important;
}
h1 { text-align: left !important; font-weight: 900 !important; font-size: 36px !important; margin-top: 10px !important; margin-bottom: 15px !important; }
strong, b { font-weight: 700; }
.fadein { animation: fadeInText 0.5s ease-in-out forwards; opacity: 0; }
@keyframes fadeInText { from {opacity: 0; transform: translateY(3px);} to {opacity: 1; transform: translateY(0);} }
[data-testid="stChatMessageContent"] { font-size: 16px !important; }
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.6")
st.warning("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- 3. API í‚¤ ë° RAG ì—”ì§„ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
         raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
    st.stop()

# --- [RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜] (â˜…í•µì‹¬ ìˆ˜ì •: JSONL/TXT í•˜ì´ë¸Œë¦¬ë“œ ë¡œë”â˜…) ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text, task_type="retrieval_document"):
    try:
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text: return None
        result = genai.embed_content(model=EMBEDDING_MODEL_NAME, content=clean_text, task_type=task_type)
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}"); return None

# [â˜…í•µì‹¬ ìˆ˜ì •â˜…] í†µí•© ë°ì´í„° ë¡œë” (JSONL ë° TXT ì§€ì›)
@st.cache_data
def load_and_embed_data(file_path, separator_regex=None):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}"); return [], []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}"); return [], []

    if not content.strip(): return [], []

    data_items, embeddings = [], []

    # JSONL íŒŒì¼ ì²˜ë¦¬ (.jsonl í™•ì¥ì)
    if file_path.endswith('.jsonl'):
        for line in content.strip().split('\n'):
            try:
                item = json.loads(line)
                # 'rag_index' í•„ë“œë¥¼ ì„ë² ë”© (í•µì‹¬!)
                text_to_embed = item.get('rag_index')
                if text_to_embed:
                    ebd = embed_text(text_to_embed, task_type="retrieval_document")
                    if ebd:
                        embeddings.append(ebd)
                        # ì „ì²´ ê°ì²´(ì›ë¬¸, ë§í¬ í¬í•¨)ë¥¼ ì €ì¥
                        data_items.append(item)
            except json.JSONDecodeError:
                continue
    
    # TXT íŒŒì¼ ì²˜ë¦¬ (ë²•ë ¹ ë°ì´í„° ë° í•˜ìœ„ í˜¸í™˜ì„±)
    elif separator_regex:
        chunks = re.split(separator_regex, content)
        raw_items = [p.strip() for p in chunks if p and p.strip()]
        for item_text in raw_items:
            ebd = embed_text(item_text, task_type="retrieval_document")
            if ebd:
                embeddings.append(ebd)
                # TXTëŠ” êµ¬ì¡°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸ ìì²´ë¥¼ ê°ì²´í™”í•˜ì—¬ ì €ì¥ (êµ¬ì¡° í†µì¼)
                data_items.append({"rag_index": item_text, "raw_text": item_text})

    print(f"[RAG] Loaded {len(data_items)} items from {file_path}.")
    return data_items, embeddings

# ê²€ìƒ‰ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.50):
    if not embeddings or not items: return []
    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None: return []
    
    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]
    
    results = []
    for i in idxs:
        if float(sims[i]) >= threshold:
            # ê²°ê³¼ì— ì „ì²´ ê°ì²´ì™€ ìœ ì‚¬ë„ë¥¼ ì €ì¥ (ì´ë¯¸ ê°ì²´í™”ë˜ì–´ ìˆìŒ)
            result_item = items[i].copy()
            result_item["similarity"] = float(sims[i])
            results.append(result_item)
    return results


# (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ìœ ì§€ - ìƒëµ)
def _is_menu_input(s: str) -> bool: ...
def _is_final_report(txt: str) -> bool: ...
def _query_title(prompt_text: str) -> str: ...
def update_active_module(response_text): ...

# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì (Prime Genome) ë¡œë“œ ë° ì´ˆê¸°í™” ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    # ... (ê²€ì¦ ë¡œì§ ìƒëµ) ...
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì½”ì–´(system_prompt.txt) ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()


if "model" not in st.session_state:
    try:
        st.session_state.model = genai.GenerativeModel("models/gemini-2.5-flash",
                                                    system_instruction=SYSTEM_INSTRUCTION)
        
        # [â˜…ìˆ˜ì •ë¨â˜…] ë“€ì–¼ RAG ì´ˆê¸°í™” (JSONL + TXT)
        with st.spinner("ë¶„ì„ ì—”ì§„(Dual RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            # 1. íŒë¡€ ë°ì´í„° ë¡œë“œ (P-RAG) - JSONL ìš°ì„ , TXT í´ë°±
            p_data, p_emb = load_and_embed_data('precedents_data.jsonl')
            if not p_data:
                 # JSONLì´ ì—†ê±°ë‚˜ ë¹„ì—ˆìœ¼ë©´ TXT ì‹œë„
                 p_data, p_emb = load_and_embed_data('precedents_data.txt', r'\s*---END OF PRECEDENT---\s*')

            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb

            # 2. ë²•ë ¹ ë°ì´í„° ë¡œë“œ (S-RAG) - TXT ë°©ì‹ ìœ ì§€
            s_data, s_emb = load_and_embed_data('statutes_data.txt', r'\s*---END OF STATUTE---\s*')
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb
        
        st.session_state.active_module = "ì´ˆê¸° ìƒíƒœ (ë¯¸ì •ì˜)"

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

# --- 5, 6. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ì¶œë ¥ (ê¸°ì¡´ ìœ ì§€) ---
# ... (ìƒëµ) ...

# --- 7. ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…í•µì‹¬ ìˆ˜ì •: JSONL ê¸°ë°˜ ì¶œë ¥ ë° ë¦´ë ˆì´â˜…) ---

# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
def stream_and_store_response(chat_session, prompt_to_send, spinner_text="Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
    # ... (í•¨ìˆ˜ ë‚´ìš© ìœ ì§€ - ìƒëµ) ...

# ë©”ì¸ ì…ë ¥ ë£¨í”„
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    # ... (ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° Phase 2 ê°ì§€ ìƒëµ) ...

    # Contextual RAG ì‹¤í–‰
    rag_context = ""
    similar_precedents = []
    
    if not _is_menu_input(prompt):
        # ... (Contextual Query ìƒì„± ìƒëµ) ...

        with st.spinner("ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... (Dual RAG: íŒë¡€/ë²•ë ¹)..."):
            # 1. ë²•ë ¹ ê²€ìƒ‰ (S-RAG)
            if ("statutes" in st.session_state and st.session_state.statutes):
                similar_statutes = find_similar_items(contextual_query, st.session_state.statutes, st.session_state.s_embeddings, top_k=3, threshold=0.75)
                if similar_statutes:
                    # LLM ì£¼ì…ìš© í…ìŠ¤íŠ¸ ìƒì„± ('rag_index' ì‚¬ìš©)
                    s_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n" for c in similar_statutes]
                    rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n" + "\n".join(s_texts)

            # 2. íŒë¡€ ê²€ìƒ‰ (P-RAG)
            if ("precedents" in st.session_state and st.session_state.precedents):
                similar_precedents = find_similar_items(contextual_query, st.session_state.precedents, st.session_state.p_embeddings, top_k=5, threshold=0.75)
                if similar_precedents:
                    # LLM ì£¼ì…ìš© í…ìŠ¤íŠ¸ ìƒì„± ('rag_index' ì‚¬ìš©)
                    p_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n" for c in similar_precedents]
                    rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" + "\n".join(p_texts)

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„±
    final_prompt = f"{prompt}\n{rag_context}"
    current_response = stream_and_store_response(st.session_state.chat, final_prompt)

    # ë¦´ë ˆì´ ë©”ì»¤ë‹ˆì¦˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # ... (ìƒëµ) ...

    # [â˜…í•µì‹¬ ìˆ˜ì •â˜…] íŒë¡€ ì‹œê°í™” ë° ì›ë¬¸ ë³´ê¸° ê¸°ëŠ¥ (JSONL ê¸°ë°˜)
    clean_response = re.sub('<[^<]+?>', '', current_response)
    if _is_final_report(clean_response) and similar_precedents:
        q_title = _query_title(prompt)
        st.markdown("**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n* ê²€ìƒ‰ ì¿¼ë¦¬: `[" + q_title + "]`\n")

        for case_data in similar_precedents[:3]:
            # JSONL ê°ì²´ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            sim_pct = int(round(case_data["similarity"] * 100))
            title = case_data.get('title', 'ì œëª© ì—†ìŒ')
            case_no = case_data.get('case_no', case_data.get('id', ''))
            court = case_data.get('court', '')
            date = case_data.get('date', '')
            url = case_data.get('url')
            full_text = case_data.get('full_text', case_data.get('raw_text')) # ì „ë¬¸ ë˜ëŠ” TXT í´ë°±
            
            label = f"íŒë¡€ [{title}]"
            if court and case_no:
                label += f" â€” {court} {case_no}"

            # ìš”ì•½ ì¹´ë“œ ì¶œë ¥ ('rag_index' ì‚¬ìš©)
            summary = case_data.get('rag_index', 'ìš”ì•½ ë‚´ìš© ì—†ìŒ')
            if len(summary) > 200: summary = summary[:197] + "..."

            # ë§í¬ ìƒì„±
            action_link = f"[ğŸ”— ì›ë¬¸ ë§í¬ ë³´ê¸°]({url})" if url else ""

            item_md = (
                f"* **{label}**\n"
                f"  - ì„ ê³ : {date} | ìœ ì‚¬ë„: {sim_pct}% | {action_link}\n"
                f"  - ë‚´ìš© ìš”ì•½ (RAG Index): {summary}"
            )
            st.markdown(item_md)
            
            # [â˜…ì‹ ì„¤â˜…] ì›ë¬¸ ë³´ê¸° ê¸°ëŠ¥ (Expander ì‚¬ìš©)
            if full_text:
                with st.expander("ğŸ“„ íŒë¡€ ì „ë¬¸ ë³´ê¸°"):
                    # ì „ë¬¸ì€ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (ê°€ë…ì„± í™•ë³´)
                    st.text(full_text)

    elif _is_final_report(clean_response) and not _is_menu_input(prompt) and not similar_precedents:
         st.info("â„¹ï¸ ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.75)")
