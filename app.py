# ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1 - ì™„ì „ ìˆ˜ì •íŒ

import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time
import json
import PyPDF2
from io import BytesIO

# ---------------------------------------
# 0. ê¸°ë³¸ ì„¸íŒ…
# ---------------------------------------
st.set_page_config(
    page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1",
    page_icon="ğŸ›¡ï¸",
    layout="centered"
)

custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    font-size: 16px !important;
    line-height: 1.7 !important;
}
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
}
strong, b { font-weight: 700; }
.fadein { animation: fadeInText 0.5s ease-in-out forwards; opacity: 0; }
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}
[data-testid="stChatMessageContent"] {
    font-size: 16px !important;
}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1")
st.caption("The Architect â€” ì „ëµ ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„")

st.warning(
    "âš ï¸ ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. "
    "ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
)

# ---------------------------------------
# 1. API í‚¤ ì„¤ì •
# ---------------------------------------
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
        raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
    st.stop()

# ---------------------------------------
# 2. ì„ë² ë”© / RAG ìœ í‹¸
# ---------------------------------------
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text: str, task_type: str = "retrieval_document"):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    clean_text = text.replace("\n", " ").strip()
    if not clean_text:
        return None
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type,
        )
        return result["embedding"]
    except Exception as e:
        print(f"[Embedding error] {e}")
        return None

def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.5):
    """ìœ ì‚¬ë„ ê²€ìƒ‰"""
    if not items or not embeddings:
        return []

    try:
        q_emb = embed_text(query_text, task_type="retrieval_query")
        if q_emb is None:
            return []

        sims = np.dot(np.array(embeddings), np.array(q_emb))
        idxs = np.argsort(sims)[::-1][:top_k]

        results = []
        for i in idxs:
            score = float(sims[i])
            if score < threshold:
                continue
            item = items[i].copy()
            item["similarity"] = score
            results.append(item)

        return results
    except Exception as e:
        print(f"[RAG Error] {e}")
        return []

# ---------------------------------------
# 2-1. ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”© ë¡œë“œ
# ---------------------------------------
@st.cache_data(show_spinner=False)
def load_precomputed_embeddings():
    """ì„ë² ë”© íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ ìƒì„±"""
    statute_items = []
    statute_embeddings = []
    precedent_items = []
    precedent_embeddings = []
    
    try:
        # ë²•ë ¹ ë¡œë“œ
        if os.path.exists("statutes_data.txt"):
            with open("statutes_data.txt", "r", encoding="utf-8") as f:
                content = f.read()
            
            parts = re.split(r"\s*---END OF STATUTE---\s*", content)
            for p in parts:
                p = p.strip()
                if not p:
                    continue
                emb = embed_text(p)
                if emb:
                    statute_items.append({"rag_index": p, "raw_text": p})
                    statute_embeddings.append(emb)
            
            print(f"[RAG] ë²•ë ¹ ë¡œë“œ: {len(statute_items)}ê°œ")
        
        # íŒë¡€ ë¡œë“œ
        if os.path.exists("precedents_data.jsonl"):
            with open("precedents_data.jsonl", "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        txt = obj.get("rag_index", "")
                        if txt:
                            emb = embed_text(txt)
                            if emb:
                                precedent_items.append(obj)
                                precedent_embeddings.append(emb)
                    except:
                        continue
            
            print(f"[RAG] íŒë¡€ ë¡œë“œ: {len(precedent_items)}ê°œ")
    
    except Exception as e:
        print(f"[RAG ë¡œë”© ì—ëŸ¬] {e}")
    
    return statute_items, statute_embeddings, precedent_items, precedent_embeddings

# ---------------------------------------
# 3. PDF ì²˜ë¦¬ í•¨ìˆ˜
# ---------------------------------------
def extract_text_from_pdf(uploaded_file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        
        for page_num, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text()
            if page_text:
                text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n"
                text += page_text
        
        return text
    
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

def analyze_case_file(pdf_text: str, model):
    """PDF ë‚´ìš© ìë™ ë¶„ì„"""
    analysis_prompt = f"""
ë‹¤ìŒì€ ì‚¬ê±´ê¸°ë¡ PDFì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ì…ë‹ˆë‹¤. 

[PDF ë‚´ìš©]
{pdf_text[:15000]}

[ë¶„ì„ ì§€ì¹¨]
1. ì´ ì‚¬ê±´ì˜ ë„ë©”ì¸ ë¶„ë¥˜ (í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬/í–‰ì •/íŒŒì‚°/IP/ì˜ë£Œ/ì„¸ë¬´ ì¤‘ 1ê°œ)
2. ì„¸ë¶€ ë¶„ì•¼ (ì˜ˆ: í˜•ì‚¬-ë§ˆì•½, ë¯¼ì‚¬-ê³„ì•½ë¶„ìŸ ë“±)
3. í•µì‹¬ ì‚¬ì‹¤ê´€ê³„ 5ê°€ì§€ (ì‹œê°„ìˆœ ë˜ëŠ” ì¤‘ìš”ë„ìˆœ)
4. í™•ë³´ëœ ì¦ê±° ëª©ë¡ (ë¬¸ì„œëª…, ì¢…ë¥˜)
5. í”¼ê³ ì¸/ì›ê³  ì¸¡ ì£¼ì¥ ìš”ì•½
6. ìƒëŒ€ë°© ì¸¡ ì£¼ì¥ ìš”ì•½

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.

{{
  "domain": "í˜•ì‚¬",
  "subdomain": "ë§ˆì•½",
  "key_facts": ["2023-05-01 í•„ë¡œí° 5g ì†Œì§€ë¡œ ì²´í¬", "ê²½ì°° ì¡°ì‚¬ ì¤‘ íˆ¬ì•½ ì¸ì •", "ì´ˆë²”", "ìƒí™œë¹„ ëª©ì  ì£¼ì¥", "3ê°œì›”ê°„ 10íšŒ íŒë§¤ ì •í™©"],
  "evidence": ["ì••ìˆ˜ì¡°ì„œ", "ê°ì •ì„œ(ì–‘ì„±)", "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë‚´ì—­", "ê³„ì¢Œì´ì²´ ë‚´ì—­"],
  "our_claim": "ë‹¨ìˆœ íˆ¬ì•½ ëª©ì ì´ë©° ì´ˆë²”ìœ¼ë¡œ ì„ ì²˜ í•„ìš”",
  "their_claim": "ë°˜ë³µ íŒë§¤ë¡œ ì˜ë¦¬ ëª©ì  ì¸ì •"
}}
"""
    
    try:
        response = model.generate_content(analysis_prompt)
        result_text = response.text.strip()
        
        result_text = result_text.replace("```json", "").replace("```", "").strip()
        result = json.loads(result_text)
        return result
    
    except Exception as e:
        st.error(f"ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

# ---------------------------------------
# 4. ê°ì¢… ìœ í‹¸ í•¨ìˆ˜
# ---------------------------------------
def _is_menu_input(s: str) -> bool:
    """ë©”ë‰´ ë²ˆí˜¸ ì…ë ¥ ê°ì§€"""
    return bool(re.fullmatch(r"^\s*\d{1,2}(?:-\d{1,2})?\s*$", s))

def _is_reset_keyword(s: str) -> bool:
    """ì´ˆê¸°í™” í‚¤ì›Œë“œ ê°ì§€"""
    keywords = ["ì²˜ìŒ", "ë©”ì¸", "ì´ˆê¸°í™”", "reset", "ëŒì•„ê°€", "ì²˜ìŒìœ¼ë¡œ"]
    return any(kw in s.lower() for kw in keywords)

def _is_final_report(txt: str) -> bool:
    """ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì—¬ë¶€ í™•ì¸"""
    return "ì „ëµ ë¸Œë¦¬í•‘ ë³´ê³ ì„œ" in txt

def _query_title(prompt_text: str) -> str:
    """ì¿¼ë¦¬ ì œëª© ìƒì„±"""
    return prompt_text[:67] + "..." if len(prompt_text) > 70 else prompt_text

def update_active_module(response_text: str):
    """í™œì„± ëª¨ë“ˆ ì—…ë°ì´íŠ¸"""
    if ("9." in response_text and "ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œ" in response_text) or \
       ("Auto-Analysis Modeë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤" in response_text):
        st.session_state.active_module = "Auto-Analysis Mode"
        return
    
    m = re.search(r"'(.+?)' ëª¨ë“ˆì„ (?:ìµœì¢… )?í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
    if m:
        st.session_state.active_module = m.group(1).strip()
    elif "Phase 0" in response_text and not st.session_state.get("active_module"):
        st.session_state.active_module = "Phase 0"

# ---------------------------------------
# 5. ì‹œìŠ¤í…œ í”„ë¼ì„ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
# ---------------------------------------
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if len(SYSTEM_INSTRUCTION) < 100:
        raise ValueError("System prompt is too short.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: system_prompt.txt ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()

# ---------------------------------------
# 6. ëª¨ë¸ & ì„¸ì…˜ ì´ˆê¸°í™”
# ---------------------------------------
if "model" not in st.session_state:
    try:
        st.session_state.model = genai.GenerativeModel(
            "models/gemini-2.0-flash-exp",
            system_instruction=SYSTEM_INSTRUCTION,
        )
        st.session_state.chat = st.session_state.model.start_chat(history=[])
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

    st.session_state.messages = []
    st.session_state.active_module = "Phase 0"

    st.session_state.precedents = []
    st.session_state.p_embeddings = []
    st.session_state.statutes = []
    st.session_state.s_embeddings = []

    try:
        init_prompt = "ì‹œìŠ¤í…œ ê°€ë™. Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
        resp = st.session_state.chat.send_message(init_prompt)
        init_text = resp.text
    except Exception as e:
        init_text = f"[ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}]"

    st.session_state.messages.append({"role": "Architect", "content": init_text})
    update_active_module(init_text)

# ---------------------------------------
# 7. ê³¼ê±° ë©”ì‹œì§€ ë Œë”ë§
# ---------------------------------------
for m in st.session_state.messages:
    role_name = "Client" if m["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if m["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(m["content"], unsafe_allow_html=True)

# ---------------------------------------
# 8. PDF ì—…ë¡œë“œ UI
# ---------------------------------------
if st.session_state.get("active_module") == "Auto-Analysis Mode":
    last_user_msg = None
    for m in reversed(st.session_state.messages):
        if m["role"] == "user":
            last_user_msg = m["content"].strip()
            break
    
    if last_user_msg == "9":
        st.markdown("---")
        
        st.info("""
        **ğŸ“„ ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œë€?**
        
        PDF íŒŒì¼(íŒê²°ë¬¸, ê³ ì†Œì¥, ë‹µë³€ì„œ ë“±)ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ìë™ìœ¼ë¡œ:
        - ì‚¬ê±´ ë„ë©”ì¸ ë¶„ë¥˜ (í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬ ë“±)
        - í•µì‹¬ ì‚¬ì‹¤ê´€ê³„ 5ê°€ì§€ ì¶”ì¶œ
        - í™•ë³´ëœ ì¦ê±° ëª©ë¡ ì •ë¦¬
        - ì–‘ì¸¡ ì£¼ì¥ ìš”ì•½
        
        **ì²˜ë¦¬ ì‹œê°„:** ì•½ 1-3ë¶„ | **ìµœëŒ€ í¬ê¸°:** 50MB | **í˜•ì‹:** í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFë§Œ ê°€ëŠ¥
        """)
        
        st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader(
            "ì‚¬ê±´ê¸°ë¡ PDFë¥¼ ì„ íƒí•˜ì„¸ìš”",
            type=["pdf"],
            help="íŒê²°ë¬¸, ê³ ì†Œì¥, ë‹µë³€ì„œ, ì‚¬ê±´ê¸°ë¡ ë“±",
        )
        
        if uploaded_file is not None:
            file_size = uploaded_file.size / (1024 * 1024)
            st.success(f"**íŒŒì¼ëª…:** {uploaded_file.name}  |  **í¬ê¸°:** {file_size:.1f}MB")
            
            if st.button("ğŸš€ ìë™ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
                with st.spinner("ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                    pdf_text = extract_text_from_pdf(uploaded_file)
                    
                    if not pdf_text:
                        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()
                    
                    st.success(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(pdf_text):,} ê¸€ì)")
                
                with st.spinner("ğŸ§  AI ë¶„ì„ ì¤‘..."):
                    analysis = analyze_case_file(pdf_text, st.session_state.model)
                    
                    if not analysis:
                        st.error("ë¶„ì„ ì‹¤íŒ¨. PDF í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                        st.stop()
                
                st.success("ë¶„ì„ ì™„ë£Œ!")
                
                with st.expander("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë³´ê¸°", expanded=True):
                    st.markdown(f"**ë„ë©”ì¸:** {analysis['domain']}")
                    st.markdown(f"**ì„¸ë¶€ ë¶„ì•¼:** {analysis.get('subdomain', 'ë¯¸ë¶„ë¥˜')}")
                    
                    st.markdown("**í•µì‹¬ ì‚¬ì‹¤ê´€ê³„:**")
                    for i, fact in enumerate(analysis.get('key_facts', []), 1):
                        st.markdown(f"{i}. {fact}")
                    
                    st.markdown("**í™•ë³´ëœ ì¦ê±°:**")
                    for i, ev in enumerate(analysis.get('evidence', []), 1):
                        st.markdown(f"{i}. {ev}")
                    
                    st.info(f"**ìš°ë¦¬ ì¸¡:** {analysis.get('our_claim', '(ì •ë³´ ì—†ìŒ)')}")
                    st.warning(f"**ìƒëŒ€ ì¸¡:** {analysis.get('their_claim', '(ì •ë³´ ì—†ìŒ)')}")
                
                st.session_state["auto_analysis"] = analysis
                st.session_state["pdf_text"] = pdf_text
        
        st.markdown("---")

# ---------------------------------------
# 9. ìë™ ë¶„ì„ ê²°ê³¼ í™œìš© UI
# ---------------------------------------
if "auto_analysis" in st.session_state and st.session_state.get("active_module") != "Auto-Analysis Mode":
    auto_data = st.session_state["auto_analysis"]
    
    st.success("AIê°€ ìë™ìœ¼ë¡œ í•´ë‹¹ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ì™„ì „í•œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    
    domain_map = {
        "í˜•ì‚¬": "2",
        "ë¯¼ì‚¬": "8",
        "ê°€ì‚¬": "1",
        "ì´í˜¼": "1",
        "íŒŒì‚°": "3",
        "í–‰ì •": "7",
        "ì„¸ë¬´": "6",
        "IP": "4",
        "ì˜ë£Œ": "5",
    }
    
    domain_num = domain_map.get(auto_data["domain"], "8")
    
    auto_input = f"""
[ìë™ ì¶”ì¶œëœ ì‚¬ê±´ ì •ë³´]

ë„ë©”ì¸: {auto_data['domain']} - {auto_data.get('subdomain', 'ë¯¸ë¶„ë¥˜')}

í•µì‹¬ ì‚¬ì‹¤ê´€ê³„:
{chr(10).join(f"{i}. {fact}" for i, fact in enumerate(auto_data.get('key_facts', []), 1))}

í™•ë³´ëœ ì¦ê±°:
{chr(10).join(f"- {ev}" for ev in auto_data.get('evidence', []))}

ìš°ë¦¬ ì¸¡ ì£¼ì¥:
{auto_data.get('our_claim', '(ì •ë³´ ì—†ìŒ)')}

ìƒëŒ€ë°© ì£¼ì¥:
{auto_data.get('their_claim', '(ì •ë³´ ì—†ìŒ)')}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•œ ì „ëµ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
"""
    
    st.session_state.messages.append({"role": "user", "content": f"ìë™ ë¶„ì„ ì™„ë£Œ. {domain_num}ë²ˆ ëª¨ë“ˆ ì‹¤í–‰"})
    
    with st.spinner("ì™„ì „í•œ ì „ëµ ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
        try:
            resp1 = st.session_state.chat.send_message(domain_num)
            st.session_state.messages.append({"role": "Architect", "content": resp1.text})
            
            if domain_num == "2":
                subdomain_map = {
                    "ë§ˆì•½": "2-1",
                    "ì„±ë²”ì£„": "2-2",
                    "ìŒì£¼ìš´ì „": "2-3",
                    "ë„ë°•": "2-4",
                    "ê¸ˆìœµ": "2-5",
                    "ëª…ì˜ˆí›¼ì†": "2-6",
                    "ìœ ì‚¬ìˆ˜ì‹ ": "2-7",
                }
                
                subdomain_num = subdomain_map.get(auto_data.get("subdomain", ""), "2-8")
                
                resp2 = st.session_state.chat.send_message(subdomain_num)
                st.session_state.messages.append({"role": "Architect", "content": resp2.text})
            
            resp3 = st.session_state.chat.send_message(auto_input)
            
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                st.markdown(resp3.text)
            
            st.session_state.messages.append({"role": "Architect", "content": resp3.text})
            
        except Exception as e:
            st.error(f"ìë™ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    del st.session_state["auto_analysis"]
    st.rerun()

# ---------------------------------------
# 10. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜
# ---------------------------------------
def stream_and_store_response(chat_session, prompt_to_send: str):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
    full_response = ""

    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        placeholder = st.empty()
        try:
            stream = chat_session.send_message(prompt_to_send, stream=True)
            for chunk in stream:
                if not getattr(chunk, "parts", None):
                    full_response = "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.]"
                    placeholder.error(full_response)
                    break
                full_response += chunk.text
                placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
            placeholder.markdown(full_response, unsafe_allow_html=True)
        except Exception as e:
            full_response = f"[ì¹˜ëª…ì  ì˜¤ë¥˜: {e}]"
            placeholder.error(full_response)

    st.session_state.messages.append({"role": "Architect", "content": full_response})
    update_active_module(full_response)

    return full_response

# ---------------------------------------
# 11. ë©”ì¸ ì…ë ¥ ë£¨í”„
# ---------------------------------------
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤"):
    
    if _is_reset_keyword(prompt):
        st.session_state.active_module = "Phase 0"
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("Client", avatar="ğŸ‘¤"):
            st.markdown(prompt)
        
        reset_response = "ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. Phase 0ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤."
        
        with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
            st.markdown(reset_response)
        
        st.session_state.messages.append({"role": "Architect", "content": reset_response})
        
        try:
            init_prompt = "ì‹œìŠ¤í…œ ê°€ë™. Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
            resp = st.session_state.chat.send_message(init_prompt)
            init_text = resp.text
        except Exception as e:
            init_text = f"[ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}]"
        
        st.session_state.messages.append({"role": "Architect", "content": init_text})
        st.rerun()
    
    if prompt.strip() == "9":
        st.session_state.active_module = "Auto-Analysis Mode"
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("Client", avatar="ğŸ‘¤"):
            st.markdown(prompt)
        
        stream_and_store_response(st.session_state.chat, prompt)
        st.rerun()
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    is_data_ingestion_phase = "Phase 2" in (st.session_state.active_module or "")

    if (not st.session_state.statutes) and (not st.session_state.precedents):
        try:
            s_data, s_emb, p_data, p_emb = load_precomputed_embeddings()
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb
            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb
        except Exception as e:
            print(f"[RAG ì´ˆê¸°í™” ì‹¤íŒ¨] {e}")
            st.session_state.statutes = []
            st.session_state.s_embeddings = []
            st.session_state.precedents = []
            st.session_state.p_embeddings = []

    rag_context = ""
    similar_precedents = []

    if not _is_menu_input(prompt) and not is_data_ingestion_phase:
        keywords = []
        
        domain_keywords = {
            "í˜•ì‚¬": "ë§ˆì•½ í•„ë¡œí° íˆ¬ì•½ ë§¤ë§¤ ì„±ë²”ì£„ ê°•ê°„ ì¶”í–‰ ìŒì£¼ìš´ì „ í˜ˆì¤‘ì•Œì½”ì˜¬ ë„ë°•",
            "ë¯¼ì‚¬": "ê³„ì•½ ì†í•´ë°°ìƒ ì±„ë¬´ ì´í–‰ í•´ì œ ìœ„ì•½ê¸ˆ",
            "ê°€ì‚¬": "ì´í˜¼ ì–‘ìœ¡ê¶Œ ì¬ì‚°ë¶„í•  ìœ„ìë£Œ í˜¼ì¸",
            "íŒŒì‚°": "íŒŒì‚° ë©´ì±… ì±„ë¬´ íšŒìƒ",
            "í–‰ì •": "ì˜ì—…ì •ì§€ ê³¼ì§•ê¸ˆ ì²˜ë¶„ ì·¨ì†Œ",
        }
        
        for domain, kw in domain_keywords.items():
            if domain in (st.session_state.active_module or ""):
                keywords.append(kw)
        
        keywords.append(prompt)
        contextual_query = " ".join(keywords)

        if st.session_state.statutes:
            s_hits = find_similar_items(contextual_query, st.session_state.statutes, st.session_state.s_embeddings, top_k=3, threshold=0.55)
            if s_hits:
                s_texts = [f"[ìœ ì‚¬ë„: {hit['similarity']:.2f}]\n{hit.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n" for hit in s_hits]
                rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n" + "\n".join(s_texts)

        if st.session_state.precedents:
            similar_precedents = find_similar_items(contextual_query, st.session_state.precedents, st.session_state.p_embeddings, top_k=5, threshold=0.55)
            if similar_precedents:
                p_texts = [f"[ìœ ì‚¬ë„: {hit['similarity']:.2f}]\n{hit.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n" for hit in similar_precedents]
                rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" + "\n".join(p_texts)

    final_prompt = f"[ì‚¬ìš©ì ì›ë¬¸ ì…ë ¥]\n{prompt}\n{rag_context}"
    current_response = stream_and_store_response(st.session_state.chat, final_prompt)

    clean_response = re.sub("<[^<]+?>", "", current_response)

    if _is_final_report(clean_response) and similar_precedents:
        q_title = _query_title(prompt)
        st.markdown(f"**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n* ê²€ìƒ‰ ì¿¼ë¦¬: `[{q_title}]`\n")

        for case_data in similar_precedents[:3]:
            sim_pct = int(round(case_data["similarity"] * 100))
            title = case_data.get("title", "ì œëª© ì—†ìŒ")
            case_no = case_data.get("case_no", case_data.get("id", ""))
            court = case_data.get("court", "")
            date = case_data.get("date", "")
            url = case_data.get("url")
            full_text = case_data.get("full_text", case_data.get("raw_text"))

            label = f"íŒë¡€ [{title}]"
            if court and case_no:
                label += f" - {court} {case_no}"

            summary = case_data.get("rag_index", "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
            if len(summary) > 200:
                summary = summary[:197] + "..."

            link_md = f"[ì›ë¬¸ ë§í¬]({url})" if url else ""

            md = f"* **{label}**\n  - ì„ ê³ : {date} | ìœ ì‚¬ë„: {sim_pct}% | {link_md}\n  - ë‚´ìš© ìš”ì•½: {summary}"
            st.markdown(md)

            if full_text:
                with st.expander("ğŸ“„ íŒë¡€ ì „ë¬¸ ë³´ê¸°"):
                    st.text(full_text)

    elif _is_final_report(clean_response) and not similar_precedents:
        st.info("ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.55)")
