# =====================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.6 â€” Contextual Dual RAG (JSONL/TXT Hybrid) + Relay Mechanism
# =====================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time
import json  # â˜… JSONL ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • ë° CSS ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.6", page_icon="ğŸ›¡ï¸", layout="centered")

# 'SaaS ì‚ë¼' ìƒˆë¼ë“¤ì˜ 'ì“°ë ˆê¸°' 'UI'ë¥¼ 'ì œê±°'í•˜ê³  'í°íŠ¸'ë¥¼ 'ê°•ì œ'í•œë‹¤.
custom_css = '''
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}
html, body, div, span, p {
    font-family: "Noto Sans KR", sans-serif !important;
    font-size: 16px !important;
    line-height: 1.7 !important;
}
h1 { text-align: left !important; font-weight: 900 !important; font-size: 36px !important; margin-top: 10px !important; margin-bottom: 15px !important; }
strong, b { font-weight: 700; }
.fadein { animation: fadeInText 0.5s ease-in-out forwards; opacity: 0; }
@keyframes fadeInText { from {opacity: 0; transform: translateY(3px);} to {opacity: 1; transform: translateY(0);} }
[data-testid="stChatMessageContent"] { font-size: 16px !important; }
</style>
'''
st.markdown(custom_css, unsafe_allow_html=True)

# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.6")
st.warning("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- 3. API í‚¤ ë° RAG ì—”ì§„ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
        raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
    st.stop()

# --- [RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜] (â˜…í•µì‹¬ ìˆ˜ì •: JSONL/TXT í•˜ì´ë¸Œë¦¬ë“œ ë¡œë”â˜…) ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004"


def embed_text(text, task_type="retrieval_document"):
    """'í…ìŠ¤íŠ¸'ë¥¼ 'ë²¡í„°(ìˆ«ì)'ë¡œ 'ë³€í™˜'í•˜ëŠ” 'ì—°ê¸ˆìˆ '."""
    try:
        clean_text = text.replace("\n", " ").strip()
        if not clean_text:
            return None
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type
        )
        return result["embedding"]
    except Exception as e:
        print(f"Embedding error: {e}")
        return None


@st.cache_data(show_spinner=True)  # 'íƒ„ì•½ê³ ' 'ì¥ì „'ì€ 'ëˆˆ'ìœ¼ë¡œ 'í™•ì¸'ì‹œì¼œì¤€ë‹¤.
def load_and_embed_data(file_path, separator_regex=None):
    """
    'JSONL'ê³¼ 'TXT' 'íƒ„ì•½ê³ 'ë¥¼ 'ì½ì–´' 'ë²¡í„°' 'íƒ„ì•½'ìœ¼ë¡œ 'ì£¼ì¡°'í•œë‹¤.
    - íŒŒì¼ì´ ì•„ì˜ˆ ì—†ê±°ë‚˜ ì½ê¸° ì‹¤íŒ¨: (None, None) ë°˜í™˜ â†’ ì§„ì§œ 'ë¡œë“œ ì‹¤íŒ¨'
    - íŒŒì¼ì€ ì½ì—ˆëŠ”ë° ì»¨í…ì¸  ì—†ìŒ: ([], []) ë°˜í™˜ â†’ íŒŒì¼ì€ ì •ìƒ, ë°ì´í„°ë§Œ ì—†ìŒ
    """
    # 1) íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
    if not os.path.exists(file_path):
        print(f"[RAG] File not found: {file_path}")
        return None, None  # â˜… ì—¬ê¸°ì„œë§Œ 'ì§„ì§œ' ì‹¤íŒ¨ ì·¨ê¸‰

    # 2) íŒŒì¼ ì½ê¸°
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
    except Exception as e:
        print(f"[RAG] Error reading file {file_path}: {e}")
        return None, None  # â˜… ì½ê¸° ìì²´ê°€ ì•ˆ ë˜ë©´ ì´ê²ƒë„ 'ì§„ì§œ' ì‹¤íŒ¨

    if not content.strip():
        print(f"[RAG] File {file_path} is empty.")
        return [], []  # íŒŒì¼ì€ ìˆìœ¼ë‚˜ ë‚´ìš© ì—†ìŒ

    data_items, embeddings = [], []

    # 3) JSONL ëª¨ë“œ
    if file_path.endswith(".jsonl"):
        total_lines = 0
        parsed = 0
        embedded = 0

        for line_no, line in enumerate(content.strip().split("\n"), start=1):
            total_lines += 1
            line = line.strip()
            if not line:
                continue

            try:
                item = json.loads(line)
                parsed += 1
            except json.JSONDecodeError as e:
                print(f"[RAG][JSONL] Parse error {file_path}:{line_no} â†’ {e}")
                continue

            # 'rag_index' í•„ë“œë¥¼ 'ì„ë² ë”©' (í•µì‹¬!)
            text_to_embed = item.get("rag_index")
            if not text_to_embed:
                print(f"[RAG][JSONL] Missing 'rag_index' at {file_path}:{line_no}")
                continue

            ebd = embed_text(text_to_embed, task_type="retrieval_document")
            if ebd:
                embeddings.append(ebd)
                data_items.append(item)  # ì „ì²´ ê°ì²´ ì €ì¥
                embedded += 1
            else:
                print(f"[RAG][JSONL] Embedding failed at {file_path}:{line_no}")

        print(
            f"[RAG][JSONL] {file_path} â†’ lines={total_lines}, parsed={parsed}, embedded={embedded}"
        )

    # 4) TXT ëª¨ë“œ (ë²•ë ¹ ë°ì´í„° ë° í•˜ìœ„ í˜¸í™˜ì„±)
    elif separator_regex:
        chunks = re.split(separator_regex, content)
        raw_items = [p.strip() for p in chunks if p and p.strip()]
        print(f"[RAG][TXT] {file_path} â†’ chunks={len(raw_items)}")
        for item_text in raw_items:
            ebd = embed_text(item_text, task_type="retrieval_document")
            if ebd:
                embeddings.append(ebd)
                data_items.append({"rag_index": item_text, "raw_text": item_text})

    print(f"[RAG] Loaded {len(data_items)} items from {file_path}.")
    return data_items, embeddings


def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.50):
    """'ì‚¬ê±´'ê³¼ 'ê°€ì¥' 'ìœ ì‚¬í•œ' 'ì´ì•Œ' 3ê°œë¥¼ 'ë°œì‚¬'í•œë‹¤."""
    if not embeddings or not items:
        return []
    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None:
        return []

    # 'NumPy'ë¥¼ 'ì‚¬ìš©'í•œ 'ë²¡í„°' 'ë‚´ì ' 'ì—°ì‚°' (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]

    results = []
    for i in idxs:
        if float(sims[i]) >= threshold:
            # 'ê²°ê³¼'ì— 'ì „ì²´' 'ê°ì²´'ì™€ 'ìœ ì‚¬ë„'ë¥¼ 'ì €ì¥'
            result_item = items[i].copy()
            result_item["similarity"] = float(sims[i])
            results.append(result_item)
    return results


# --- â˜…â˜…â˜… 'ì‚­ì œ'ëœ 'ìœ í‹¸ë¦¬í‹°' 'í•¨ìˆ˜' 'ì‹¬ì¥' 'ì´ì‹' â˜…â˜…â˜… ---
def _is_menu_input(s: str) -> bool:
    """'ì…ë ¥'ì´ 'ë‹¨ìˆœ' 'ìˆ«ì' 'ë©”ë‰´' 'ì„ íƒ'ì¸ì§€ 'íŒë‹¨'í•œë‹¤."""
    return bool(re.fullmatch(r"^\s*\d{1,2}(?:-\d{1,2})?\s*$", s))


def _is_final_report(txt: str) -> bool:
    """'ì‘ë‹µ'ì´ 'ìµœì¢… ë³´ê³ ì„œ' 'í˜•ì‹'ì¸ì§€ 'íŒë‹¨'í•œë‹¤."""
    return "ì „ëµ ë¸Œë¦¬í•‘ ë³´ê³ ì„œ" in txt


def _query_title(prompt_text: str) -> str:
    """'RAG' 'ì‹œê°í™”'ì— 'ì‚¬ìš©'í•  'ì¿¼ë¦¬' 'ì œëª©'ì„ 'ì¶”ì¶œ'í•œë‹¤."""
    if len(prompt_text) > 70:
        return prompt_text[:67] + "..."
    return prompt_text


def update_active_module(response_text):
    """'ë‡Œ(EPE)'ì˜ 'ì‘ë‹µ'ì—ì„œ 'í˜„ì¬' 'í™œì„±í™”'ëœ 'ëª¨ë“ˆ' 'ì´ë¦„'ì„ 'ì¶”ì¶œ'í•œë‹¤."""
    match = re.search(r"\[(.+?)\]' ëª¨ë“ˆì„ í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
    if match:
        st.session_state.active_module = match.group(1).strip()
    elif "Phase 0" in response_text:
        st.session_state.active_module = "Phase 0 (ë„ë©”ì¸ ì„ íƒ)"


# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì (Prime Genome) ë¡œë“œ ë° ì´ˆê¸°í™” ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if len(SYSTEM_INSTRUCTION) < 100:
        raise ValueError("System prompt is too short.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì½”ì–´(system_prompt.txt) ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()

if "model" not in st.session_state:
    try:
        st.session_state.model = genai.GenerativeModel(
            "models/gemini-2.5-flash",
            system_instruction=SYSTEM_INSTRUCTION
        )

        # [â˜…ìˆ˜ì •ë¨â˜…] ë“€ì–¼ RAG ì´ˆê¸°í™” (JSONL + TXT)
        with st.spinner("ë¶„ì„ ì—”ì§„(Dual RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            # 1. íŒë¡€ ë°ì´í„° ë¡œë“œ (P-RAG) - JSONL ìš°ì„ , TXT í´ë°±
            p_data, p_emb = load_and_embed_data("precedents_data.jsonl")

            if p_data is None:
                # ì§„ì§œë¡œ íŒŒì¼ì´ ì—†ê±°ë‚˜ ì½ê¸° ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ í´ë°±
                st.warning(
                    "ê²½ê³ : 'precedents_data.jsonl' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì½ê¸° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 'txt' íŒŒì¼ë¡œ í´ë°±í•©ë‹ˆë‹¤."
                )
                p_data, p_emb = load_and_embed_data(
                    "precedents_data.txt",
                    r"\s*---END OF PRECEDENT---\s*"
                )
            elif isinstance(p_data, list) and len(p_data) == 0:
                # íŒŒì¼ì€ ì½ì—ˆëŠ”ë° ìœ íš¨í•œ íŒë¡€ê°€ 0ê±´ì¸ ê²½ìš° â†’ í¬ë§·/ì„ë² ë”© ë¬¸ì œì¼ ê°€ëŠ¥ì„±
                st.info(
                    "â„¹ï¸ 'precedents_data.jsonl'ì€ ë¡œë“œë˜ì—ˆìœ¼ë‚˜ ìœ íš¨í•œ íŒë¡€ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. JSONL í˜•ì‹ê³¼ 'rag_index' ë° ì„ë² ë”© ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
                )

            st.session_state.precedents = p_data or []
            st.session_state.p_embeddings = p_emb or []

            # 2. ë²•ë ¹ ë°ì´í„° ë¡œë“œ (S-RAG) - TXT ë°©ì‹ ìœ ì§€ (ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
            s_data, s_emb = load_and_embed_data(
                "statutes_data.txt",
                r"\s*---END OF STATUTE---\s*"
            )
            st.session_state.statutes = s_data or []
            st.session_state.s_embeddings = s_emb or []

        st.session_state.active_module = "ì´ˆê¸° ìƒíƒœ (ë¯¸ì •ì˜)"

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

# --- 5. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # 'ì´ˆê¸°' 'í”„ë¡¬í”„íŠ¸'ë¥¼ 'ì—¬ê¸°'ì„œ 'ì‹¤í–‰' (v7.5 êµë¦¬)
    with st.spinner("Architect ì‹œìŠ¤í…œ ê°€ë™..."):
        try:
            initial_prompt = (
                "ì‹œìŠ¤í…œ ê°€ë™. 'ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ì—¬ Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
            )
            chat = st.session_state.model.start_chat(history=[])
            response = chat.send_message(initial_prompt)
            st.session_state.messages.append(
                {"role": "Architect", "content": response.text}
            )
            st.session_state.chat = chat
            update_active_module(response.text)
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            st.stop()

# --- 6. ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
for message in st.session_state.messages:
    role_name = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(message["content"], unsafe_allow_html=True)

# --- 7. ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…í•µì‹¬ ìˆ˜ì •: JSONL ê¸°ë°˜ ì¶œë ¥ ë° ë¦´ë ˆì´â˜…) ---


def stream_and_store_response(
    chat_session,
    prompt_to_send,
    spinner_text="Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘...",
):
    """'ë‡Œ(EPE)'ì— 'ëª…ë ¹'ì„ 'ì „ì†¡'í•˜ê³ , 'ì‘ë‹µ'ì„ 'ì‹¤ì‹œê°„' 'ì¶œë ¥' ë° 'ì €ì¥'í•œë‹¤."""
    full_response = ""
    start_time = time.time()

    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        response_placeholder = st.empty()
        try:
            with st.spinner(spinner_text):
                response_stream = chat_session.send_message(
                    prompt_to_send,
                    stream=True
                )

                for chunk in response_stream:
                    # 'ì•ˆì „' 'í•„í„°'ê°€ 'ì‘ë™'í•˜ë©´ 'ì¦‰ì‹œ' 'ì¤‘ë‹¨'
                    if not chunk.parts:
                        full_response = (
                            "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µì´ 'ì•ˆì „ í•„í„°'ì— ì˜í•´ 'ì°¨ë‹¨'ë˜ì—ˆìŠµë‹ˆë‹¤.]"
                        )
                        response_placeholder.error(full_response)
                        break

                    full_response += chunk.text
                    # 'íƒ€ì´í•‘' 'íš¨ê³¼'
                    response_placeholder.markdown(
                        full_response + "â–Œ",
                        unsafe_allow_html=True
                    )

            # 'íƒ€ì´í•‘' 'íš¨ê³¼' 'ì œê±°'
            response_placeholder.markdown(
                full_response,
                unsafe_allow_html=True
            )

        except Exception as e:
            full_response = f"[ì¹˜ëª…ì  ì˜¤ë¥˜: {e}]"
            response_placeholder.error(full_response)

    # 'ì„¸ì…˜'ì— 'ìµœì¢…' 'ì‘ë‹µ' 'ì €ì¥'
    st.session_state.messages.append(
        {"role": "Architect", "content": full_response}
    )

    # ëª¨ë“ˆ ìƒíƒœ ê°±ì‹ 
    update_active_module(full_response)

    end_time = time.time()
    print(f"Response time: {end_time - start_time:.2f}s")
    return full_response


# ë©”ì¸ ì…ë ¥ ë£¨í”„
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt, unsafe_allow_html=True)

    # 'ë‚ ê²ƒ(Raw Data)' 'ì…ë ¥' 'ë‹¨ê³„' 'ê°ì§€' (Phase 2)
    is_data_ingestion_phase = "Phase 2" in st.session_state.active_module

    # Contextual RAG ì‹¤í–‰
    rag_context = ""
    similar_precedents = []

    # 'ë©”ë‰´' 'ì„ íƒ'ì´ 'ì•„ë‹ˆ'ê±°ë‚˜, 'Phase 2' 'ë°ì´í„°' 'ì…ë ¥'ì´ 'ì•„ë‹' 'ë•Œ'ë§Œ 'RAG' 'ì‹¤í–‰'
    if not _is_menu_input(prompt) and not is_data_ingestion_phase:

        # 'ì»¨í…ìŠ¤íŠ¸' 'ì¿¼ë¦¬' 'ìƒì„±'
        contextual_query = (
            f"í˜„ì¬ í™œì„±í™”ëœ ëª¨ë“ˆ: {st.session_state.active_module}. "
            f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
        )

        with st.spinner("ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... (Dual RAG: íŒë¡€/ë²•ë ¹)..."):
            # 1. ë²•ë ¹ ê²€ìƒ‰ (S-RAG)
            if "statutes" in st.session_state and st.session_state.statutes:
                similar_statutes = find_similar_items(
                    contextual_query,
                    st.session_state.statutes,
                    st.session_state.s_embeddings,
                    top_k=3,
                    threshold=0.75,
                )
                if similar_statutes:
                    s_texts = [
                        f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n"
                        f"{c.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n"
                        for c in similar_statutes
                    ]
                    rag_context += (
                        "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n"
                        + "\n".join(s_texts)
                    )

            # 2. íŒë¡€ ê²€ìƒ‰ (P-RAG)
            if "precedents" in st.session_state and st.session_state.precedents:
                similar_precedents = find_similar_items(
                    contextual_query,
                    st.session_state.precedents,
                    st.session_state.p_embeddings,
                    top_k=5,
                    threshold=0.75,
                )
                if similar_precedents:
                    p_texts = [
                        f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n"
                        f"{c.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n"
                        for c in similar_precedents
                    ]
                    rag_context += (
                        "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n"
                        + "\n".join(p_texts)
                    )

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„±
    final_prompt = f"{prompt}\n{rag_context}"
    current_response = stream_and_store_response(
        st.session_state.chat,
        final_prompt
    )

    # [â˜…í•µì‹¬ ìˆ˜ì •â˜…] íŒë¡€ ì‹œê°í™” ë° ì›ë¬¸ ë³´ê¸° ê¸°ëŠ¥ (JSONL ê¸°ë°˜)
    clean_response = re.sub(
        "<[^<]+?>",
        "",
        current_response
    )  # 'HTML' 'ì“°ë ˆê¸°' 'ì œê±°'

    if _is_final_report(clean_response) and similar_precedents:
        q_title = _query_title(prompt)
        st.markdown(
            f"**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n"
            f"* ê²€ìƒ‰ ì¿¼ë¦¬: `[{q_title}]`\n"
        )

        for case_data in similar_precedents[:3]:  # 'ìƒìœ„' 3ê°œë§Œ 'ì‹œê°í™”'
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            sim_pct = int(round(case_data["similarity"] * 100))
            title = case_data.get("title", "ì œëª© ì—†ìŒ")
            case_no = case_data.get("case_no", case_data.get("id", ""))
            court = case_data.get("court", "")
            date = case_data.get("date", "")
            url = case_data.get("url")
            full_text = case_data.get(
                "full_text",
                case_data.get("raw_text")
            )  # 'ì „ë¬¸' ë˜ëŠ” 'TXT' 'í´ë°±'

            label = f"íŒë¡€ [{title}]"
            if court and case_no:
                label += f" â€” {court} {case_no}"

            # ìš”ì•½ ì¹´ë“œ
            summary = case_data.get("rag_index", "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
            if len(summary) > 200:
                summary = summary[:197] + "..."

            action_link = f"[ğŸ”— ì›ë¬¸ ë§í¬ ë³´ê¸°]({url})" if url else ""

            item_md = (
                f"* **{label}**\n"
                f"  - ì„ ê³ : {date} | ìœ ì‚¬ë„: {sim_pct}% | {action_link}\n"
                f"  - ë‚´ìš© ìš”ì•½ (RAG Index): {summary}"
            )
            st.markdown(item_md)

            # ì›ë¬¸ ë³´ê¸°
            if full_text:
                with st.expander("ğŸ“„ íŒë¡€ ì „ë¬¸ ë³´ê¸°"):
                    st.text(full_text)

    elif (
        _is_final_report(clean_response)
        and not _is_menu_input(prompt)
        and not similar_precedents
    ):
        st.info(
            "â„¹ï¸ ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.75)"
        )
