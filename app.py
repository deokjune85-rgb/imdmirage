# ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1 â€” Auto-Analysis Mode + Dual RAG (ì‚¬ì „ ì„ë² ë”©)
# ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1 - ì™„ì „ ìˆ˜ì •íŒ

import streamlit as st
import google.generativeai as genai
@@ -73,6 +73,7 @@
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text: str, task_type: str = "retrieval_document"):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
clean_text = text.replace("\n", " ").strip()
if not clean_text:
return None
@@ -87,6 +88,33 @@ def embed_text(text: str, task_type: str = "retrieval_document"):
print(f"[Embedding error] {e}")
return None

def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.5):
    """ìœ ì‚¬ë„ ê²€ìƒ‰"""
    if not items or not embeddings:
        return []

    try:
        q_emb = embed_text(query_text, task_type="retrieval_query")
        if q_emb is None:
            return []

        sims = np.dot(np.array(embeddings), np.array(q_emb))
        idxs = np.argsort(sims)[::-1][:top_k]

        results = []
        for i in idxs:
            score = float(sims[i])
            if score < threshold:
                continue
            item = items[i].copy()
            item["similarity"] = score
            results.append(item)

        return results
    except Exception as e:
        print(f"[RAG Error] {e}")
        return []

# ---------------------------------------
# 2-1. ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”© ë¡œë“œ
# ---------------------------------------
@@ -98,49 +126,54 @@ def load_precomputed_embeddings():
precedent_items = []
precedent_embeddings = []

    # ë²•ë ¹ ë¡œë“œ
    if os.path.exists("statutes_data.txt"):
        with open("statutes_data.txt", "r", encoding="utf-8") as f:
            content = f.read()
        
        parts = re.split(r"\s*---END OF STATUTE---\s*", content)
        for p in parts:
            p = p.strip()
            if not p:
                continue
            emb = embed_text(p)
            if emb:
                statute_items.append({"rag_index": p, "raw_text": p})
                statute_embeddings.append(emb)
        
        print(f"[RAG] âœ… ë²•ë ¹ ë¡œë“œ: {len(statute_items)}ê°œ")
    
    # íŒë¡€ ë¡œë“œ
    if os.path.exists("precedents_data.jsonl"):
        with open("precedents_data.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    txt = obj.get("rag_index", "")
                    if txt:
                        emb = embed_text(txt)
                        if emb:
                            precedent_items.append(obj)
                            precedent_embeddings.append(emb)
                except:
    try:
        # ë²•ë ¹ ë¡œë“œ
        if os.path.exists("statutes_data.txt"):
            with open("statutes_data.txt", "r", encoding="utf-8") as f:
                content = f.read()
            
            parts = re.split(r"\s*---END OF STATUTE---\s*", content)
            for p in parts:
                p = p.strip()
                if not p:
continue
                emb = embed_text(p)
                if emb:
                    statute_items.append({"rag_index": p, "raw_text": p})
                    statute_embeddings.append(emb)
            
            print(f"[RAG] ë²•ë ¹ ë¡œë“œ: {len(statute_items)}ê°œ")

        print(f"[RAG] âœ… íŒë¡€ ë¡œë“œ: {len(precedent_items)}ê°œ")
        # íŒë¡€ ë¡œë“œ
        if os.path.exists("precedents_data.jsonl"):
            with open("precedents_data.jsonl", "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        txt = obj.get("rag_index", "")
                        if txt:
                            emb = embed_text(txt)
                            if emb:
                                precedent_items.append(obj)
                                precedent_embeddings.append(emb)
                    except:
                        continue
            
            print(f"[RAG] íŒë¡€ ë¡œë“œ: {len(precedent_items)}ê°œ")
    
    except Exception as e:
        print(f"[RAG ë¡œë”© ì—ëŸ¬] {e}")

return statute_items, statute_embeddings, precedent_items, precedent_embeddings

# ---------------------------------------
# 3. PDF ì²˜ë¦¬ í•¨ìˆ˜
# ---------------------------------------
def extract_text_from_pdf(uploaded_file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
try:
pdf_reader = PyPDF2.PdfReader(uploaded_file)
text = ""
@@ -158,6 +191,7 @@ def extract_text_from_pdf(uploaded_file):
return None

def analyze_case_file(pdf_text: str, model):
    """PDF ë‚´ìš© ìë™ ë¶„ì„"""
analysis_prompt = f"""
ë‹¤ìŒì€ ì‚¬ê±´ê¸°ë¡ PDFì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ì…ë‹ˆë‹¤. 

@@ -200,27 +234,29 @@ def analyze_case_file(pdf_text: str, model):
# 4. ê°ì¢… ìœ í‹¸ í•¨ìˆ˜
# ---------------------------------------
def _is_menu_input(s: str) -> bool:
    """ë©”ë‰´ ë²ˆí˜¸ ì…ë ¥ ê°ì§€"""
return bool(re.fullmatch(r"^\s*\d{1,2}(?:-\d{1,2})?\s*$", s))

def _is_reset_keyword(s: str) -> bool:
    """ì²˜ìŒìœ¼ë¡œ/ë©”ì¸/ì´ˆê¸°í™” í‚¤ì›Œë“œ ê°ì§€"""
    """ì´ˆê¸°í™” í‚¤ì›Œë“œ ê°ì§€"""
keywords = ["ì²˜ìŒ", "ë©”ì¸", "ì´ˆê¸°í™”", "reset", "ëŒì•„ê°€", "ì²˜ìŒìœ¼ë¡œ"]
return any(kw in s.lower() for kw in keywords)

def _is_final_report(txt: str) -> bool:
    """ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì—¬ë¶€ í™•ì¸"""
return "ì „ëµ ë¸Œë¦¬í•‘ ë³´ê³ ì„œ" in txt

def _query_title(prompt_text: str) -> str:
    """ì¿¼ë¦¬ ì œëª© ìƒì„±"""
return prompt_text[:67] + "..." if len(prompt_text) > 70 else prompt_text

def update_active_module(response_text: str):
    # Auto-Analysis Mode ê°ì§€ (ì—„ê²©í•œ ì¡°ê±´)
    """í™œì„± ëª¨ë“ˆ ì—…ë°ì´íŠ¸"""
if ("9." in response_text and "ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œ" in response_text) or \
("Auto-Analysis Modeë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤" in response_text):
st.session_state.active_module = "Auto-Analysis Mode"
return

    # ì¼ë°˜ ëª¨ë“ˆ í™œì„±í™”
m = re.search(r"'(.+?)' ëª¨ë“ˆì„ (?:ìµœì¢… )?í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
if m:
st.session_state.active_module = m.group(1).strip()
@@ -245,7 +281,7 @@ def update_active_module(response_text: str):
if "model" not in st.session_state:
try:
st.session_state.model = genai.GenerativeModel(
            "models/gemini-2.5-flash",
            "models/gemini-2.0-flash-exp",
system_instruction=SYSTEM_INSTRUCTION,
)
st.session_state.chat = st.session_state.model.start_chat(history=[])
@@ -280,127 +316,77 @@ def update_active_module(response_text: str):
with st.chat_message(role_name, avatar=avatar):
st.markdown(m["content"], unsafe_allow_html=True)

if st.session_state.messages:
    st.markdown(
        '<script>setTimeout(()=>{const el=window.parent.document.querySelector("section.main");if(el)el.scrollTop=el.scrollHeight},100)</script>',
        unsafe_allow_html=True
    )

# ---------------------------------------
# 8. PDF ì—…ë¡œë“œ UI (â˜…â˜…â˜… í•µì‹¬ ìˆ˜ì • â˜…â˜…â˜…)
# 8. PDF ì—…ë¡œë“œ UI
# ---------------------------------------
# ì¡°ê±´: active_moduleì´ ì •í™•íˆ "Auto-Analysis Mode"ì´ê³ , 9ë²ˆì„ ì…ë ¥í•œ ì§í›„ì¼ ë•Œë§Œ í‘œì‹œ
if st.session_state.get("active_module") == "Auto-Analysis Mode":
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ê°€ "9"ì¸ì§€ í™•ì¸
last_user_msg = None
for m in reversed(st.session_state.messages):
if m["role"] == "user":
last_user_msg = m["content"].strip()
break

    # 9ë²ˆ ì…ë ¥ ì§í›„ì—ë§Œ PDF UI í‘œì‹œ
if last_user_msg == "9":
st.markdown("---")

st.info("""
       **ğŸ“„ ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œë€?**
       
       PDF íŒŒì¼(íŒê²°ë¬¸, ê³ ì†Œì¥, ë‹µë³€ì„œ ë“±)ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ìë™ìœ¼ë¡œ:
        - âœ… ì‚¬ê±´ ë„ë©”ì¸ ë¶„ë¥˜ (í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬ ë“±)
        - âœ… í•µì‹¬ ì‚¬ì‹¤ê´€ê³„ 5ê°€ì§€ ì¶”ì¶œ
        - âœ… í™•ë³´ëœ ì¦ê±° ëª©ë¡ ì •ë¦¬
        - âœ… ì–‘ì¸¡ ì£¼ì¥ ìš”ì•½
        - ì‚¬ê±´ ë„ë©”ì¸ ë¶„ë¥˜ (í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬ ë“±)
        - í•µì‹¬ ì‚¬ì‹¤ê´€ê³„ 5ê°€ì§€ ì¶”ì¶œ
        - í™•ë³´ëœ ì¦ê±° ëª©ë¡ ì •ë¦¬
        - ì–‘ì¸¡ ì£¼ì¥ ìš”ì•½
       
       **ì²˜ë¦¬ ì‹œê°„:** ì•½ 1-3ë¶„ | **ìµœëŒ€ í¬ê¸°:** 50MB | **í˜•ì‹:** í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFë§Œ ê°€ëŠ¥
       """)

st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")

        col1, col2 = st.columns([3, 1])
        
        with col1:
            uploaded_file = st.file_uploader(
                "ì‚¬ê±´ê¸°ë¡ PDFë¥¼ ì„ íƒí•˜ì„¸ìš”",
                type=["pdf"],
                help="íŒê²°ë¬¸, ê³ ì†Œì¥, ë‹µë³€ì„œ, ì‚¬ê±´ê¸°ë¡ ë“±",
                label_visibility="collapsed"
            )
        
        with col2:
            if uploaded_file:
                st.metric("ìƒíƒœ", "âœ… ì¤€ë¹„ ì™„ë£Œ", delta="ì—…ë¡œë“œ ì™„ë£Œ")
            else:
                st.metric("ìƒíƒœ", "â³ ëŒ€ê¸° ì¤‘", delta="íŒŒì¼ ì„ íƒ")
        uploaded_file = st.file_uploader(
            "ì‚¬ê±´ê¸°ë¡ PDFë¥¼ ì„ íƒí•˜ì„¸ìš”",
            type=["pdf"],
            help="íŒê²°ë¬¸, ê³ ì†Œì¥, ë‹µë³€ì„œ, ì‚¬ê±´ê¸°ë¡ ë“±",
        )

if uploaded_file is not None:
file_size = uploaded_file.size / (1024 * 1024)
            
            with st.container():
                st.success(f"**íŒŒì¼ëª…:** {uploaded_file.name}  |  **í¬ê¸°:** {file_size:.1f}MB")
            st.success(f"**íŒŒì¼ëª…:** {uploaded_file.name}  |  **í¬ê¸°:** {file_size:.1f}MB")

if st.button("ğŸš€ ìë™ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
                with st.spinner("ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (30ì´ˆ~2ë¶„ ì†Œìš”)"):
                with st.spinner("ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
pdf_text = extract_text_from_pdf(uploaded_file)

if not pdf_text:
                        st.error("âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
st.stop()

                    st.success(f"âœ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(pdf_text):,} ê¸€ì)")
                    st.success(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(pdf_text):,} ê¸€ì)")

                with st.spinner("ğŸ§  AI ë¶„ì„ ì¤‘... (1-2ë¶„ ì†Œìš”)"):
                with st.spinner("ğŸ§  AI ë¶„ì„ ì¤‘..."):
analysis = analyze_case_file(pdf_text, st.session_state.model)

if not analysis:
                        st.error("âŒ ë¶„ì„ ì‹¤íŒ¨. PDF í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                        st.error("ë¶„ì„ ì‹¤íŒ¨. PDF í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
st.stop()

                st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
                st.success("ë¶„ì„ ì™„ë£Œ!")

with st.expander("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë³´ê¸°", expanded=True):
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.metric("ğŸ›ï¸ ë„ë©”ì¸", analysis["domain"])
                        st.metric("ğŸ“Œ ì„¸ë¶€ ë¶„ì•¼", analysis.get("subdomain", "ë¯¸ë¶„ë¥˜"))
                    st.markdown(f"**ë„ë©”ì¸:** {analysis['domain']}")
                    st.markdown(f"**ì„¸ë¶€ ë¶„ì•¼:** {analysis.get('subdomain', 'ë¯¸ë¶„ë¥˜')}")

                    with col_b:
                        st.metric("ğŸ“‹ í•µì‹¬ ì‚¬ì‹¤", f"{len(analysis.get('key_facts', []))}ê°œ")
                        st.metric("ğŸ“‚ ì¦ê±° í•­ëª©", f"{len(analysis.get('evidence', []))}ê°œ")
                    
                    st.markdown("---")
                    st.markdown("**ğŸ“Œ í•µì‹¬ ì‚¬ì‹¤ê´€ê³„**")
                    for i, fact in enumerate(analysis.get("key_facts", []), 1):
                    st.markdown("**í•µì‹¬ ì‚¬ì‹¤ê´€ê³„:**")
                    for i, fact in enumerate(analysis.get('key_facts', []), 1):
st.markdown(f"{i}. {fact}")

                    st.markdown("**ğŸ“‚ í™•ë³´ëœ ì¦ê±°**")
                    for i, ev in enumerate(analysis.get("evidence", []), 1):
                    st.markdown("**í™•ë³´ëœ ì¦ê±°:**")
                    for i, ev in enumerate(analysis.get('evidence', []), 1):
st.markdown(f"{i}. {ev}")

                    st.markdown("**âš–ï¸ ì–‘ì¸¡ ì£¼ì¥**")
st.info(f"**ìš°ë¦¬ ì¸¡:** {analysis.get('our_claim', '(ì •ë³´ ì—†ìŒ)')}")
st.warning(f"**ìƒëŒ€ ì¸¡:** {analysis.get('their_claim', '(ì •ë³´ ì—†ìŒ)')}")

                domain_map = {
                    "í˜•ì‚¬": "2",
                    "ë¯¼ì‚¬": "8",
                    "ê°€ì‚¬": "1",
                    "ì´í˜¼": "1",
                    "íŒŒì‚°": "3",
                    "í–‰ì •": "7",
                    "ì„¸ë¬´": "6",
                    "IP": "4",
                    "ì˜ë£Œ": "5",
                }
                
                domain_num = domain_map.get(analysis["domain"], "8")
                
                st.info(
                    f"ğŸ’¡ **ë‹¤ìŒ ë‹¨ê³„**\n\n"
                    f"ì´ ì‚¬ê±´ì€ **{analysis['domain']}** ì‚¬ê±´ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                    f"ê³„ì† ì§„í–‰í•˜ë ¤ë©´ ì•„ë˜ ì±„íŒ…ì°½ì— **{domain_num}**ì„ ì…ë ¥í•˜ì„¸ìš”."
                )
                
st.session_state["auto_analysis"] = analysis
st.session_state["pdf_text"] = pdf_text

@@ -412,12 +398,8 @@ def update_active_module(response_text: str):
if "auto_analysis" in st.session_state and st.session_state.get("active_module") != "Auto-Analysis Mode":
auto_data = st.session_state["auto_analysis"]

    st.success(
        "ğŸ’¡ **ìë™ ë¶„ì„ ê²°ê³¼ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!**\n\n"
        "AIê°€ ìë™ìœ¼ë¡œ í•´ë‹¹ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ì™„ì „í•œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    )
    st.success("AIê°€ ìë™ìœ¼ë¡œ í•´ë‹¹ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ì™„ì „í•œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    # ìë™ ëª¨ë“ˆ ì‹¤í–‰
domain_map = {
"í˜•ì‚¬": "2",
"ë¯¼ì‚¬": "8",
@@ -432,7 +414,6 @@ def update_active_module(response_text: str):

domain_num = domain_map.get(auto_data["domain"], "8")

    # ìë™ ì…ë ¥ ë©”ì‹œì§€ ìƒì„±
auto_input = f"""
[ìë™ ì¶”ì¶œëœ ì‚¬ê±´ ì •ë³´]

@@ -450,25 +431,17 @@ def update_active_module(response_text: str):
ìƒëŒ€ë°© ì£¼ì¥:
{auto_data.get('their_claim', '(ì •ë³´ ì—†ìŒ)')}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {domain_num}ë²ˆ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ì™„ì „í•œ ì „ëµ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì „í•œ ì „ëµ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
"""

    # ë©”ì‹œì§€ ì¶”ê°€
st.session_state.messages.append({"role": "user", "content": f"ìë™ ë¶„ì„ ì™„ë£Œ. {domain_num}ë²ˆ ëª¨ë“ˆ ì‹¤í–‰"})

    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(f"**ìë™ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ {domain_num}ë²ˆ ëª¨ë“ˆì„ ì‹¤í–‰í•©ë‹ˆë‹¤.**")
    
    # AIì—ê²Œ ì „ì†¡
    with st.spinner("ì™„ì „í•œ ì „ëµ ë³´ê³ ì„œ ìƒì„± ì¤‘... (1-2ë¶„ ì†Œìš”)"):
    with st.spinner("ì™„ì „í•œ ì „ëµ ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
try:
            # 1ë‹¨ê³„: ë„ë©”ì¸ ë²ˆí˜¸ ì…ë ¥
resp1 = st.session_state.chat.send_message(domain_num)
st.session_state.messages.append({"role": "Architect", "content": resp1.text})

            # í˜•ì‚¬ì¸ ê²½ìš° 2-1 ìë™ ì…ë ¥
if domain_num == "2":
                # ì„¸ë¶€ ë¶„ì•¼ ë§¤í•‘
subdomain_map = {
"ë§ˆì•½": "2-1",
"ì„±ë²”ì£„": "2-2",
@@ -484,7 +457,6 @@ def update_active_module(response_text: str):
resp2 = st.session_state.chat.send_message(subdomain_num)
st.session_state.messages.append({"role": "Architect", "content": resp2.text})

            # 2ë‹¨ê³„: ìë™ ì…ë ¥ ë°ì´í„° ì „ì†¡
resp3 = st.session_state.chat.send_message(auto_input)

with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
@@ -495,30 +467,27 @@ def update_active_module(response_text: str):
except Exception as e:
st.error(f"ìë™ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # ìë™ ë¶„ì„ ë°ì´í„° ì‚­ì œ
del st.session_state["auto_analysis"]
    
    st.markdown("---")
    st.rerun()

# ---------------------------------------
# 10. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜
# ---------------------------------------
def stream_and_store_response(chat_session, prompt_to_send: str, spinner_text: str = "Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
def stream_and_store_response(chat_session, prompt_to_send: str):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
full_response = ""
    start_time = time.time()

with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
placeholder = st.empty()
try:
            with st.spinner(spinner_text):
                stream = chat_session.send_message(prompt_to_send, stream=True)
                for chunk in stream:
                    if not getattr(chunk, "parts", None):
                        full_response = "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µì´ ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.]"
                        placeholder.error(full_response)
                        break
                    full_response += chunk.text
                    placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
            stream = chat_session.send_message(prompt_to_send, stream=True)
            for chunk in stream:
                if not getattr(chunk, "parts", None):
                    full_response = "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.]"
                    placeholder.error(full_response)
                    break
                full_response += chunk.text
                placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
placeholder.markdown(full_response, unsafe_allow_html=True)
except Exception as e:
full_response = f"[ì¹˜ëª…ì  ì˜¤ë¥˜: {e}]"
@@ -527,22 +496,19 @@ def stream_and_store_response(chat_session, prompt_to_send: str, spinner_text: s
st.session_state.messages.append({"role": "Architect", "content": full_response})
update_active_module(full_response)

    end_time = time.time()
    print(f"[LLM] ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}s")
return full_response

# ---------------------------------------
# 11. ë©”ì¸ ì…ë ¥ ë£¨í”„
# ---------------------------------------
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤"):

    # â˜…â˜…â˜… 1. ì´ˆê¸°í™” í‚¤ì›Œë“œ ê°ì§€ (ìµœìš°ì„ ) â˜…â˜…â˜…
if _is_reset_keyword(prompt):
st.session_state.active_module = "Phase 0"
st.session_state.messages.append({"role": "user", "content": prompt})

with st.chat_message("Client", avatar="ğŸ‘¤"):
            st.markdown(prompt, unsafe_allow_html=True)
            st.markdown(prompt)

reset_response = "ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. Phase 0ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤."

@@ -551,7 +517,6 @@ def stream_and_store_response(chat_session, prompt_to_send: str, spinner_text: s

st.session_state.messages.append({"role": "Architect", "content": reset_response})

        # Phase 0 ë©”ë‰´ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
try:
init_prompt = "ì‹œìŠ¤í…œ ê°€ë™. Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
resp = st.session_state.chat.send_message(init_prompt)
@@ -562,30 +527,35 @@ def stream_and_store_response(chat_session, prompt_to_send: str, spinner_text: s
st.session_state.messages.append({"role": "Architect", "content": init_text})
st.rerun()

    # â˜…â˜…â˜… 2. 9ë²ˆ ì…ë ¥ ê°ì§€ â˜…â˜…â˜…
if prompt.strip() == "9":
st.session_state.active_module = "Auto-Analysis Mode"
st.session_state.messages.append({"role": "user", "content": prompt})

with st.chat_message("Client", avatar="ğŸ‘¤"):
            st.markdown(prompt, unsafe_allow_html=True)
            st.markdown(prompt)

        response_text = stream_and_store_response(st.session_state.chat, prompt)
        stream_and_store_response(st.session_state.chat, prompt)
st.rerun()

st.session_state.messages.append({"role": "user", "content": prompt})
with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt, unsafe_allow_html=True)
        st.markdown(prompt)

is_data_ingestion_phase = "Phase 2" in (st.session_state.active_module or "")

    # â˜…â˜…â˜… RAG ì´ˆê¸°í™” (ì‚¬ì „ ì„ë² ë”© ì‚¬ìš©) - ê²½ê³  ì œê±° â˜…â˜…â˜…
if (not st.session_state.statutes) and (not st.session_state.precedents):
        s_data, s_emb, p_data, p_emb = load_precomputed_embeddings()
        st.session_state.statutes = s_data
        st.session_state.s_embeddings = s_emb
        st.session_state.precedents = p_data
        st.session_state.p_embeddings = p_emb
        try:
            s_data, s_emb, p_data, p_emb = load_precomputed_embeddings()
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb
            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb
        except Exception as e:
            print(f"[RAG ì´ˆê¸°í™” ì‹¤íŒ¨] {e}")
            st.session_state.statutes = []
            st.session_state.s_embeddings = []
            st.session_state.precedents = []
            st.session_state.p_embeddings = []

rag_context = ""
similar_precedents = []
@@ -640,13 +610,13 @@ def stream_and_store_response(chat_session, prompt_to_send: str, spinner_text: s

label = f"íŒë¡€ [{title}]"
if court and case_no:
                label += f" â€” {court} {case_no}"
                label += f" - {court} {case_no}"

summary = case_data.get("rag_index", "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
if len(summary) > 200:
summary = summary[:197] + "..."

            link_md = f"[ğŸ”— ì›ë¬¸ ë§í¬ ë³´ê¸°]({url})" if url else ""
            link_md = f"[ì›ë¬¸ ë§í¬]({url})" if url else ""

md = f"* **{label}**\n  - ì„ ê³ : {date} | ìœ ì‚¬ë„: {sim_pct}% | {link_md}\n  - ë‚´ìš© ìš”ì•½: {summary}"
st.markdown(md)
@@ -656,4 +626,4 @@ def stream_and_store_response(chat_session, prompt_to_send: str, spinner_text: s
st.text(full_text)

elif _is_final_report(clean_response) and not similar_precedents:
        st.info("â„¹ï¸ ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.55)")
        st.info("ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.55)")
