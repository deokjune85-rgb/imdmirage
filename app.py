import streamlit as st
import google.generativeai as genai
import os
import re
import json
import numpy as np
import PyPDF2
import time

# ---------------------------------------
# 0. ì‹œìŠ¤í…œ ì„¤ì •
# ---------------------------------------
st.set_page_config(
    page_title="Veritas Engine 8.1 | Legal Architect",
    page_icon="âš–ï¸",
    layout="centered"
)

# API í‚¤ ì„¤ì • (Streamlit Secretsì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
# st.secrets["GOOGLE_API_KEY"] ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
if "GOOGLE_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
else:
    st.error("Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

SYSTEM_INSTRUCTION = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë²•ë¥  ì „ë¬¸ê°€ì´ì ì „ëµê°€ì¸ 'Veritas Architect'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ì‚¬ê±´ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë²•ë¦¬ì  ê·¼ê±°(ì¡°ë¬¸, íŒë¡€)ì— ê¸°ë°˜í•œ ëª…í™•í•œ ì „ëµì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
"""

EMBEDDING_MODEL_NAME = "models/text-embedding-004"

# ---------------------------------------
# 1. ì„ë² ë”© ë° RAG ê²€ìƒ‰ í•¨ìˆ˜
# ---------------------------------------
def embed_text(text: str, task_type: str = "retrieval_document"):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    clean_text = text.replace("\n", " ").strip()
    if not clean_text:
        return None
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type
        )
        return result['embedding']
    except Exception as e:
        print(f"[Embedding error] {e}")
        return None

def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.5):
    """ìœ ì‚¬ë„ ê²€ìƒ‰"""
    if not items or not embeddings:
        return []

    try:
        q_emb = embed_text(query_text, task_type="retrieval_query")
        if q_emb is None:
            return []

        sims = np.dot(np.array(embeddings), np.array(q_emb))
        idxs = np.argsort(sims)[::-1][:top_k]

        results = []
        for i in idxs:
            score = float(sims[i])
            if score < threshold:
                continue
            item = items[i].copy()
            item["similarity"] = score
            results.append(item)

        return results
    except Exception as e:
        print(f"[RAG Error] {e}")
        return []

# ---------------------------------------
# 2. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì‚¬ì „ ì„ë² ë”©)
# ---------------------------------------
@st.cache_resource
def load_precomputed_embeddings():
    statute_items = []
    statute_embeddings = []
    precedent_items = []
    precedent_embeddings = []

    try:
        # ë²•ë ¹ ë¡œë“œ
        if os.path.exists("statutes_data.txt"):
            with open("statutes_data.txt", "r", encoding="utf-8") as f:
                content = f.read()
            
            parts = re.split(r"\s*---END OF STATUTE---\s*", content)
            for p in parts:
                p = p.strip()
                if not p:
                    continue
                # ì‹¤ì œë¡œëŠ” ì„ë² ë”© ê°’ì´ ì €ì¥ëœ íŒŒì¼ì„ ë¡œë“œí•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ìƒì„± (ì‹œê°„ ì†Œìš”ë¨)
                # ì—¬ê¸°ì„œëŠ” ë°ëª¨ìš©ìœ¼ë¡œ ì‹¤ì‹œê°„ ìƒì„± ë¡œì§ ìœ ì§€
                emb = embed_text(p)
                if emb:
                    statute_items.append({"rag_index": p, "raw_text": p})
                    statute_embeddings.append(emb)
            
            print(f"[RAG] âœ… ë²•ë ¹ ë¡œë“œ: {len(statute_items)}ê°œ")
        
        # íŒë¡€ ë¡œë“œ
        if os.path.exists("precedents_data.jsonl"):
            with open("precedents_data.jsonl", "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        txt = obj.get("rag_index", "")
                        if txt:
                            emb = embed_text(txt)
                            if emb:
                                precedent_items.append(obj)
                                precedent_embeddings.append(emb)
                    except:
                        continue
            
            print(f"[RAG] âœ… íŒë¡€ ë¡œë“œ: {len(precedent_items)}ê°œ")
            
    except Exception as e:
        print(f"[RAG ë¡œë”© ì—ëŸ¬] {e}")

    return statute_items, statute_embeddings, precedent_items, precedent_embeddings

# ---------------------------------------
# 3. PDF ì²˜ë¦¬ í•¨ìˆ˜
# ---------------------------------------
def extract_text_from_pdf(uploaded_file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDF ì½ê¸° ì˜¤ë¥˜: {e}")
        return None

def analyze_case_file(pdf_text: str, model):
    """PDF ë‚´ìš© ìë™ ë¶„ì„"""
    analysis_prompt = f"""
    ë‹¤ìŒì€ ì‚¬ê±´ê¸°ë¡ PDFì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ì…ë‹ˆë‹¤. 
    ë‚´ìš©ì„ ì •ë°€ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
    
    {{
        "domain": "í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬ ì¤‘ íƒ1",
        "subdomain": "ì„¸ë¶€ ì£„ëª… ë˜ëŠ” ìŸì  (ì˜ˆ: ì‚¬ê¸°, ì†í•´ë°°ìƒ)",
        "key_facts": ["í•µì‹¬ ì‚¬ì‹¤ê´€ê³„1", "í•µì‹¬ ì‚¬ì‹¤ê´€ê³„2", ... (5ê°œ ë‚´ì™¸)],
        "evidence": ["í™•ë³´ëœ ì¦ê±°1", "í™•ë³´ëœ ì¦ê±°2", ...],
        "our_claim": "ìš°ë¦¬ ì¸¡ í•µì‹¬ ì£¼ì¥ ìš”ì•½",
        "their_claim": "ìƒëŒ€ë°© í•µì‹¬ ì£¼ì¥ ìš”ì•½"
    }}

    [ì‚¬ê±´ ë‚´ìš©]
    {pdf_text[:10000]}
    """
    try:
        response = model.generate_content(analysis_prompt, generation_config={"response_mime_type": "application/json"})
        return json.loads(response.text)
    except Exception as e:
        st.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None

# ---------------------------------------
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ---------------------------------------
def _is_reset_keyword(s: str) -> bool:
    """ì´ˆê¸°í™” í‚¤ì›Œë“œ ê°ì§€"""
    keywords = ["ì²˜ìŒ", "ë©”ì¸", "ì´ˆê¸°í™”", "reset", "ëŒì•„ê°€", "ì²˜ìŒìœ¼ë¡œ"]
    return any(kw in s.lower() for kw in keywords)

def _is_final_report(txt: str) -> bool:
    return "ì „ëµ ë¸Œë¦¬í•‘ ë³´ê³ ì„œ" in txt

def update_active_module(response_text: str):
    """í™œì„± ëª¨ë“ˆ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    if ("9." in response_text and "ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œ" in response_text) or \
       ("Auto-Analysis Modeë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤" in response_text):
        st.session_state.active_module = "Auto-Analysis Mode"
        return

    m = re.search(r"'(.+?)' ëª¨ë“ˆì„ (?:ìµœì¢… )?í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
    if m:
        st.session_state.active_module = m.group(1).strip()

def stream_and_store_response(chat_session, prompt_to_send: str):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ë° ì €ì¥"""
    full_response = ""
    
    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        placeholder = st.empty()
        try:
            with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
                stream = chat_session.send_message(prompt_to_send, stream=True)
                for chunk in stream:
                    if not getattr(chunk, "parts", None):
                        full_response = "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µì´ ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.]"
                        placeholder.error(full_response)
                        break
                    text_chunk = chunk.text
                    full_response += text_chunk
                    placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
            
            placeholder.markdown(full_response, unsafe_allow_html=True)
            
        except Exception as e:
            full_response = f"[ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}]"
            placeholder.error(full_response)

    st.session_state.messages.append({"role": "Architect", "content": full_response})
    update_active_module(full_response)
    return full_response

# ---------------------------------------
# 5. ë©”ì¸ ì•± ë¡œì§
# ---------------------------------------

# ì„¸ì…˜ ì´ˆê¸°í™”
if "model" not in st.session_state:
    try:
        st.session_state.model = genai.GenerativeModel(
            "models/gemini-2.0-flash-exp", # ìµœì‹  ëª¨ë¸ ì‚¬ìš©
            system_instruction=SYSTEM_INSTRUCTION,
        )
        st.session_state.chat = st.session_state.model.start_chat(history=[])
        st.session_state.messages = []
        st.session_state.active_module = "Phase 0"
        
        # ì´ˆê¸° ì¸ì‚¬ë§
        init_msg = "Veritas Engine 8.1 ê°€ë™. ë²•ë¥  ì „ëµ ìˆ˜ë¦½ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        st.session_state.messages.append({"role": "Architect", "content": init_msg})
        
        # RAG ë°ì´í„° ë¡œë“œ
        s_data, s_emb, p_data, p_emb = load_precomputed_embeddings()
        st.session_state.statutes = s_data
        st.session_state.s_embeddings = s_emb
        st.session_state.precedents = p_data
        st.session_state.p_embeddings = p_emb
        
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¶œë ¥
for m in st.session_state.messages:
    role = m["role"]
    avatar = "ğŸ›¡ï¸" if role == "Architect" else "ğŸ‘¤"
    with st.chat_message(role, avatar=avatar):
        st.markdown(m["content"], unsafe_allow_html=True)

# í™”ë©´ ìŠ¤í¬ë¡¤ í•˜ë‹¨ ê³ ì •
st.markdown('<script>window.scrollTo(0, document.body.scrollHeight);</script>', unsafe_allow_html=True)


# ---------------------------------------
# 6. PDF ì—…ë¡œë“œ UI (Auto-Analysis Mode)
# ---------------------------------------
if st.session_state.get("active_module") == "Auto-Analysis Mode":
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ 9ë²ˆ ì„ íƒì¸ ê²½ìš°ì—ë§Œ í‘œì‹œ
    last_user_msg = ""
    for m in reversed(st.session_state.messages):
        if m["role"] == "user":
            last_user_msg = m["content"].strip()
            break
            
    if last_user_msg == "9":
        st.markdown("---")
        st.info("""
        **ğŸ“„ ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œ**
        íŒê²°ë¬¸, ê³ ì†Œì¥ ë“± PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ìë™ìœ¼ë¡œ ìŸì ì„ ì¶”ì¶œí•˜ê³  ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
        """)
        
        uploaded_file = st.file_uploader("ì‚¬ê±´ê¸°ë¡ PDF ì„ íƒ", type=["pdf"])
        
        if uploaded_file:
            if st.button("ğŸš€ ìë™ ë¶„ì„ ì‹œì‘", type="primary"):
                with st.spinner("í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¶„ì„ ì¤‘..."):
                    pdf_text = extract_text_from_pdf(uploaded_file)
                    if pdf_text:
                        analysis = analyze_case_file(pdf_text, st.session_state.model)
                        if analysis:
                            # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                            with st.expander("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½", expanded=True):
                                st.markdown(f"**ë„ë©”ì¸:** {analysis.get('domain')}")
                                st.markdown("**í•µì‹¬ ì‚¬ì‹¤:**")
                                for f in analysis.get('key_facts', []):
                                    st.markdown(f"- {f}")
                            
                            # ìë™ ì§„í–‰ ë¡œì§
                            st.session_state["auto_analysis"] = analysis
                            
                            # ë‹¤ìŒ ë‹¨ê³„ ìë™ íŠ¸ë¦¬ê±° ë©”ì‹œì§€ ìƒì„±
                            domain_map = {"í˜•ì‚¬": "2", "ë¯¼ì‚¬": "8", "ì´í˜¼": "1"}
                            domain_num = domain_map.get(analysis.get("domain", ""), "8")
                            
                            auto_prompt = f"""
                            [ìë™ ë¶„ì„ ë°ì´í„°]
                            ë„ë©”ì¸: {analysis.get('domain')}
                            ì‚¬ì‹¤ê´€ê³„: {analysis.get('key_facts')}
                            
                            ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {domain_num}ë²ˆ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ì „ëµì„ ì œì‹œí•˜ë¼.
                            """
                            
                            # ì±—ë´‡ì—ê²Œ ìë™ ì „ì†¡ íš¨ê³¼
                            st.session_state.messages.append({"role": "user", "content": "PDF ë¶„ì„ ì™„ë£Œ. ìë™ ì „ëµ ìˆ˜ë¦½ ì‹œì‘."})
                            stream_and_store_response(st.session_state.chat, auto_prompt)
                            st.rerun()

# ---------------------------------------
# 7. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# ---------------------------------------
if prompt := st.chat_input("ëª…ë ¹ ë˜ëŠ” ë‚´ìš©ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    # 1. ì´ˆê¸°í™” ê°ì§€
    if _is_reset_keyword(prompt):
        st.session_state.active_module = "Phase 0"
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state.chat = st.session_state.model.start_chat(history=[]) # ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”
        
        reset_msg = "ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤."
        st.session_state.messages.append({"role": "Architect", "content": reset_msg})
        
        # ë©”ì¸ ë©”ë‰´ í˜¸ì¶œ
        stream_and_store_response(st.session_state.chat, "ì‹œìŠ¤í…œ ë©”ë‰´ë¥¼ ì¶œë ¥í•˜ë¼.")
        st.rerun()

    # 2. ì¼ë°˜ ëŒ€í™”
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    # 3. 9ë²ˆ(PDF ëª¨ë“œ) ì§„ì… ê°ì§€
    if prompt.strip() == "9":
        st.session_state.active_module = "Auto-Analysis Mode"
        response_text = stream_and_store_response(st.session_state.chat, prompt)
        st.rerun()
    else:
        # ì¼ë°˜ ì‘ë‹µ ìƒì„±
        stream_and_store_response(st.session_state.chat, prompt)
