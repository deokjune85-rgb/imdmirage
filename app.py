# =====================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.6 â€” Contextual Dual RAG (JSONL/TXT Hybrid) + Relay Mechanism
# =====================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time
import json  # JSONL ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • ë° CSS ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.6", page_icon="ğŸ›¡ï¸", layout="centered")

custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    font-size: 16px !important;
    line-height: 1.7 !important;
}
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
}
strong, b { font-weight: 700; }
.fadein { animation: fadeInText 0.5s ease-in-out forwards; opacity: 0; }
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}
[data-testid="stChatMessageContent"] { font-size: 16px !important; }
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.6")
st.warning("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- 3. API í‚¤ ë° RAG ì—”ì§„ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
        raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
    st.stop()

EMBEDDING_MODEL_NAME = "models/text-embedding-004"


def embed_text(text, task_type="retrieval_document"):
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜."""
    try:
        clean_text = text.replace("\n", " ").strip()
        if not clean_text:
            return None
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type,
        )
        return result["embedding"]
    except Exception as e:
        print(f"Embedding error: {e}")
        return None


@st.cache_data(show_spinner=True)
def load_and_embed_data(file_path, separator_regex=None):
    """
    JSONL / TXT ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” RAG ë¡œë”.
    - JSONL: í•œ ì¤„ë‹¹ í•˜ë‚˜ì˜ JSON, 'rag_index' í•„ë“œ ê¸°ë°˜ ì„ë² ë”©
    - TXT: separator_regexë¡œ êµ¬ë¶„ëœ ë©ì–´ë¦¬ë§ˆë‹¤ ì„ë² ë”©
    """
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return [], []

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return [], []

    if not content.strip():
        return [], []

    data_items = []
    embeddings = []

    # JSONL íŒŒì¼
    if file_path.endswith(".jsonl"):
        for line in content.strip().split("\n"):
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
            except json.JSONDecodeError:
                continue

            text_to_embed = item.get("rag_index")
            if text_to_embed:
                ebd = embed_text(text_to_embed, task_type="retrieval_document")
                if ebd:
                    embeddings.append(ebd)
                    data_items.append(item)

    # TXT íŒŒì¼
    elif separator_regex:
        chunks = re.split(separator_regex, content)
        raw_items = [p.strip() for p in chunks if p and p.strip()]
        for item_text in raw_items:
            ebd = embed_text(item_text, task_type="retrieval_document")
            if ebd:
                embeddings.append(ebd)
                data_items.append(
                    {
                        "rag_index": item_text,
                        "raw_text": item_text,
                    }
                )

    print(f"[RAG] Loaded {len(data_items)} items from {file_path}.")
    return data_items, embeddings


def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.50):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰."""
    if not embeddings or not items:
        return []

    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None:
        return []

    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]

    results = []
    for i in idxs:
        if float(sims[i]) >= threshold:
            result_item = items[i].copy()
            result_item["similarity"] = float(sims[i])
            results.append(result_item)
    return results


# --- ìœ í‹¸ í•¨ìˆ˜ë“¤ ---
def _is_menu_input(s: str) -> bool:
    """ì…ë ¥ì´ ë‹¨ìˆœ ìˆ«ì ë©”ë‰´ ì„ íƒì¸ì§€ íŒë‹¨."""
    return bool(re.fullmatch(r"^\s*\d{1,2}(?:-\d{1,2})?\s*$", s))


def _is_final_report(txt: str) -> bool:
    """ì‘ë‹µì´ ìµœì¢… ë³´ê³ ì„œ í˜•ì‹ì¸ì§€ íŒë³„."""
    return "ì „ëµ ë¸Œë¦¬í•‘ ë³´ê³ ì„œ" in txt


def _query_title(prompt_text: str) -> str:
    """RAG ì‹œê°í™”ì— ì‚¬ìš©í•  ì§§ì€ ì¿¼ë¦¬ ì œëª©."""
    if len(prompt_text) > 70:
        return prompt_text[:67] + "..."
    return prompt_text


def update_active_module(response_text: str):
    """ì‘ë‹µì—ì„œ í˜„ì¬ í™œì„± ëª¨ë“ˆ ì´ë¦„ ì¶”ì¶œ."""
    match = re.search(r"\[(.+?)\]' ëª¨ë“ˆì„ í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
    if match:
        st.session_state.active_module = match.group(1).strip()
    elif "Phase 0" in response_text:
        st.session_state.active_module = "Phase 0 (ë„ë©”ì¸ ì„ íƒ)"


# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì ë¡œë“œ ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if len(SYSTEM_INSTRUCTION) < 100:
        raise ValueError("System prompt is too short.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì½”ì–´(system_prompt.txt) ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()

# --- 5. ëª¨ë¸ ë° RAG ì´ˆê¸°í™” ---
if "model" not in st.session_state:
    try:
        st.session_state.model = genai.GenerativeModel(
            "models/gemini-2.5-flash",
            system_instruction=SYSTEM_INSTRUCTION,
        )

        # ë“€ì–¼ RAG ì´ˆê¸°í™” (JSONL + TXT, í´ë°± ê²½ê³  ì œê±° ë²„ì „)
        with st.spinner("ë¶„ì„ ì—”ì§„(Dual RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            # 1. íŒë¡€ ë°ì´í„° (JSONL â†’ ì—†ìœ¼ë©´ TXT)
            p_data, p_emb = load_and_embed_data("precedents_data.jsonl")
            if not p_data:
                p_data, p_emb = load_and_embed_data(
                    "precedents_data.txt",
                    r"\s*---END OF PRECEDENT---\s*",
                )

            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb

            # 2. ë²•ë ¹ ë°ì´í„° (TXT)
            s_data, s_emb = load_and_embed_data(
                "statutes_data.txt",
                r"\s*---END OF STATUTE---\s*",
            )
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb

        st.session_state.active_module = "ì´ˆê¸° ìƒíƒœ (ë¯¸ì •ì˜)"

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()


# --- 6. ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    with st.spinner("Architect ì‹œìŠ¤í…œ ê°€ë™..."):
        try:
            initial_prompt = "ì‹œìŠ¤í…œ ê°€ë™. 'ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ì—¬ Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
            chat = st.session_state.model.start_chat(history=[])
            response = chat.send_message(initial_prompt)
            st.session_state.messages.append(
                {"role": "Architect", "content": response.text}
            )
            st.session_state.chat = chat
            update_active_module(response.text)
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            st.stop()


# --- 7. ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
for message in st.session_state.messages:
    role_name = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(message["content"], unsafe_allow_html=True)


# --- 8. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜ ---
def stream_and_store_response(chat_session, prompt_to_send, spinner_text="Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
    """ëª¨ë¸ì— ëª…ë ¹ì„ ë³´ë‚´ê³  ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ ì €ì¥."""
    full_response = ""
    start_time = time.time()

    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        response_placeholder = st.empty()
        try:
            with st.spinner(spinner_text):
                response_stream = chat_session.send_message(
                    prompt_to_send, stream=True
                )

                for chunk in response_stream:
                    if not chunk.parts:
                        full_response = "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µì´ 'ì•ˆì „ í•„í„°'ì— ì˜í•´ 'ì°¨ë‹¨'ë˜ì—ˆìŠµë‹ˆë‹¤.]"
                        response_placeholder.error(full_response)
                        break

                    full_response += chunk.text
                    response_placeholder.markdown(
                        full_response + "â–Œ", unsafe_allow_html=True
                    )

            response_placeholder.markdown(full_response, unsafe_allow_html=True)

        except Exception as e:
            full_response = f"[ì¹˜ëª…ì  ì˜¤ë¥˜: {e}]"
            response_placeholder.error(full_response)

    st.session_state.messages.append(
        {"role": "Architect", "content": full_response}
    )
    update_active_module(full_response)

    end_time = time.time()
    print(f"Response time: {end_time - start_time:.2f}s")
    return full_response


# --- 9. ë©”ì¸ ì…ë ¥ ë£¨í”„ ---
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt, unsafe_allow_html=True)

    is_data_ingestion_phase = "Phase 2" in st.session_state.active_module

    rag_context = ""
    similar_precedents = []

    # ë©”ë‰´ ì…ë ¥/Phase 2ê°€ ì•„ë‹ ë•Œë§Œ RAG ìˆ˜í–‰
    if not _is_menu_input(prompt) and not is_data_ingestion_phase:
        contextual_query = (
            f"í˜„ì¬ í™œì„±í™”ëœ ëª¨ë“ˆ: {st.session_state.active_module}. "
            f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
        )

        with st.spinner("ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... (Dual RAG: íŒë¡€/ë²•ë ¹)..."):
            # 1) ë²•ë ¹ RAG
            if "statutes" in st.session_state and st.session_state.statutes:
                similar_statutes = find_similar_items(
                    contextual_query,
                    st.session_state.statutes,
                    st.session_state.s_embeddings,
                    top_k=3,
                    threshold=0.75,
                )
                if similar_statutes:
                    s_texts = [
                        f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n"
                        f"{c.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n"
                        for c in similar_statutes
                    ]
                    rag_context += (
                        "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n"
                        + "\n".join(s_texts)
                    )

            # 2) íŒë¡€ RAG
            if "precedents" in st.session_state and st.session_state.precedents:
                similar_precedents = find_similar_items(
                    contextual_query,
                    st.session_state.precedents,
                    st.session_state.p_embeddings,
                    top_k=5,
                    threshold=0.75,
                )
                if similar_precedents:
                    p_texts = [
                        f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n"
                        f"{c.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n"
                        for c in similar_precedents
                    ]
                    rag_context += (
                        "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n"
                        + "\n".join(p_texts)
                    )

    final_prompt = f"{prompt}\n{rag_context}"
    current_response = stream_and_store_response(
        st.session_state.chat, final_prompt
    )

    # --- 10. íŒë¡€ ì‹œê°í™” ë¸”ë¡ ---
    clean_response = re.sub("<[^<]+?>", "", current_response)

    if _is_final_report(clean_response) and similar_precedents:
        q_title = _query_title(prompt)
        st.markdown(
            f"**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n"
            f"* ê²€ìƒ‰ ì¿¼ë¦¬: `[{q_title}]`\n"
        )

        for case_data in similar_precedents[:3]:
            sim_pct = int(round(case_data["similarity"] * 100))
            title = case_data.get("title", "ì œëª© ì—†ìŒ")
            case_no = case_data.get("case_no", case_data.get("id", ""))
            court = case_data.get("court", "")
            date = case_data.get("date", "")
            url = case_data.get("url")
            full_text = case_data.get(
                "full_text", case_data.get("raw_text")
            )

            label = f"íŒë¡€ [{title}]"
            if court and case_no:
                label += f" â€” {court} {case_no}"

            summary = case_data.get("rag_index", "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
            if len(summary) > 200:
                summary = summary[:197] + "..."

            action_link = f"[ğŸ”— ì›ë¬¸ ë§í¬ ë³´ê¸°]({url})" if url else ""

            item_md = (
                f"* **{label}**\n"
                f"  - ì„ ê³ : {date} | ìœ ì‚¬ë„: {sim_pct}% | {action_link}\n"
                f"  - ë‚´ìš© ìš”ì•½ (RAG Index): {summary}"
            )
            st.markdown(item_md)

            if full_text:
                with st.expander("ğŸ“„ íŒë¡€ ì „ë¬¸ ë³´ê¸°"):
                    st.text(full_text)

    elif (
        _is_final_report(clean_response)
        and not _is_menu_input(prompt)
        and not similar_precedents
    ):
        st.info(
            "â„¹ï¸ ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.75)"
        )
