# ======================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.1 â€” Hybrid RAG Build (Omega-Infinitum Core)
# ======================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re # ì •ê·œì‹ ì‚¬ìš©

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol) ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.1", page_icon="ğŸ›¡ï¸", layout="centered")

# CSS í•´í‚¹ (ì‹ ê¸°ë£¨ í”„ë¡œí† ì½œ) - ë„¤ë†ˆì´ ë„£ì€ CSS ìœ ì§€ ë° ìµœì í™”
custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}

/* --- ê¸€ì ìŠ¤íƒ€ì¼ í†µì¼ --- */
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    /* color: #FFFFFF !important; (ë‹¤í¬ëª¨ë“œ ê°•ì œëŠ” ê°€ë…ì„±ì„ í•´ì¹¨. ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ í…Œë§ˆ í˜¸í™˜ì„± í™•ë³´) */
    font-size: 16px !important; /* 17pxì€ ë„ˆë¬´ í¬ë‹¤. 16pxë¡œ ì¡°ì • */
    line-height: 1.7 !important;
}

/* --- íƒ€ì´í‹€ ìœ„ì¹˜ ì¡°ì • --- */
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
}

/* --- ì¤‘ìš” ë¬¸ë‹¨ / í—¤ë“œë¼ì¸ ì»¬ëŸ¬ ê°•ì¡° --- */
strong, b {
    /* color: #5AB0FF !important; (í¬ì¸íŠ¸ ì»¬ëŸ¬ëŠ” ìœ ì§€í•˜ë˜, í…Œë§ˆ ìë™ ì¡°ì • ê¶Œì¥) */
    font-weight: 700;
}

/* --- ë¶€ë“œëŸ¬ìš´ í…ìŠ¤íŠ¸ ë“±ì¥ (ì†ë„ ê°œì„  0.8s -> 0.5s) --- */
.fadein {
    animation: fadeInText 0.5s ease-in-out forwards;
    opacity: 0;
}
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}

[data-testid="stChatMessageContent"] {
    font-size: 16px !important;
}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)


# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.1")

st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# --- 3. API í‚¤ ë° ëª¨ë¸ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
         raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. (API Key ëˆ„ë½ ë˜ëŠ” ë¹„ì–´ìˆìŒ): {e}")
    st.stop()

# --- [ì‘ì „ëª…: íŠ¸ë¡œì´ ëª©ë§ˆ] ê²Œë¦´ë¼ RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜ ---
# (ë„¤ë†ˆì´ ì¶”ê°€í•œ RAG ê´€ë ¨ í•¨ìˆ˜ë“¤ ìœ ì§€ ë° ìµœì í™”)
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text, task_type="retrieval_document"):
    # (ê¸°ì¡´ í•¨ìˆ˜ ë‚´ìš© ìœ ì§€)
    try:
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text:
            return None
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type
        )
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}")
        return None

@st.cache_data
def load_and_embed_precedents(file_path='precedents_data.txt'):
    # (ê¸°ì¡´ í•¨ìˆ˜ ë‚´ìš© ìœ ì§€ - ê²¬ê³ í•œ ìŠ¤í”Œë¦¿ í¬í•¨)
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return [], []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return [], []

    chunks = re.split(r'\s*---END OF PRECEDENT---\s*', content)
    precedents = [p.strip() for p in chunks if p and p.strip()]

    embeddings, valid_precedents = [], []
    for p in precedents:
        ebd = embed_text(p, task_type="retrieval_document")
        if ebd:
            embeddings.append(ebd)
            valid_precedents.append(p)

    print(f"[RAG] precedents={len(valid_precedents)}")
    return valid_precedents, embeddings

def _parse_precedent_block(text: str) -> dict:
    # (ê¸°ì¡´ íŒŒì‹± í•¨ìˆ˜ ë‚´ìš© ìœ ì§€)
    t = text.strip()
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    title = lines[0][:120] if lines else "ì œëª© ì—†ìŒ"
    m = re.search(
        r'\[(?P<court>[^ \[\]]+)\s+(?P<date>\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)\s*ì„ ê³ \s*(?P<caseno>\d{4}\s*[ê°€-í£]{1,2}\s*\d{3,6})\s*íŒê²°\]',
        t
    )
    court = m.group('court') if m else ""
    date  = m.group('date') if m else ""
    caseno = m.group('caseno').replace(" ", "") if (m and m.group('caseno')) else ""

    if not caseno:
        m2 = re.search(r'(?P<caseno>\d{4}\s*[ê°€-í£]{1,2}\s*\d{3,6})', t)
        if m2:
            caseno = m2.group('caseno').replace(" ", "")

    holding = ""
    m2 = re.search(r'ã€íŒê²°ìš”ì§€ã€‘(.*?)(ã€|$)', t, re.S)
    if m2:
        holding = re.sub(r'\s+', ' ', m2.group(1)).strip()
    else:
        m3 = re.search(r'ã€íŒì‹œì‚¬í•­ã€‘(.*?)(ã€|$)', t, re.S)
        if m3:
            holding = re.sub(r'\s+', ' ', m3.group(1)).strip()

    if not holding:
        holding = re.sub(r'\s+', ' ', t)[:160].strip()

    excerpt = ""
    for key in ["ã€ì „ë¬¸ã€‘", "ã€ì´ ìœ ã€‘", "ã€ì´ìœ ã€‘", "ã€ë³¸ë¬¸ã€‘"]:
        pos = t.find(key)
        if pos != -1:
            excerpt = re.sub(r'\s+', ' ', t[pos:pos+300]).strip()
            break
    if not excerpt:
        excerpt = re.sub(r'\s+', ' ', t)[:300].strip()

    if len(holding) > 130: holding = holding[:130].rstrip() + "â€¦"
    if len(excerpt) > 160: excerpt = excerpt[:160].rstrip() + "â€¦"

    return {
        "title": title, "court": court, "date": date,
        "case_no": caseno, "holding": holding, "excerpt": excerpt,
    }

def find_similar_precedents(query_text, precedents, embeddings, top_k=5):
    if not embeddings or not precedents:
        return []

    # task_type ìˆ˜ì •: retrieval_query
    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None:
        return []

    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]

    results = []
    for i in idxs:
        sim = float(sims[i])
        if sim < 0.50: # ì„ê³„ê°’ ìƒí–¥ ì¡°ì • 0.20 -> 0.50 (ì •í™•ë„ í™•ë³´)
            continue

        parsed = _parse_precedent_block(precedents[i])
        results.append({
            "similarity": sim,
            "raw_text": precedents[i], # â˜…ì¤‘ìš”: LLM ì£¼ì…ì„ ìœ„í•´ ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ê°€â˜…
            **parsed
        })

    return results

# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì (Prime Genome) ë¡œë“œ ë° ì´ˆê¸°í™” ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if not SYSTEM_INSTRUCTION.strip():
        raise ValueError("System prompt file is empty.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì½”ì–´(system_prompt.txt) ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()


if "model" not in st.session_state:
    try:
        # [ìˆ˜ì •ë¨] ì¡´ì¬í•˜ì§€ ì•ŠëŠ” '2.5'ê°€ ì•„ë‹ˆë¼ '1.5-flash-latest' ì‚¬ìš©. 'models/' ì ‘ë‘ì‚¬ ì¶”ê°€.
        st.session_state.model = genai.GenerativeModel("models/gemini-2.5-flash",
                                                    system_instruction=SYSTEM_INSTRUCTION)
        
        # [RAG ì´ˆê¸°í™”]
        with st.spinner("íŒë¡€ ë¶„ì„ ì—”ì§„(RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            p, e = load_and_embed_precedents()
            st.session_state.precedents = p
            st.session_state.embeddings = e
            if not p:
                st.warning("âš ï¸ íŒë¡€ ë°ì´í„°(precedents_data.txt) ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹„ì–´ìˆìŒ. RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”.")

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

# --- 5. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
# (ì´ˆê¸°í™” ë¡œì§ ê°•í™”)
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state or not st.session_state.messages:
    if "model" in st.session_state:
        try:
            if "chat" not in st.session_state:
                st.session_state.chat = st.session_state.model.start_chat(history=[])

            if not st.session_state.messages:
                # ì´ˆê¸°í™” ëª…ë ¹ ê°•í™”
                initial_prompt = "ê¸´ê¸‰ ëª…ë ¹: EPE í™œì„±í™”. ì¦‰ì‹œ <KnowledgeBase>ì˜ 'Phase 0: ë„ë©”ì¸ ì„ íƒ í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ê³  ë©”ë‰´ë¥¼ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í™•ì¸ì€ ìƒëµí•œë‹¤."
                response = st.session_state.chat.send_message(initial_prompt)
                if response and response.text:
                     # ì‹œê° íš¨ê³¼(fadein) ì ìš©í•˜ì—¬ ì €ì¥
                     st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{response.text}</div>"})
                else:
                     st.error("ì‹œìŠ¤í…œ ì½”ì–´ ì‘ë‹µ ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ).")
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (API í†µì‹  ì˜¤ë¥˜): {e}")


# --- 6. ëŒ€í™” ì¶œë ¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
for message in st.session_state.messages:
    role = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role, avatar=avatar):
        # ì´ë¯¸ fadeinì´ ì ìš©ëœ HTMLì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì¶œë ¥
        st.markdown(message['content'], unsafe_allow_html=True)

# --- 7. ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…í•µì‹¬ ìˆ˜ì •: í•˜ì´ë¸Œë¦¬ë“œ RAGâ˜…) ---

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ìœ ì§€ ë° ê°œì„ )
def _is_menu_input(s: str) -> bool:
    if not s: return False
    return bool(re.fullmatch(r'\d+|[1-9]-\d+', s.strip())) # ê³„ì¸µí˜• ë©”ë‰´ ëŒ€ì‘ ìˆ˜ì •

def _is_final_report(txt: str) -> bool:
    if not txt: return False
    t = txt.replace(" ", "")
    hits = 0
    # í‚¤ì›Œë“œ ìˆ˜ì •: ì‹¤ì œ ë³´ê³ ì„œ í‚¤ì›Œë“œ ë°˜ì˜
    for key in ["ì „ëµë¸Œë¦¬í•‘ë³´ê³ ì„œ", "ë¦¬ìŠ¤í¬ì‹œë®¬ë ˆì´ì…˜ë¶„ì„", "ê¶Œì¥ë‹¤ìŒë‹¨ê³„", "ë©´ì±…ì¡°í•­"]:
        if key in t: hits += 1
    return (hits >= 2) and (len(t) > 500) # ê¸¸ì´ ê¸°ì¤€ ì™„í™” 800 -> 500

def _query_title(prompt_text: str) -> str:
    # (ê¸°ì¡´ í•¨ìˆ˜ ë‚´ìš© ìœ ì§€)
    if not prompt_text: return ""
    m = re.search(r'\[([^\]]+)\]', prompt_text)
    if m: return m.group(1).strip()
    first = prompt_text.strip().splitlines()[0].strip()
    return (first[:77] + "â€¦") if len(first) > 80 else first


if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    # ì‚¬ìš©ì ì…ë ¥ í‘œì‹œ ì‹œ fadein ì ìš©
    st.session_state.messages.append({"role": "user", "content": f"<div class='fadein'>{prompt}</div>"})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(f"<div class='fadein'>{prompt}</div>", unsafe_allow_html=True)

    # [â˜…í•µì‹¬ ìˆ˜ì • 1: RAG ì‹¤í–‰ ì‹œì  ì´ë™â˜…] LLM í˜¸ì¶œ ì „ì— RAG ì‹¤í–‰
    rag_context = ""
    similar_cases = [] # ì¹´ë“œ í‘œì‹œë¥¼ ìœ„í•´ ì €ì¥
    
    # ë©”ë‰´ ì…ë ¥ì´ ì•„ë‹ˆê³ , ë°ì´í„°ê°€ ë¡œë“œëœ ê²½ìš°ì—ë§Œ RAG ì‹¤í–‰
    if not _is_menu_input(prompt) and ("precedents" in st.session_state and st.session_state.precedents):
         with st.spinner("ì‹¤ì‹œê°„ íŒë¡€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰(RAG)..."):
            similar_cases = find_similar_precedents(prompt, 
                                                    st.session_state.precedents, 
                                                    st.session_state.embeddings, 
                                                    top_k=5)
            if similar_cases:
                # LLM ì£¼ì…ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©)
                rag_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c['raw_text']}\n---\n" for c in similar_cases]
                rag_context = "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" + "\n".join(rag_texts)

    # [â˜…í•µì‹¬ ìˆ˜ì • 2: ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±â˜…] ì‚¬ìš©ì ì…ë ¥ + RAG ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
    final_prompt = f"{prompt}\n{rag_context}"

    # ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± (API í˜¸ì¶œ)
    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘... ë³€ìˆ˜ ë¶„ì„ ë° ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰..."):
        try:
            # [â˜…í•µì‹¬ ìˆ˜ì • 3: final_prompt ì‚¬ìš©â˜…] ì¦ê°•ëœ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì— ì „ì†¡
            response_stream = st.session_state.chat.send_message(final_prompt, stream=True)
            
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                placeholder = st.empty()
                full_response = ""
                for chunk in response_stream:
                    if getattr(chunk, "text", None):
                        full_response += chunk.text
                        # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì‹œ fadein ì ìš©
                        placeholder.markdown(
                            f"<div class='fadein'>{full_response}â–Œ</div>",
                            unsafe_allow_html=True
                        )
                placeholder.markdown(
                    f"<div class='fadein'>{full_response}</div>",
                    unsafe_allow_html=True
                )

            # ìŠ¤íŠ¸ë¦¼ í´ë°± ë¡œì§ (ë‹¨ìˆœí™”)
            if not full_response.strip():
                 pass

            # ìµœì¢… ì‘ë‹µ ì €ì¥ ì‹œ fadein ì ìš©
            st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{full_response}</div>"})

            # [â˜…í•µì‹¬ ìˆ˜ì • 4: íŒë¡€ ì¹´ë“œ í‘œì‹œâ˜…] ì´ë¯¸ ê³„ì‚°ëœ similar_cases ì‚¬ìš©
            # ìµœì¢… ë³´ê³ ì„œì´ê³ , RAG ê²°ê³¼ê°€ ìˆì„ ê²½ìš°ì—ë§Œ í‘œì‹œ (ë„¤ê°€ ì›í•˜ë˜ UI)
            if _is_final_report(full_response) and similar_cases:
                # í—¤ë” ì¶œë ¥
                q_title = _query_title(prompt)
                st.markdown("**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (RAG ê²°ê³¼)**\n\n* ê²€ìƒ‰ ì¿¼ë¦¬: `[" + q_title + "]`\n")

                # ìƒìœ„ 3ê±´ë§Œ ì¹´ë“œí˜• ìš”ì•½ìœ¼ë¡œ ì¶œë ¥
                for case in similar_cases[:3]:
                    sim_pct = int(round(case["similarity"] * 100))
                    label = f"íŒë¡€ [{case.get('title','ì œëª© ì—†ìŒ')}]"
                    if case.get("court") and case.get("case_no"):
                        label += f" â€” {case['court']} {case['case_no']}"

                    item_md = (
                        f"* {label}  \n"
                        f"  - ì„ ê³ : {case.get('date','').strip()} {case.get('court','').strip()} | ìœ ì‚¬ë„: {sim_pct}%  \n"
                        f"  - íŒê²°ìš”ì§€: {case.get('holding','').strip()}  \n"
                        f"  - ì „ë¬¸ ì¼ë¶€: \"{case.get('excerpt','').strip()}\""
                    )
                    st.markdown(item_md)
            elif _is_final_report(full_response) and not _is_menu_input(prompt) and not similar_cases:
                 # ë³´ê³ ì„œëŠ” ë‚˜ì™”ì§€ë§Œ RAG ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° (ì„ê³„ê°’ ë¯¸ë‹¬ ë“±)
                 st.info("â„¹ï¸ ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì„ê³„ê°’ 0.50)")


        except Exception as e:
            err = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(err)
            # ì˜¤ë¥˜ ë©”ì‹œì§€ ì €ì¥ ì‹œ fadein ì ìš©
            st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{err}</div>"})
