import streamlit as st
import google.generativeai as genai
import os
import re
import json
import numpy as np
import PyPDF2
import time

# ---------------------------------------
# 0. ì‹œìŠ¤í…œ ì„¤ì •
# ---------------------------------------
st.set_page_config(
    page_title="Veritas Engine 8.1 | Legal Architect",
    page_icon="âš–ï¸",
    layout="centered"
)

# API í‚¤ ì„¤ì •
if "GOOGLE_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
else:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (secrets.toml ì—†ì„ ê²½ìš°)
    st.warning("Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

SYSTEM_INSTRUCTION = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë²•ë¥  ì „ë¬¸ê°€ì´ì ì „ëµê°€ì¸ 'Veritas Architect'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ì‚¬ê±´ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë²•ë¦¬ì  ê·¼ê±°(ì¡°ë¬¸, íŒë¡€)ì— ê¸°ë°˜í•œ ëª…í™•í•œ ì „ëµì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
"""

EMBEDDING_MODEL_NAME = "models/text-embedding-004"

# ---------------------------------------
# 1. ì„ë² ë”© ë° RAG ê²€ìƒ‰ í•¨ìˆ˜
# ---------------------------------------
def embed_text(text: str, task_type: str = "retrieval_document"):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    clean_text = text.replace("\n", " ").strip()
    if not clean_text: return None
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type
        )
        return result['embedding']
    except Exception as e:
        print(f"[Embedding error] {e}")
        return None

def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.5):
    """ìœ ì‚¬ë„ ê²€ìƒ‰"""
    if not items or not embeddings: return []
    try:
        q_emb = embed_text(query_text, task_type="retrieval_query")
        if q_emb is None: return []
        sims = np.dot(np.array(embeddings), np.array(q_emb))
        idxs = np.argsort(sims)[::-1][:top_k]
        results = []
        for i in idxs:
            score = float(sims[i])
            if score < threshold: continue
            item = items[i].copy()
            item["similarity"] = score
            results.append(item)
        return results
    except Exception as e:
        print(f"[RAG Error] {e}")
        return []

# ---------------------------------------
# 2. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (íŒŒì¼ ê¸°ë°˜ RAG)
# ---------------------------------------
@st.cache_resource
def load_precomputed_embeddings():
    statute_items = []
    statute_embeddings = []
    precedent_items = []
    precedent_embeddings = []

    # ë¡œë”© ìƒíƒœ ì‹œê°í™”
    with st.status("ğŸ“š Veritas ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...", expanded=True) as status:
        try:
            # 1. ë²•ë ¹ ë¡œë“œ
            if os.path.exists("statutes_data.txt"):
                st.write("ğŸ“œ ë²•ë ¹ ë°ì´í„° ìŠ¤ìº”...")
                with open("statutes_data.txt", "r", encoding="utf-8") as f:
                    content = f.read()
                parts = re.split(r"\s*---END OF STATUTE---\s*", content)
                for i, p in enumerate(parts):
                    if i >= 10: break # ë°ëª¨ìš© ì œí•œ
                    p = p.strip()
                    if not p: continue
                    emb = embed_text(p)
                    if emb:
                        statute_items.append({"rag_index": p, "raw_text": p})
                        statute_embeddings.append(emb)
                        time.sleep(0.1)
                st.write(f"âœ… ë²•ë ¹ {len(statute_items)}ê±´ ë¡œë“œ")
            
            # 2. íŒë¡€ ë¡œë“œ
            if os.path.exists("precedents_data.jsonl"):
                st.write("âš–ï¸ íŒë¡€ ë°ì´í„° ìŠ¤ìº”...")
                with open("precedents_data.jsonl", "r", encoding="utf-8") as f:
                    count = 0
                    for line in f:
                        if count >= 10: break # ë°ëª¨ìš© ì œí•œ
                        line = line.strip()
                        if not line: continue
                        try:
                            obj = json.loads(line)
                            txt = obj.get("rag_index", "")
                            if txt:
                                emb = embed_text(txt)
                                if emb:
                                    precedent_items.append(obj)
                                    precedent_embeddings.append(emb)
                                    count += 1
                                    time.sleep(0.1)
                        except: continue
                st.write(f"âœ… íŒë¡€ {len(precedent_items)}ê±´ ë¡œë“œ")
            
            status.update(label="ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ", state="complete", expanded=False)
        except Exception as e:
            print(f"[RAG ë¡œë”© ì—ëŸ¬] {e}")

    return statute_items, statute_embeddings, precedent_items, precedent_embeddings

# ---------------------------------------
# 3. PDF ì²˜ë¦¬ í•¨ìˆ˜
# ---------------------------------------
def extract_text_from_pdf(uploaded_file):
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def analyze_case_file(pdf_text: str, model):
    analysis_prompt = f"""
    ë‹¤ìŒ ì‚¬ê±´ ê¸°ë¡ì„ ì •ë°€ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
    {{
        "domain": "í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬",
        "subdomain": "ì„¸ë¶€ ì£„ëª… ë˜ëŠ” ìŸì ",
        "key_facts": ["ì‚¬ì‹¤1", "ì‚¬ì‹¤2", "ì‚¬ì‹¤3", "ì‚¬ì‹¤4", "ì‚¬ì‹¤5"],
        "evidence": ["ì¦ê±°1", "ì¦ê±°2"],
        "our_claim": "ìš°ë¦¬ ì¸¡ ì£¼ì¥",
        "their_claim": "ìƒëŒ€ë°© ì£¼ì¥"
    }}
    [ë‚´ìš©]
    {pdf_text[:10000]}
    """
    try:
        response = model.generate_content(analysis_prompt, generation_config={"response_mime_type": "application/json"})
        return json.loads(response.text)
    except:
        return None

# ---------------------------------------
# 4. ìœ í‹¸ ë° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
# ---------------------------------------
def _is_reset_keyword(s: str) -> bool:
    return any(kw in s.lower() for kw in ["ì²˜ìŒ", "ë©”ì¸", "ì´ˆê¸°í™”", "reset"])

def update_active_module(response_text: str):
    if ("9." in response_text and "ì‚¬ê±´ê¸°ë¡" in response_text) or \
       ("Auto-Analysis Mode" in response_text):
        st.session_state.active_module = "Auto-Analysis Mode"
    
    m = re.search(r"'(.+?)' ëª¨ë“ˆì„ (?:ìµœì¢… )?í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
    if m:
        st.session_state.active_module = m.group(1).strip()

def stream_and_store_response(chat_session, prompt_to_send: str):
    full_response = ""
    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        placeholder = st.empty()
        try:
            stream = chat_session.send_message(prompt_to_send, stream=True)
            for chunk in stream:
                if getattr(chunk, "text", None):
                    full_response += chunk.text
                    placeholder.markdown(full_response + "â–Œ")
            placeholder.markdown(full_response)
        except Exception as e:
            placeholder.error(f"ì—°ì‚° ì˜¤ë¥˜: {e}")
    
    st.session_state.messages.append({"role": "Architect", "content": full_response})
    update_active_module(full_response)
    return full_response

# ---------------------------------------
# 5. ë©”ì¸ ë¡œì§
# ---------------------------------------

# ëª¨ë¸ ì´ˆê¸°í™”
if "model" not in st.session_state:
    try:
        # Gemini 1.5 Flash ì‚¬ìš©
        st.session_state.model = genai.GenerativeModel("models/gemini-1.5-flash", system_instruction=SYSTEM_INSTRUCTION)
        st.session_state.chat = st.session_state.model.start_chat(history=[])
        st.session_state.messages = []
        
        # [í•µì‹¬ ìˆ˜ì •] ì´ˆê¸° ë©”ì‹œì§€ ê°•ì œ ì£¼ì… (í™”ë©´ì— ê¸€ ì•ˆ ë‚˜ì˜¤ëŠ” ë¬¸ì œ í•´ê²°)
        init_msg = """
        **Veritas Engine 8.1 ê°€ë™.**
        
        ë²•ë¥  ì „ëµ ìˆ˜ë¦½ì„ ìœ„í•œ Architectê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.
        ì›í•˜ì‹œëŠ” ì‘ì—…ì´ë‚˜ ì‚¬ê±´ ê°œìš”ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤.
        
        (PDF ë¶„ì„ì„ ì›í•˜ì‹œë©´ '9'ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤)
        """
        st.session_state.messages.append({"role": "Architect", "content": init_msg})
        st.session_state.active_module = "Phase 0"
        
        # ë°ì´í„° ë¡œë“œ
        s_data, s_emb, p_data, p_emb = load_precomputed_embeddings()
        st.session_state.statutes = s_data
        st.session_state.s_embeddings = s_emb
        st.session_state.precedents = p_data
        st.session_state.p_embeddings = p_emb
        
    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ëŒ€í™” ë‚´ì—­ ì¶œë ¥
for m in st.session_state.messages:
    avatar = "ğŸ›¡ï¸" if m["role"] == "Architect" else "ğŸ‘¤"
    with st.chat_message(m["role"], avatar=avatar):
        st.markdown(m["content"])

# í™”ë©´ ìŠ¤í¬ë¡¤ í•˜ë‹¨ ê³ ì •
st.markdown('<script>window.scrollTo(0, document.body.scrollHeight);</script>', unsafe_allow_html=True)

# PDF ëª¨ë“œ UI
if st.session_state.get("active_module") == "Auto-Analysis Mode":
    # ë§ˆì§€ë§‰ ëŒ€í™”ê°€ '9'ì¼ ë•Œë§Œ í‘œì‹œ
    if st.session_state.messages and st.session_state.messages[-1]["content"] == "9":
        st.markdown("---")
        st.info("ğŸ“„ **ì‚¬ê±´ê¸°ë¡ PDF ìë™ ë¶„ì„ ëª¨ë“œ**")
        uploaded_file = st.file_uploader("íŒê²°ë¬¸/ê³ ì†Œì¥ PDF ì—…ë¡œë“œ", type=["pdf"])
        
        if uploaded_file:
            if st.button("ğŸš€ ìë™ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
                with st.spinner("í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° AI ë¶„ì„ ì¤‘..."):
                    pdf_text = extract_text_from_pdf(uploaded_file)
                    if pdf_text:
                        analysis = analyze_case_file(pdf_text, st.session_state.model)
                        if analysis:
                            st.success("ë¶„ì„ ì™„ë£Œ")
                            with st.expander("ğŸ“Š ë¶„ì„ ê²°ê³¼ ë³´ê¸°", expanded=True):
                                st.markdown(f"**ë„ë©”ì¸:** {analysis.get('domain')}")
                                st.markdown("**í•µì‹¬ ì‚¬ì‹¤:**")
                                for f in analysis.get('key_facts', []):
                                    st.markdown(f"- {f}")
                            
                            # ìë™ ì§„í–‰
                            st.session_state["auto_analysis"] = analysis
                            
                            # ë‹¤ìŒ ë‹¨ê³„ ìë™ íŠ¸ë¦¬ê±°
                            domain_map = {"í˜•ì‚¬": "2", "ë¯¼ì‚¬": "8", "ì´í˜¼": "1"}
                            domain_num = domain_map.get(analysis.get("domain", ""), "8")
                            
                            auto_prompt = f"""
                            [ìë™ ë¶„ì„ ë°ì´í„°]
                            ë„ë©”ì¸: {analysis.get('domain')}
                            ì‚¬ì‹¤ê´€ê³„: {analysis.get('key_facts')}
                            ì–‘ì¸¡ì£¼ì¥: {analysis.get('our_claim')} vs {analysis.get('their_claim')}
                            
                            ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ {domain_num}ë²ˆ ëª¨ë“ˆì„ ì‹¤í–‰í•˜ì—¬ ìŠ¹ì†Œ ì „ëµì„ ì œì‹œí•˜ë¼.
                            """
                            
                            st.session_state.messages.append({"role": "user", "content": "PDF ë¶„ì„ ì™„ë£Œ. ìë™ ì „ëµ ìˆ˜ë¦½ ì‹œì‘."})
                            stream_and_store_response(st.session_state.chat, auto_prompt)
                            st.rerun()

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ëª…ë ¹ ë˜ëŠ” ì‚¬ê±´ ë‚´ìš©ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤..."):
    # ì´ˆê¸°í™”
    if _is_reset_keyword(prompt):
        st.session_state.chat = st.session_state.model.start_chat(history=[])
        st.session_state.messages = [{"role": "Architect", "content": "ì‹œìŠ¤í…œì´ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤."}]
        st.rerun()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)
    
    if prompt.strip() == "9":
        st.session_state.active_module = "Auto-Analysis Mode"
        stream_and_store_response(st.session_state.chat, "ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œì— ëŒ€í•´ ì„¤ëª…í•˜ë¼.")
        st.rerun()
    else:
        # RAG ê²€ìƒ‰ (ë²•ë¥ /íŒë¡€ê°€ ìˆë‹¤ë©´ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€)
        rag_context = ""
        if st.session_state.statutes:
            sim_statutes = find_similar_items(prompt, st.session_state.statutes, st.session_state.s_embeddings)
            if sim_statutes:
                rag_context += "\n[ê´€ë ¨ ë²•ë ¹]\n" + "\n".join([s['raw_text'] for s in sim_statutes])
        
        if rag_context:
            full_prompt = f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}\n\n{rag_context}\n\nìœ„ ë²•ì  ê·¼ê±°ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ë¼."
        else:
            full_prompt = prompt
            
        stream_and_store_response(st.session_state.chat, full_prompt)
