# ======================================================
# ğŸ›¡ï¸ Veritas Engine v7.1 (Stabilized Build)
# ======================================================
import streamlit as st
import google.generativeai as genai
import os
import requests
import re
import numpy as np

# ======================================================
# 1. SYSTEM INIT (Vault Mode)
# ======================================================
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.1", page_icon="ğŸ›¡ï¸", layout="centered")

hide_ui = """
<style>
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
header {visibility: hidden;}
.stDeployButton {visibility: hidden;}
</style>
"""
st.markdown(hide_ui, unsafe_allow_html=True)

st.title("ğŸ§  ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ v7.1")
st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ì™¸ë¶€ ìœ ì¶œ ê¸ˆì§€.")

# ======================================================
# 2. API KEY SETUP
# ======================================================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì‹œìŠ¤í…œ ì˜¤ë¥˜: 'GOOGLE_API_KEY' ëˆ„ë½. [Secrets] íƒ­ì„ í™•ì¸í•˜ë¼.")
    st.stop()

genai.configure(api_key=API_KEY)

try:
    OC_KEY = st.secrets["LAW_API_KEY"]
except KeyError:
    OC_KEY = "DEOKJUNE"

# ======================================================
# 3. LAW.GO.KR API HANDLER
# ======================================================
def get_precedent_full(prec_id):
    """ë²•ì œì²˜ APIì—ì„œ íŒë¡€ ì „ë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤."""
    if not OC_KEY or OC_KEY == "DEOKJUNE_FALLBACK":
        return {"error": "LAW_API_KEY ëˆ„ë½"}
    url = "http://www.law.go.kr/DRF/lawService.do"
    params = {"OC": OC_KEY, "target": "prec", "ID": prec_id, "type": "JSON"}
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        data = r.json()
        if 'íŒë¡€ì •ë³´' not in data:
            return {"error": f"API ì‘ë‹µ ì˜¤ë¥˜: {data}"}
        return data
    except Exception as e:
        return {"error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}"}


def show_full_precedent(prec_id):
    """íŒë¡€ ì „ë¬¸ í‘œì‹œ í¬ë§·."""
    data = get_precedent_full(prec_id)
    if "error" in data:
        return f"**[API ì˜¤ë¥˜]** (ID: {prec_id}) â†’ {data['error']}"
    try:
        info = data.get('íŒë¡€ì •ë³´', {})
        title = info.get('ì‚¬ê±´ëª…', 'N/A')
        verdict_date = info.get('ì„ ê³ ì¼ì', 'N/A')
        court = info.get('ë²•ì›ëª…', 'N/A')
        summary = info.get('íŒê²°ìš”ì§€', 'N/A').replace('\n', ' ')
        content = info.get('íŒë¡€ë‚´ìš©', 'N/A')[:500].replace('\n', ' ')
        ref = info.get('ì°¸ì¡°ì¡°ë¬¸', 'N/A')
        return f"""
---
**ğŸ” íŒë¡€ ì „ë¬¸ ì „ì²´**
- ì‚¬ê±´ëª…: {title}
- ì„ ê³ ì¼ì: {verdict_date}
- ë²•ì›: {court}
- [ë²•ì œì²˜ ë°”ë¡œê°€ê¸°](http://www.law.go.kr/precInfo.do?precSeq={prec_id})
**ìš”ì§€**: {summary}
**ë³¸ë¬¸ (500ì)**: {content}...
**ì°¸ì¡°ì¡°ë¬¸**: {ref}
---
"""
    except Exception as e:
        return f"[ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜]: {e}"

# ======================================================
# 4. EMBEDDING ENGINE (ê²Œë¦´ë¼ RAG)
# ======================================================
EMBED_MODEL = "models/text-embedding-004"

def embed_text(text, task_type="RETRIEVAL_DOCUMENT"):
    try:
        res = genai.embed_content(model=EMBED_MODEL, content=text, task_type=task_type)
        return res['embedding']
    except Exception as e:
        st.error(f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return None


@st.cache_data(show_spinner=False)
def load_and_embed_precedents(file_path):
    """GitHub RAW ë˜ëŠ” ë¡œì»¬ txt íŒŒì¼ì„ ì½ì–´ ì„ë² ë”©í•œë‹¤."""
    try:
        # ğŸ”¹ RAW ê²½ë¡œ ìë™ íŒë³„
        if file_path.startswith("http://") or file_path.startswith("https://"):
            st.info(f"GitHub RAW ê²½ë¡œ ê°ì§€ âœ…\n{file_path}")
            r = requests.get(file_path, timeout=10)
            if r.status_code != 200:
                raise FileNotFoundError(f"HTTP ì‘ë‹µ {r.status_code}")
            content = r.text
        else:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
    except FileNotFoundError:
        st.warning(f"âš ï¸ íƒ„ì•½ê³ ({file_path})ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. GitHub ì—…ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return [], np.array([])
    except Exception as e:
        st.error(f"íƒ„ì•½ê³  ë¡œë“œ ì‹¤íŒ¨: {e}")
        return [], np.array([])

    precedents = [p.strip() for p in content.split('---END OF PRECEDENT---') if p.strip()]
    if not precedents:
        st.warning(f"âš ï¸ íƒ„ì•½ê³ ({file_path})ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return [], np.array([])

    st.success(f"âœ… íƒ„ì•½ê³  ì¥ì „ ì™„ë£Œ! íŒë¡€ {len(precedents)}ê°œ í™•ë³´.")
    embeddings = []
    valid_precedents = []
    for p in precedents:
        emb = embed_text(p)
        if emb:
            embeddings.append(emb)
            valid_precedents.append(p)
    return valid_precedents, np.array(embeddings)


def find_similar_precedents(query, precedents, embeddings, top_k=3):
    """ì…ë ¥ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ íŒë¡€ ë°˜í™˜"""
    if embeddings.size == 0:
        return ""
    q_emb = embed_text(query, task_type="RETRIEVAL_QUERY")
    if q_emb is None:
        return ""
    sims = np.dot(embeddings, q_emb)
    top = np.argsort(sims)[-top_k:][::-1]
    context = "\n\n[ì°¸ì¡°: ê²Œë¦´ë¼ RAG ìœ ì‚¬ íŒë¡€ íƒìƒ‰ ê²°ê³¼]\n"
    for i in top:
        if sims[i] > 0.7:
            context += f"--- (ìœ ì‚¬ë„ {sims[i]*100:.0f}%)\n{precedents[i][:800]}...\n"
    return context

# ======================================================
# 5. SYSTEM PROMPT + ëª¨ë¸ ì´ˆê¸°í™”
# ======================================================
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()
except Exception:
    SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ë²•ë¥  AI ì‹œìŠ¤í…œ 'ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„'ì…ë‹ˆë‹¤."

if "precedents" not in st.session_state:
    RAW_URL = "https://raw.githubusercontent.com/deokjune85-rgb/imdmirage/main/precedents_data.txt"
    st.session_state.precedents, st.session_state.embeddings = load_and_embed_precedents(RAW_URL)

if "model" not in st.session_state:
    st.session_state.model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=SYSTEM_PROMPT)

if "chat" not in st.session_state:
    st.session_state.chat = st.session_state.model.start_chat(history=[])
    st.session_state.messages = []

# ======================================================
# 6. UI (ëŒ€í™” ì¸í„°í˜ì´ìŠ¤)
# ======================================================
for msg in st.session_state.messages:
    role = "Client" if msg["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role, avatar=avatar):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    with st.spinner("Architect ì—°ì‚° ì¤‘..."):
        try:
            # ğŸ”¹ RAG ê²€ìƒ‰
            rag_context = find_similar_precedents(prompt, st.session_state.precedents, st.session_state.embeddings)
            full_prompt = prompt + rag_context

            # ğŸ”¹ ìƒì„± ëª¨ë¸ í˜¸ì¶œ
            response_stream = st.session_state.chat.send_message(full_prompt, stream=True)
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                placeholder = st.empty()
                answer = ""
                for chunk in response_stream:
                    answer += chunk.text
                    placeholder.markdown(answer + "â–Œ")
                placeholder.markdown(answer)

            # ğŸ”¹ ë²•ì œì²˜ API íŒë¡€ í˜¸ì¶œ ìë™ í›„ì²˜ë¦¬
            if any(x in prompt for x in ["íŒë¡€", "ì „ë¬¸", "ID", "ë³¸ë¬¸"]):
                ids = re.findall(r'\d{6,8}', prompt)
                for pid in ids[:3]:
                    with st.spinner(f"ë²•ì œì²˜ íŒë¡€ {pid} í˜¸ì¶œ ì¤‘..."):
                        answer += "\n\n" + show_full_precedent(pid)
                placeholder.markdown(answer)

            st.session_state.messages.append({"role": "Architect", "content": answer})

        except Exception as e:
            st.error(f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: {e}")
