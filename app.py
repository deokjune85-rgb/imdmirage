# ======================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.2 â€” Dual RAG Build (Omega-Infinitum Core)
# ======================================================
import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time # ì†ë„ ì¡°ì ˆì„ ìœ„í•œ ëª¨ë“ˆ

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol) ---
# í…Œë§ˆ ì„¤ì •: ì‹œìŠ¤í…œ ê¸°ë³¸ê°’ ì‚¬ìš© (í° ë°”íƒ•/ê²€ì€ ê¸€ì”¨ ë˜ëŠ” ë‹¤í¬ ëª¨ë“œ ìë™ í˜¸í™˜)
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.2", page_icon="ğŸ›¡ï¸", layout="centered")

# CSS í•´í‚¹ (ì‹ ê¸°ë£¨ í”„ë¡œí† ì½œ) - [â˜…ìˆ˜ì •ë¨: ìƒ‰ìƒ ê°•ì œ ì œê±°, ì• ë‹ˆë©”ì´ì…˜ ìµœì í™”]
custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}

/* --- ê¸€ì ìŠ¤íƒ€ì¼ í†µì¼ (ìƒ‰ìƒ ê°•ì œ ì œê±°) --- */
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    /* color: #FFFFFF !important; <-- ì´ ì“°ë ˆê¸°ê°€ ë¬¸ì œì˜€ë‹¤. ì œê±°í•¨. */
    font-size: 16px !important;
    line-height: 1.7 !important;
}

/* --- íƒ€ì´í‹€ ìœ„ì¹˜ ì¡°ì • --- */
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
}

/* --- ì¤‘ìš” ë¬¸ë‹¨ / í—¤ë“œë¼ì¸ ê°•ì¡° --- */
strong, b {
    font-weight: 700;
}

/* --- ë¶€ë“œëŸ¬ìš´ í…ìŠ¤íŠ¸ ë“±ì¥ (ì†ë„ ì¡°ì ˆ 0.5s) --- */
.fadein {
    animation: fadeInText 0.5s ease-in-out forwards;
    opacity: 0;
}
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}

[data-testid="stChatMessageContent"] {
    font-size: 16px !important;
}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)


# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.2")
# ë¼ì´íŠ¸ ëª¨ë“œì—ì„œëŠ” st.errorë³´ë‹¤ st.warningì´ ê°€ë…ì„±ì´ ì¢‹ìŒ.
st.warning("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# --- 3. API í‚¤ ë° ëª¨ë¸ ì„¤ì • ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
         raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
    st.stop()

# --- [ì‘ì „ëª…: ë“€ì–¼ RAG ì—”ì§„] í•¨ìˆ˜ ì •ì˜ (ì¼ë°˜í™”) ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text, task_type="retrieval_document"):
    try:
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text: return None
        result = genai.embed_content(model=EMBEDDING_MODEL_NAME, content=clean_text, task_type=task_type)
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}"); return None

# [â˜…ìˆ˜ì •ë¨â˜…] ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”© í•¨ìˆ˜ (ì¼ë°˜í™”)
@st.cache_data
def load_and_embed_data(file_path, separator_regex):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}"); return [], []
    try:
        with open(file_path, 'r', encoding='utf-8') as f: content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}"); return [], []

    if not content.strip(): return [], []

    # ì •ê·œì‹ ê¸°ë°˜ ë¶„í• 
    chunks = re.split(separator_regex, content)
    data_items = [p.strip() for p in chunks if p and p.strip()]

    embeddings, valid_items = [], []
    for item in data_items:
        ebd = embed_text(item, task_type="retrieval_document")
        if ebd:
            embeddings.append(ebd); valid_items.append(item)
    print(f"[RAG] Loaded {len(valid_items)} items from {file_path}.")
    return valid_items, embeddings

# [â˜…ìˆ˜ì •ë¨â˜…] ê²€ìƒ‰ í•¨ìˆ˜ ì¼ë°˜í™” (íŒë¡€/ë²•ë ¹ ê³µìš©)
def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.50):
    if not embeddings or not items: return []
    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None: return []
    
    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]
    
    results = []
    for i in idxs:
        if float(sims[i]) >= threshold:
            results.append({"similarity": float(sims[i]), "raw_text": items[i]})
    return results

# (íŒë¡€ íŒŒì‹± í•¨ìˆ˜ëŠ” ì‹œê°í™”ë¥¼ ìœ„í•´ ìœ ì§€)
def _parse_precedent_block(text: str) -> dict:
    # (ê¸°ì¡´ íŒŒì‹± í•¨ìˆ˜ ë‚´ìš© ìœ ì§€ - ìƒëµ)
    t = text.strip()
    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    title = lines[0][:120] if lines else "ì œëª© ì—†ìŒ"
    m = re.search(r'\[(?P<court>[^ \[\]]+)\s+(?P<date>\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)\s*ì„ ê³ \s*(?P<caseno>\d{4}\s*[ê°€-í£]{1,2}\s*\d{3,6})\s*íŒê²°\]', t)
    court, date, caseno = (m.group('court'), m.group('date'), m.group('caseno').replace(" ", "")) if m else ("", "", "")
    if not caseno:
        m2 = re.search(r'(?P<caseno>\d{4}\s*[ê°€-í£]{1,2}\s*\d{3,6})', t)
        if m2: caseno = m2.group('caseno').replace(" ", "")
    holding = ""
    m2 = re.search(r'ã€íŒê²°ìš”ì§€ã€‘(.*?)(ã€|$)', t, re.S)
    if m2: holding = re.sub(r'\s+', ' ', m2.group(1)).strip()
    else:
        m3 = re.search(r'ã€íŒì‹œì‚¬í•­ã€‘(.*?)(ã€|$)', t, re.S)
        if m3: holding = re.sub(r'\s+', ' ', m3.group(1)).strip()
    if not holding: holding = re.sub(r'\s+', ' ', t)[:160].strip()
    excerpt = ""
    for key in ["ã€ì „ë¬¸ã€‘", "ã€ì´ ìœ ã€‘", "ã€ì´ìœ ã€‘", "ã€ë³¸ë¬¸ã€‘"]:
        pos = t.find(key)
        if pos != -1:
            excerpt = re.sub(r'\s+', ' ', t[pos:pos+300]).strip(); break
    if not excerpt: excerpt = re.sub(r'\s+', ' ', t)[:300].strip()
    if len(holding) > 130: holding = holding[:130].rstrip() + "â€¦"
    if len(excerpt) > 160: excerpt = excerpt[:160].rstrip() + "â€¦"
    return {"title": title, "court": court, "date": date, "case_no": caseno, "holding": holding, "excerpt": excerpt}

# (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ìœ ì§€)
def _is_menu_input(s: str) -> bool:
    if not s: return False
    return bool(re.fullmatch(r'\d+|[1-9]-\d+', s.strip()))

def _is_final_report(txt: str) -> bool:
    if not txt: return False
    t = txt.replace(" ", "")
    hits = 0
    for key in ["ì „ëµë¸Œë¦¬í•‘ë³´ê³ ì„œ", "ë¦¬ìŠ¤í¬ì‹œë®¬ë ˆì´ì…˜ë¶„ì„", "ê¶Œì¥ë‹¤ìŒë‹¨ê³„", "ë©´ì±…ì¡°í•­"]:
        if key in t: hits += 1
    return (hits >= 2) and (len(t) > 500)

def _query_title(prompt_text: str) -> str:
    # (ê¸°ì¡´ í•¨ìˆ˜ ë‚´ìš© ìœ ì§€)
    if not prompt_text: return ""
    m = re.search(r'\[([^\]]+)\]', prompt_text)
    if m: return m.group(1).strip()
    first = prompt_text.strip().splitlines()[0].strip()
    return (first[:77] + "â€¦") if len(first) > 80 else first

# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì (Prime Genome) ë¡œë“œ ë° ì´ˆê¸°í™” ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if not SYSTEM_INSTRUCTION.strip():
        raise ValueError("System prompt file is empty.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì½”ì–´(system_prompt.txt) ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()


if "model" not in st.session_state:
    try:
        # ëª¨ë¸ëª… í™•ì¸: 'models/gemini-2.5-flash'
        st.session_state.model = genai.GenerativeModel("models/gemini-2.5-flash",
                                                    system_instruction=SYSTEM_INSTRUCTION)
        
        # [â˜…ìˆ˜ì •ë¨â˜…] ë“€ì–¼ RAG ì´ˆê¸°í™”
        with st.spinner("ë¶„ì„ ì—”ì§„(Dual RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            # 1. íŒë¡€ ë°ì´í„° ë¡œë“œ (P-RAG)
            p_data, p_emb = load_and_embed_data('precedents_data.txt', r'\s*---END OF PRECEDENT---\s*')
            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb
            if not p_data: st.warning("âš ï¸ íŒë¡€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. P-RAG ë¹„í™œì„±í™”.")

            # 2. ë²•ë ¹ ë°ì´í„° ë¡œë“œ (S-RAG)
            s_data, s_emb = load_and_embed_data('statutes_data.txt', r'\s*---END OF STATUTE---\s*')
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb
            if not s_data: st.warning("âš ï¸ ë²•ë ¹ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. S-RAG ë¹„í™œì„±í™”.")

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.stop()

# --- 5. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state or not st.session_state.messages:
    if "model" in st.session_state:
        try:
            if "chat" not in st.session_state:
                st.session_state.chat = st.session_state.model.start_chat(history=[])

            if not st.session_state.messages:
                initial_prompt = "ê¸´ê¸‰ ëª…ë ¹: EPE í™œì„±í™”. ì¦‰ì‹œ <KnowledgeBase>ì˜ 'Phase 0: ë„ë©”ì¸ ì„ íƒ í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ê³  ë©”ë‰´ë¥¼ ì¶œë ¥í•˜ë¼. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í™•ì¸ì€ ìƒëµí•œë‹¤."
                response = st.session_state.chat.send_message(initial_prompt)
                if response and response.text:
                     st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{response.text}</div>"})
                else:
                     st.error("ì‹œìŠ¤í…œ ì½”ì–´ ì‘ë‹µ ì‹¤íŒ¨ (ì‘ë‹µ ì—†ìŒ).")
        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (API í†µì‹  ì˜¤ë¥˜): {e}")

# --- 6. ëŒ€í™” ì¶œë ¥ ---
for message in st.session_state.messages:
    role = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role, avatar=avatar):
        st.markdown(message['content'], unsafe_allow_html=True)

# --- 7. ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…ë“€ì–¼ RAG í†µí•© ë° ì†ë„ ì œì–´â˜…) ---

if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": f"<div class='fadein'>{prompt}</div>"})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(f"<div class='fadein'>{prompt}</div>", unsafe_allow_html=True)

    # [â˜…í•µì‹¬ ìˆ˜ì • 1: ë“€ì–¼ RAG ì‹¤í–‰â˜…] LLM í˜¸ì¶œ ì „ì— ì‹¤í–‰
    rag_context = ""
    similar_precedents = [] # ì‹œê°í™”ìš© ì €ì¥
    
    # ë©”ë‰´ ì…ë ¥ì´ ì•„ë‹ ê²½ìš° RAG ì‹¤í–‰
    if not _is_menu_input(prompt):
         with st.spinner("ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... (Dual RAG: íŒë¡€/ë²•ë ¹)..."):
            # 1. ë²•ë ¹ ê²€ìƒ‰ (S-RAG)
            if ("statutes" in st.session_state and st.session_state.statutes):
                similar_statutes = find_similar_items(prompt,
                                                     st.session_state.statutes,
                                                     st.session_state.s_embeddings,
                                                     top_k=3, threshold=0.60) # ë²•ë ¹ì€ ì„ê³„ê°’ ìƒí–¥
                if similar_statutes:
                    s_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c['raw_text']}\n---\n" for c in similar_statutes]
                    rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n" + "\n".join(s_texts)

            # 2. íŒë¡€ ê²€ìƒ‰ (P-RAG)
            if ("precedents" in st.session_state and st.session_state.precedents):
                similar_precedents = find_similar_items(prompt, 
                                                        st.session_state.precedents, 
                                                        st.session_state.p_embeddings, 
                                                        top_k=5, threshold=0.50)
                if similar_precedents:
                    p_texts = [f"[ìœ ì‚¬ë„: {c['similarity']:.2f}]\n{c['raw_text']}\n---\n" for c in similar_precedents]
                    rag_context += "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" + "\n".join(p_texts)


    # [â˜…í•µì‹¬ ìˆ˜ì • 2: ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±â˜…] ì‚¬ìš©ì ì…ë ¥ + RAG ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
    final_prompt = f"{prompt}\n{rag_context}"

    # ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± (API í˜¸ì¶œ)
    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘... ë³€ìˆ˜ ë¶„ì„ ë° ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰..."):
        try:
            response_stream = st.session_state.chat.send_message(final_prompt, stream=True)
            
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                placeholder = st.empty()
                full_response = ""
                
                # [â˜…í•µì‹¬ ìˆ˜ì • 3: ìŠ¤ë¬´ìŠ¤ ìŠ¤íŠ¸ë¦¬ë° + ì†ë„ ì œì–´â˜…]
                word_buffer = ""
                for chunk in response_stream:
                    if getattr(chunk, "text", None):
                        word_buffer += chunk.text
                        
                        # ê³µë°±ì´ë‚˜ êµ¬ë‘ì ì„ ë§Œë‚˜ë©´ ë²„í¼ë¥¼ ë¹„ìš°ê³  í™”ë©´ ì—…ë°ì´íŠ¸
                        if re.search(r'[\s.,!?\n]', chunk.text):
                            full_response += word_buffer
                            word_buffer = ""
                            # ì†ë„ ì¡°ì ˆì„ ìœ„í•œ ë¯¸ì„¸í•œ ì§€ì—° (0.01ì´ˆ)
                            time.sleep(0.01) 
                            placeholder.markdown(
                                f"<div class='fadein'>{full_response}â–Œ</div>",
                                unsafe_allow_html=True
                            )
                
                # ë§ˆì§€ë§‰ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
                if word_buffer:
                    full_response += word_buffer

                placeholder.markdown(
                    f"<div class='fadein'>{full_response}</div>",
                    unsafe_allow_html=True
                )

            st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{full_response}</div>"})

            # [â˜…í•µì‹¬ ìˆ˜ì • 4: íŒë¡€ ì‹œê°í™”â˜…] ìµœì¢… ë³´ê³ ì„œì´ê³ , P-RAG ê²°ê³¼ê°€ ìˆì„ ê²½ìš° í‘œì‹œ
            if _is_final_report(full_response) and similar_precedents:
                # (ê¸°ì¡´ íŒë¡€ ì¹´ë“œ í‘œì‹œ ë¡œì§ ìœ ì§€)
                q_title = _query_title(prompt)
                st.markdown("**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n* ê²€ìƒ‰ ì¿¼ë¦¬: `[" + q_title + "]`\n")

                for case_data in similar_precedents[:3]:
                    # íŒŒì‹± ì‹¤í–‰
                    case = _parse_precedent_block(case_data['raw_text'])
                    sim_pct = int(round(case_data["similarity"] * 100))
                    
                    label = f"íŒë¡€ [{case.get('title','ì œëª© ì—†ìŒ')}]"
                    if case.get("court") and case.get("case_no"):
                        label += f" â€” {case['court']} {case['case_no']}"

                    item_md = (
                        f"* {label}  \n"
                        f"  - ì„ ê³ : {case.get('date','').strip()} {case.get('court','').strip()} | ìœ ì‚¬ë„: {sim_pct}%  \n"
                        f"  - íŒê²°ìš”ì§€: {case.get('holding','').strip()}  \n"
                        f"  - ì „ë¬¸ ì¼ë¶€: \"{case.get('excerpt','').strip()}\""
                    )
                    st.markdown(item_md)

        except Exception as e:
            err = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(err)
            st.session_state.messages.append({"role": "Architect", "content": f"<div class='fadein'>{err}</div>"})
