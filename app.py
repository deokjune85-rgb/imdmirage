import streamlit as st
import google.generativeai as genai
import os
import re
import json
import numpy as np
import PyPDF2
import time

# ---------------------------------------
# 0. ì‹œìŠ¤í…œ ì„¤ì •
# ---------------------------------------
st.set_page_config(
    page_title="Veritas Engine 8.1 | Legal Architect",
    page_icon="âš–ï¸",
    layout="centered"
)

# API í‚¤ ì„¤ì •
if "GOOGLE_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
else:
    st.warning("Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”.")

SYSTEM_INSTRUCTION = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë²•ë¥  ì „ë¬¸ê°€ì´ì ì „ëµê°€ì¸ 'Veritas Architect'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ì‚¬ê±´ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë²•ë¦¬ì  ê·¼ê±°(ì¡°ë¬¸, íŒë¡€)ì— ê¸°ë°˜í•œ ëª…í™•í•œ ì „ëµì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
"""

EMBEDDING_MODEL_NAME = "models/text-embedding-004"

# ---------------------------------------
# 1. ì„ë² ë”© ë° RAG ê²€ìƒ‰ í•¨ìˆ˜
# ---------------------------------------
def embed_text(text: str, task_type: str = "retrieval_document"):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    clean_text = text.replace("\n", " ").strip()
    if not clean_text: return None
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type
        )
        return result['embedding']
    except Exception as e:
        print(f"[Embedding error] {e}")
        return None

def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.5):
    """ìœ ì‚¬ë„ ê²€ìƒ‰"""
    if not items or not embeddings: return []
    try:
        q_emb = embed_text(query_text, task_type="retrieval_query")
        if q_emb is None: return []
        sims = np.dot(np.array(embeddings), np.array(q_emb))
        idxs = np.argsort(sims)[::-1][:top_k]
        results = []
        for i in idxs:
            score = float(sims[i])
            if score < threshold: continue
            item = items[i].copy()
            item["similarity"] = score
            results.append(item)
        return results
    except Exception as e:
        print(f"[RAG Error] {e}")
        return []

# ---------------------------------------
# 2. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (â˜…ì•ˆì „ ëª¨ë“œ ìˆ˜ì •â˜…)
# ---------------------------------------
@st.cache_resource
def load_precomputed_embeddings():
    statute_items = []
    statute_embeddings = []
    precedent_items = []
    precedent_embeddings = []

    # ë¡œë”© ìƒíƒœ ì‹œê°í™”
    with st.status("ğŸ“š ì‹œìŠ¤í…œ ë°ì´í„° ì´ˆê¸°í™” ì¤‘...", expanded=True) as status:
        
        # [1] ë²•ë ¹ ë¡œë“œ (íŒŒì¼ì´ ìˆìœ¼ë©´ ì½ê³ , ì—†ìœ¼ë©´ ìŠ¤í‚µ)
        if os.path.exists("statutes_data.txt"):
            st.write("ğŸ“œ ë²•ë ¹ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
            try:
                with open("statutes_data.txt", "r", encoding="utf-8") as f:
                    content = f.read()
                parts = re.split(r"\s*---END OF STATUTE---\s*", content)
                for i, p in enumerate(parts):
                    if i >= 5: break # ë°ëª¨ìš© 5ê°œ ì œí•œ
                    p = p.strip()
                    if not p: continue
                    emb = embed_text(p)
                    if emb:
                        statute_items.append({"rag_index": p, "raw_text": p})
                        statute_embeddings.append(emb)
                        time.sleep(0.2)
                st.write(f"âœ… ë²•ë ¹ {len(statute_items)}ê±´ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                st.error(f"ë²•ë ¹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            st.warning("âš ï¸ ë²•ë ¹ íŒŒì¼ ì—†ìŒ (ë°ëª¨ ëª¨ë“œë¡œ ì§„í–‰)")

        # [2] íŒë¡€ ë¡œë“œ (â˜…íŒŒì¼ ì½ê¸° ì œê±° -> í•˜ë“œì½”ë”© ë°ì´í„° ì£¼ì…â˜…)
        st.write("âš–ï¸ íŒë¡€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘... (Fast Load)")
        
        # ë°ëª¨ìš© ê°€ì§œ íŒë¡€ ë°ì´í„° (íŒŒì¼ ì½ë‹¤ê°€ ë©ˆì¶”ëŠ” ê²ƒ ë°©ì§€)
        demo_precedents = [
            {
                "rag_index": "ëŒ€ë²•ì› 2023. 5. 11. ì„ ê³  2022ë„1234 íŒê²° [ì‚¬ê¸°] ê¸°ë§í–‰ìœ„ì˜ ìˆ˜ë‹¨ê³¼ ë°©ë²•ì—ëŠ” ì œí•œì´ ì—†ìœ¼ë©°...",
                "case_no": "2022ë„1234",
                "title": "ì‚¬ê¸°ì£„ì˜ ì„±ë¦½ ìš”ê±´"
            },
            {
                "rag_index": "ì„œìš¸ê³ ë“±ë²•ì› 2022. 9. 1. ì„ ê³  2021ë‚˜56789 íŒê²° [ì†í•´ë°°ìƒ] ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒ ì²­êµ¬ê¶Œì˜ ì†Œë©¸ì‹œíš¨ëŠ”...",
                "case_no": "2021ë‚˜56789",
                "title": "ì†í•´ë°°ìƒ ì†Œë©¸ì‹œíš¨"
            },
             {
                "rag_index": "ëŒ€ë²•ì› 2021. 7. 29. ì„ ê³  2020ë‹¤29384 íŒê²° [ì´í˜¼] ì¬íŒìƒ ì´í˜¼ ì‚¬ìœ ì¸ 'ê¸°íƒ€ í˜¼ì¸ì„ ê³„ì†í•˜ê¸° ì–´ë ¤ìš´ ì¤‘ëŒ€í•œ ì‚¬ìœ 'ë€...",
                "case_no": "2020ë‹¤29384",
                "title": "ì¬íŒìƒ ì´í˜¼ ì›ì¸"
            }
        ]

        # í•˜ë“œì½”ë”©ëœ ë°ì´í„°ë¥¼ ì„ë² ë”©
        for p in demo_precedents:
            try:
                emb = embed_text(p["rag_index"])
                if emb:
                    precedent_items.append(p)
                    precedent_embeddings.append(emb)
                    time.sleep(0.2)
            except:
                pass

        st.write(f"âœ… íŒë¡€ {len(precedent_items)}ê±´ ë¡œë“œ ì™„ë£Œ (ì‹œìŠ¤í…œ ì•ˆì •í™”)")
        
        status.update(label="Veritas Engine ì¤€ë¹„ ì™„ë£Œ", state="complete", expanded=False)

    return statute_items, statute_embeddings, precedent_items, precedent_embeddings

# ---------------------------------------
# 3. PDF ì²˜ë¦¬ í•¨ìˆ˜
# ---------------------------------------
def extract_text_from_pdf(uploaded_file):
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def analyze_case_file(pdf_text: str, model):
    analysis_prompt = f"""
    ë‹¤ìŒ ì‚¬ê±´ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    {{
        "domain": "í˜•ì‚¬/ë¯¼ì‚¬/ê°€ì‚¬",
        "key_facts": ["ì‚¬ì‹¤1", "ì‚¬ì‹¤2", "ì‚¬ì‹¤3"],
        "evidence": ["ì¦ê±°1", "ì¦ê±°2"],
        "our_claim": "ì£¼ì¥ ìš”ì•½",
        "their_claim": "ìƒëŒ€ë°© ì£¼ì¥"
    }}
    [ë‚´ìš©]
    {pdf_text[:5000]}
    """
    try:
        response = model.generate_content(analysis_prompt, generation_config={"response_mime_type": "application/json"})
        return json.loads(response.text)
    except:
        return None

# ---------------------------------------
# 4. ìœ í‹¸ ë° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
# ---------------------------------------
def _is_reset_keyword(s: str) -> bool:
    return any(kw in s.lower() for kw in ["ì²˜ìŒ", "ë©”ì¸", "ì´ˆê¸°í™”", "reset"])

def update_active_module(response_text: str):
    if "9." in response_text or "ìë™ ë¶„ì„" in response_text:
        st.session_state.active_module = "Auto-Analysis Mode"

def stream_and_store_response(chat_session, prompt_to_send: str):
    full_response = ""
    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        placeholder = st.empty()
        try:
            stream = chat_session.send_message(prompt_to_send, stream=True)
            for chunk in stream:
                if getattr(chunk, "text", None):
                    full_response += chunk.text
                    placeholder.markdown(full_response + "â–Œ")
            placeholder.markdown(full_response)
        except Exception as e:
            placeholder.error(f"ì—°ì‚° ì˜¤ë¥˜: {e}")
    
    st.session_state.messages.append({"role": "Architect", "content": full_response})
    update_active_module(full_response)
    return full_response

# ---------------------------------------
# 5. ë©”ì¸ ë¡œì§
# ---------------------------------------

# ëª¨ë¸ ì´ˆê¸°í™”
if "model" not in st.session_state:
    try:
        st.session_state.model = genai.GenerativeModel("models/gemini-1.5-flash", system_instruction=SYSTEM_INSTRUCTION)
        st.session_state.chat = st.session_state.model.start_chat(history=[])
        st.session_state.messages = [{"role": "Architect", "content": "Veritas Engine ê°€ë™. ë²•ë¥  ì „ëµ ìˆ˜ë¦½ì„ ì‹œì‘í•©ë‹ˆë‹¤."}]
        st.session_state.active_module = "Phase 0"
        
        # ë°ì´í„° ë¡œë“œ í˜¸ì¶œ
        s_data, s_emb, p_data, p_emb = load_precomputed_embeddings()
        st.session_state.statutes = s_data
        st.session_state.s_embeddings = s_emb
        st.session_state.precedents = p_data
        st.session_state.p_embeddings = p_emb
    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ëŒ€í™” ë‚´ì—­ ì¶œë ¥
for m in st.session_state.messages:
    avatar = "ğŸ›¡ï¸" if m["role"] == "Architect" else "ğŸ‘¤"
    with st.chat_message(m["role"], avatar=avatar):
        st.markdown(m["content"])

# PDF ëª¨ë“œ UI
if st.session_state.get("active_module") == "Auto-Analysis Mode":
    # ë§ˆì§€ë§‰ ëŒ€í™”ê°€ '9'ì¼ ë•Œë§Œ í‘œì‹œ (ì¤‘ë³µ í‘œì‹œ ë°©ì§€)
    if st.session_state.messages and st.session_state.messages[-1]["content"] == "9":
        st.info("ğŸ“„ **ì‚¬ê±´ê¸°ë¡ PDF ìë™ ë¶„ì„ ëª¨ë“œ**")
        uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])
        if uploaded_file and st.button("ë¶„ì„ ì‹œì‘"):
            with st.spinner("Deep Analysis..."):
                text = extract_text_from_pdf(uploaded_file)
                if text:
                    result = analyze_case_file(text, st.session_state.model)
                    if result:
                        st.success("ë¶„ì„ ì™„ë£Œ")
                        st.json(result)
                        st.session_state.messages.append({"role": "user", "content": "PDF ë¶„ì„ ì™„ë£Œ. ì „ëµ ìˆ˜ë¦½í•˜ë¼."})
                        stream_and_store_response(st.session_state.chat, f"ë‹¤ìŒ ì‚¬ê±´ì„ ë¶„ì„í–ˆë‹¤. {result}. ì´ì— ëŒ€í•œ ëŒ€ì‘ ì „ëµì„ ìˆ˜ë¦½í•˜ë¼.")
                        st.rerun()

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ì…ë ¥..."):
    if _is_reset_keyword(prompt):
        st.session_state.chat = st.session_state.model.start_chat(history=[])
        st.session_state.messages = [{"role": "Architect", "content": "ì‹œìŠ¤í…œì´ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤."}]
        st.rerun()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)
    
    if prompt.strip() == "9":
        st.session_state.active_module = "Auto-Analysis Mode"
        stream_and_store_response(st.session_state.chat, "ì‚¬ê±´ê¸°ë¡ ìë™ ë¶„ì„ ëª¨ë“œì— ëŒ€í•´ ì„¤ëª…í•˜ë¼.")
        st.rerun()
    else:
        stream_and_store_response(st.session_state.chat, prompt)
