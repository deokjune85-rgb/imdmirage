import streamlit as st
import google.generativeai as genai
import os 
import requests 
import re 
import numpy as np 

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol) ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ì—”ì§„ ë²„ì „ 7.0", page_icon="ğŸ›¡ï¸", layout="centered")

hide_streamlit_style = """
<style>
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
header {visibility: hidden;}
.stDeployButton {visibility: hidden;}
</style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.0")
st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- 3. API í‚¤ ë° ëª¨ë¸ ì„¤ì • ---

try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì‹œìŠ¤í…œ ì˜¤ë¥˜: 'GOOGLE_API_KEY' 'ì•½íƒˆ' ì‹¤íŒ¨. 'Secrets'ë¥¼ 'í™•ì¸'í•˜ë¼.")
    st.stop()

genai.configure(api_key=API_KEY)

# --- â˜…â˜…â˜… 'íƒ„ì•½ê³  B': ë²•ì œì²˜ API (ìíŒê¸°) â˜…â˜…â˜… ---
try:
    OC_KEY = st.secrets["LAW_API_KEY"]
except KeyError:
    OC_KEY = "DEOKJUNE_FALLBACK"

def get_precedent_full(prec_id):
    if OC_KEY == "DEOKJUNE_FALLBACK":
        return {"error": "[ì¹˜ëª…ì  ì˜¤ë¥˜]: 'LAW_API_KEY' 'ì•½íƒˆ' ì‹¤íŒ¨. [st.secrets]ë¥¼ 'í™•ì¸'í•˜ë¼."}
    url = "http://www.law.go.kr/DRF/lawService.do"
    params = {"OC": OC_KEY, "target": "prec", "ID": prec_id, "type": "JSON"}
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status() 
        data = r.json()
        if 'íŒë¡€ì •ë³´' not in data:
             return {"error": f"ë²•ì œì²˜ API ì˜¤ë¥˜: {data.get('Error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ')}"}
        return data
    except requests.exceptions.RequestException as e:
        return {"error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}"}

def show_full_precedent(prec_id):
    data = get_precedent_full(prec_id)
    if "error" in data:
        return f"--- \n**[API ë¶„ì„ ì‹¤íŒ¨]** (ID: {prec_id})\n{data['error']}\n---"
    try:
        info = data.get('íŒë¡€ì •ë³´', {})
        if not info:
             return f"--- \n**[API ë¶„ì„ ì‹¤íŒ¨]** (ID: {prec_id})\n'íŒë¡€ì •ë³´' í•„ë“œë¥¼ 'ë°ì´í„°'ì—ì„œ 'ì‹ë³„'í•  ìˆ˜ ì—†ìŒ.\n---"
        prec_id_display = info.get('íŒë¡€ì¼ë ¨ë²ˆí˜¸', prec_id)
        title = info.get('ì‚¬ê±´ëª…', 'N/A')
        verdict_date = info.get('ì„ ê³ ì¼ì', 'N/A')
        court_name = info.get('ë²•ì›ëª…', 'N/A')
        summary = info.get('íŒê²°ìš”ì§€', 'N/A').replace(chr(10), ' ') 
        full_text = info.get('íŒë¡€ë‚´ìš©', 'N/A')[:500].replace(chr(10), ' ')
        ref_law = info.get('ì°¸ì¡°ì¡°ë¬¸', 'N/A').replace(chr(10), ' ')
        return f"""
---
**ğŸ” íŒë¡€ ì „ë¬¸ ì „ì²´ (ë²•ì œì²˜ ì‹¤ì‹œê°„ í˜¸ì¶œ)**
**ì‚¬ê±´ëª…**: {title}
**ì„ ê³ **: {verdict_date} | **ë²•ì›**: {court_name}
**íŒë¡€ ë§í¬**: [ë²•ì œì²˜ ë°”ë¡œê°€ê¸°](http://www.law.go.kr/precInfo.do?precSeq={prec_id_display})
**íŒê²°ìš”ì§€**: {summary}
**ì „ë¬¸ ì¼ë¶€ (500ì)**: {full_text}...
**ì°¸ì¡°ì¡°ë¬¸**: {ref_law}
---
"""
    except Exception as e:
        return f"--- \n**[API ë¶„ì„ ì‹¤íŒ¨]** (ID: {prec_id})\n'ë°ì´í„°' 'ê°€ê³µ' ì¤‘ 'ì¹˜ëª…ì  ì˜¤ë¥˜' ë°œìƒ: {e}\n---"
# --- (ë²•ì œì²˜ API ì¢…ë£Œ) ---


# --- â˜…â˜…â˜… 'íƒ„ì•½ê³  A': ê²Œë¦´ë¼ RAG (íŠ¸ë¡œì´ ëª©ë§ˆ) â˜…â˜…â˜… ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004" 

def embed_text(text, task_type="RETRIEVAL_DOCUMENT"):
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=text,
            task_type=task_type)
        return result['embedding']
    except Exception as e:
        st.error(f"ì„ë² ë”© 'ì˜¤ë¥˜' (ëª¨ë¸ 'í˜¸ì¶œ' 'ì‹¤íŒ¨'): {e}")
        return None

@st.cache_data(show_spinner=False)
def load_and_embed_precedents(file_path='precedents_data.txt'):
    """'txt' 'ì“°ë ˆê¸°'ë¥¼ 'ì½ì–´' 'ë²¡í„°' 'íƒ„ì•½'ìœ¼ë¡œ 'ì£¼ì¡°'í•œë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        st.warning(f"ê²½ê³ : 'íƒ„ì•½ê³ ({file_path})' 'ë°œê²¬' 'ì‹¤íŒ¨'. 'ê²Œë¦´ë¼ RAG'ê°€ 'ì‘ë™'í•˜ì§€ 'ì•ŠëŠ”ë‹¤'.")
        # --- â˜…â˜…â˜… 'ì˜¤ë¥˜' 'ìˆ˜ì •' (v4.1) â˜…â˜…â˜… ---
        # '3ê°œ'ê°€ 'ì•„ë‹ˆë¼' '2ê°œ'ì˜ 'ì“°ë ˆê¸°'ë¥¼ 'ë°˜í™˜'í•œë‹¤.
        return [], np.array([])
    except Exception as e:
        st.error(f"'íƒ„ì•½ê³ ' 'ë¡œë“œ' 'ì‹¤íŒ¨': {e}")
        return [], np.array([]) # '2ê°œ' 'ë°˜í™˜'

    precedents = content.split('---END OF PRECEDENT---')
    precedents = [p.strip() for p in precedents if p.strip()]
    
    if not precedents:
        st.warning(f"ê²½ê³ : 'íƒ„ì•½ê³ ({file_path})'ê°€ 'ë¹„ì–´'ìˆë‹¤. 'ì‚¬ê¸°ê·¹' 'ì‹¤íŒ¨'.")
        return [], np.array([]) # '2ê°œ' 'ë°˜í™˜'

    st.success(f"'{file_path}' 'íƒ„ì•½ê³ ' 'ì¥ì „' 'ì™„ë£Œ'. 'ì´ì•Œ(íŒë¡€)' {len(precedents)}ê°œ 'í™•ì¸'.")
    embeddings = []
    valid_precedents = []
    for p in precedents:
        emb = embed_text(p)
        if emb:
            embeddings.append(emb)
            valid_precedents.append(p)
    
    # 'ì´ì•Œ(í…ìŠ¤íŠ¸)'ê³¼ 'ì¸ì‹í‘œ(ë²¡í„°)'ë¥¼ 'ë°˜í™˜'í•œë‹¤.
    return valid_precedents, np.array(embeddings)

def find_similar_precedents(query_text, precedents, embeddings, top_k=3):
    """'ì‚¬ê±´'ê³¼ 'ê°€ì¥' 'ìœ ì‚¬í•œ' 'ì´ì•Œ' 3ê°œë¥¼ 'ë°œì‚¬'í•œë‹¤."""
    if embeddings.size == 0:
        return "" # 'íƒ„ì•½ê³ 'ê°€ 'ë¹„ì—ˆ'ë‹¤.

    query_embedding = embed_text(query_text, task_type="RETRIEVAL_QUERY")
    if query_embedding is None:
        return ""

    similarities = np.dot(embeddings, query_embedding)
    
    top_k_indices = np.argsort(similarities)[-top_k:][::-1]
    
    context = "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: 'ê²Œë¦´ë¼ RAG'ê°€ 'íƒ„ì•½ê³ (txt)'ì—ì„œ 'ìœ ì‚¬ íŒë¡€' 'íƒì§€' 'ì™„ë£Œ']\n"
    for i in top_k_indices:
        if similarities[i] > 0.7: 
            context += f"--- (ìœ ì‚¬ë„: {similarities[i]*100:.0f}%)\n{precedents[i]}\n---\n"
            
    return context
# --- â˜…â˜…â˜… ê²Œë¦´ë¼ RAG ì´ì‹ ì¢…ë£Œ â˜…â˜…â˜… ---


# --- 'ë‡Œ(EPE)'ì™€ 'íƒ„ì•½ê³ ' 'ë¡œë”©' ---
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
except FileNotFoundError:
    st.error("'system_prompt.txt' íŒŒì¼ì„ 'ì•½íƒˆ'í•˜ëŠ” ë° 'ì‹¤íŒ¨'í–ˆë‹¤, ì´ ë¨¸ì €ë¦¬ì•¼. 'íŒŒì¼'ì„ 'ì—…ë¡œë“œ'í•´.")
    st.stop()

# 'íƒ„ì•½ê³  A(RAG)' 'ì¥ì „' (ì•± 'ì‹œì‘' ì‹œ '1íšŒ' 'ì‹¤í–‰')
# --- â˜…â˜…â˜… 'ì˜¤ë¥˜' 'ìˆ˜ì •' ì§€ì  (v4.1) â˜…â˜…â˜… ---
# '171ë²ˆ' 'ë¼ì¸'ì´ 'ì—¬ê¸°'ë‹¤. 'load_and_embed_precedents'ëŠ” 'ì´ì œ' '2ê°œ'ë§Œ 'ë°˜í™˜'í•œë‹¤.
if "precedents" not in st.session_state:
    st.session_state.precedents, st.session_state.embeddings = load_and_embed_precedents()

if "model" not in st.session_state:
    st.session_state.model = genai.GenerativeModel(
        "gemini-2.5-flash", 
        system_instruction=SYSTEM_INSTRUCTION
    )

# --- 4. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state:
    st.session_state.chat = st.session_state.model.start_chat(history=[])
    
    initial_prompt = "ì‹œìŠ¤í…œ ê°€ë™. 'ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ì—¬ Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
    try:
        response = st.session_state.chat.send_message(initial_prompt)
        st.session_state.messages.append({"role": "Architect", "content": response.text})
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    role_name = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(message["content"])

# --- 5. ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…ê¶ê·¹ì˜ ìœµí•© êµë¦¬â˜…) ---
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
        try:
            # --- â˜… 1. 'ê²Œë¦´ë¼ RAG' 'ì„ ì œ' 'ë°œì‚¬' â˜… ---
            with st.spinner("'íƒ„ì•½ê³  A(txt)'ì—ì„œ 'ìœ ì‚¬ íŒë¡€' 'íƒìƒ‰' ì¤‘..."):
                rag_context = find_similar_precedents(
                    prompt, 
                    st.session_state.precedents, 
                    st.session_state.embeddings
                )
            
            final_prompt_to_epe = prompt + rag_context

            # --- â˜… 2. 'ë‡Œ(EPE)' 'ì‘ë™' â˜… ---
            response_stream = st.session_state.chat.send_message(final_prompt_to_epe, stream=True)
            
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                response_placeholder = st.empty()
                full_response = ""
                for chunk in response_stream:
                    full_response += chunk.text
                    response_placeholder.markdown(full_response + "â–Œ") 
                
                # --- â˜… 3. 'ìíŒê¸°(API)' 'í›„ì²˜ë¦¬' â˜… ---
                if any(x in prompt.lower() for x in ["íŒë¡€", "ì „ë¬¸", "ë³¸ë¬¸", "íŒê²°ë¬¸", "ì „ì²´", "ì•„ì´ë””"]):
                    ids = re.findall(r'\d{6,8}', prompt) 
                    if ids:
                        with st.spinner(f"ë²•ì œì²˜ API í˜¸ì¶œ... íŒë¡€ ID {', '.join(ids)} 'ì‹¤ì‹œê°„ ì•½íƒˆ' ì¤‘..."):
                            for pid in ids[:3]:
                                precedent_text = show_full_precedent(pid)
                                full_response += "\n\n" + precedent_text
                
                response_placeholder.markdown(full_response) 
            
            st.session_state.messages.append({"role": "Architect", "content": full_response})
        
        except Exception as e:
            error_msg = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_msg)
            st.session_state.messages.append({"role": "Architect", "content": error_msg})
