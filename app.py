import streamlit as st
import google.generativeai as genai
import os
import numpy as np  # RAG ì—”ì§„ì„ ìœ„í•œ ë²¡í„° ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬

# =====================================================
# 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol)
# =====================================================
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.0", page_icon="ğŸ›¡ï¸", layout="centered")

# CSS í•´í‚¹ (ì‹ ê¸°ë£¨ í”„ë¡œí† ì½œ)
st.markdown("""
<style>
#MainMenu, footer, header, .stDeployButton {visibility: hidden;}

body {
    background-color: #FFFFFF !important;
    color: #000000 !important;
    font-family: 'Noto Sans KR', sans-serif !important;
    line-height: 1.7 !important;
    font-size: 17px !important;
    margin-top: -10px !important; /* ìƒë‹¨ ì—¬ë°± ì¤„ì´ê¸° */
}

h1 {
    color: #003399 !important; /* ì§„íŒŒë‘ */
    font-weight: 900 !important;
    font-size: 33px !important;
    margin-top: 15px !important;
    margin-bottom: 10px !important;
}

/* ì „ì²´ ê¸€ììƒ‰ ë¸”ë™ í†µì¼ */
div, span, p, li {
    color: #000000 !important;
}

/* ì¤‘ìš” ë¬¸ë‹¨ ì œëª© ì§„íŒŒë‘ ì²˜ë¦¬ */
strong, b {
    color: #003399 !important;
}

/* ê·¸ë¼ë°ì´ì…˜ í…ìŠ¤íŠ¸ íš¨ê³¼ */
.gradient-text {
    background: linear-gradient(90deg, #003399, #0066cc, #0099ff);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    font-weight: 700;
}
</style>
""", unsafe_allow_html=True)

# =====================================================
# 2. íƒ€ì´í‹€ ë° ê²½ê³ 
# =====================================================
st.markdown("<br><br>", unsafe_allow_html=True)  # ìƒë‹¨ ì—¬ë°± 2ì¤„ ìœ ì§€
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.0")

st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# =====================================================
# 3. API í‚¤ ë° ëª¨ë¸ ì„¤ì •
# =====================================================
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError:
    st.error("ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. (API Key ëˆ„ë½)")
    st.stop()

genai.configure(api_key=API_KEY)

# =====================================================
# 4. [ì‘ì „ëª…: íŠ¸ë¡œì´ ëª©ë§ˆ] ê²Œë¦´ë¼ RAG ì—”ì§„
# =====================================================
EMBEDDING_MODEL_NAME = "models/text-embedding-004"

def embed_text(text, task_type="retrieval_document"):
    try:
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text:
            return None
        result = genai.embed_content(model=EMBEDDING_MODEL_NAME, content=clean_text, task_type=task_type)
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}")
        return None

@st.cache_data
def load_and_embed_precedents(file_path='precedents_data.txt'):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return [], []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return [], []

    precedents = [p.strip() for p in content.split('---END OF PRECEDENT---') if p.strip()]
    embeddings, valid_precedents = [], []
    for precedent in precedents:
        embedding = embed_text(precedent)
        if embedding:
            embeddings.append(embedding)
            valid_precedents.append(precedent)
    print(f"Successfully loaded and embedded {len(valid_precedents)} precedents.")
    return valid_precedents, embeddings

def find_similar_precedents(query_text, precedents, embeddings, top_k=3):
    if not embeddings or not precedents:
        return []
    query_embedding = embed_text(query_text, task_type="search_query")
    if query_embedding is None:
        return []
    embeddings_np = np.array(embeddings)
    query_embedding_np = np.array(query_embedding)
    similarities = np.dot(embeddings_np, query_embedding_np)
    top_k_indices = np.argsort(similarities)[::-1][:top_k]
    results = []
    for idx in top_k_indices:
        if similarities[idx] > 0.6:
            results.append(f"[ìœ ì‚¬ íŒë¡€ ë°œê²¬ (ìœ ì‚¬ë„: {similarities[idx]:.2f})]\n{precedents[idx]}\n---\n")
    return results

# =====================================================
# 5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
# =====================================================
with open("system_prompt.txt", "r", encoding="utf-8") as f:
    SYSTEM_INSTRUCTION = f.read()

if "model" not in st.session_state:
    st.session_state.model = genai.GenerativeModel(
        "gemini-2.5-flash",
        system_instruction=SYSTEM_INSTRUCTION
    )

# =====================================================
# 6. ì„¸ì…˜ ë° ìë™ ì‹œì‘
# =====================================================
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state:
    st.session_state.chat = st.session_state.model.start_chat(history=[])
    initial_prompt = "ì‹œìŠ¤í…œ ê°€ë™. 'ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ì—¬ Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
    try:
        response = st.session_state.chat.send_message(initial_prompt)
        st.session_state.messages.append({"role": "Architect", "content": response.text})
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# =====================================================
# 7. ëŒ€í™” í‘œì‹œ
# =====================================================
for message in st.session_state.messages:
    role_name = "Client" if message["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if message["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(f"<div class='gradient-text'>{message['content']}</div>", unsafe_allow_html=True)

# =====================================================
# 8. ì…ë ¥ ë° íŒë¡€ ê²°ê³¼ (ìµœì¢… Phaseì—ì„œë§Œ)
# =====================================================
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
        try:
            response_stream = st.session_state.chat.send_message(prompt, stream=True)
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                response_placeholder = st.empty()
                full_response = ""
                for chunk in response_stream:
                    full_response += chunk.text
                    response_placeholder.markdown(f"<div class='gradient-text'>{full_response}â–Œ</div>", unsafe_allow_html=True)
                response_placeholder.markdown(f"<div class='gradient-text'>{full_response}</div>", unsafe_allow_html=True)

            # âœ… íŒë¡€ëŠ” ë§ˆì§€ë§‰ Phase í‚¤ì›Œë“œê°€ í¬í•¨ë  ë•Œë§Œ ì¶œë ¥
            if any(k in prompt for k in ["ìµœì¢…ê²°ë¡ ", "ìµœì¢… ë¶„ì„", "ê²°ê³¼ ìš”ì•½", "ìµœì¢… Phase", "ë§ˆì§€ë§‰ ë‹¨ê³„"]):
                precedents, embeddings = load_and_embed_precedents()
                results = find_similar_precedents(prompt, precedents, embeddings)
                if results:
                    st.markdown("<br><b>ğŸ” ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„</b>", unsafe_allow_html=True)
                    for r in results:
                        st.markdown(f"<div class='gradient-text'>{r}</div>", unsafe_allow_html=True)

            st.session_state.messages.append({"role": "Architect", "content": full_response})
        except Exception as e:
            error_msg = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_msg)
            st.session_state.messages.append({"role": "Architect", "content": error_msg})
