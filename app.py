# ======================================================
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.7 â€” Contextual Dual RAG + File Upload + Relay Mechanism
# ğŸ›¡ï¸ ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1 â€” Domain ë©”ë‰´ ê°œì„  + Dual RAG (TXT/JSONL í•˜ì´ë¸Œë¦¬ë“œ)
# ======================================================

import streamlit as st
import google.generativeai as genai
import os
import numpy as np
import re
import time
import json
# â˜…ì‹ ê·œ ì„í¬íŠ¸â˜…
from pdfminer.high_level import extract_text
import io

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • ë° CSS (ê¸°ì¡´ ìœ ì§€) ---
st.set_page_config(page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 7.7", page_icon="ğŸ›¡ï¸", layout="centered")
# ---------------------------------------
# 0. ê¸°ë³¸ ì„¸íŒ…
# ---------------------------------------
st.set_page_config(
    page_title="ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1",
    page_icon="ğŸ›¡ï¸",
    layout="centered"
)

# (CSS ë‚´ìš© ìƒëµ - ì´ì „ ë²„ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ë˜, color: #FFFFFF ê°•ì œëŠ” ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸)
custom_css = """<style>...</style>""" 
# (CSS ë‚´ìš© ìœ ì§€)
custom_css = """
<style>
#MainMenu, footer, header, .stDeployButton {visibility:hidden;}
html, body, div, span, p {
    font-family: 'Noto Sans KR', sans-serif !important;
    font-size: 16px !important;
    line-height: 1.7 !important;
}
h1 {
    text-align: left !important;
    font-weight: 900 !important;
    font-size: 36px !important;
    margin-top: 10px !important;
    margin-bottom: 15px !important;
}
strong, b { font-weight: 700; }
.fadein { animation: fadeInText 0.5s ease-in-out forwards; opacity: 0; }
@keyframes fadeInText {
    from {opacity: 0; transform: translateY(3px);}
    to {opacity: 1; transform: translateY(0);}
}
[data-testid="stChatMessageContent"] {
    font-size: 16px !important;
}
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  ---
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ ë²„ì „ 7.7")
st.warning("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
# ìƒë‹¨ íƒ€ì´í‹€ + ê²½ê³ 
st.title("ë² ë¦¬íƒ€ìŠ¤ ì—”ì§„ 8.1")
st.caption("Phase 0: ë„ë©”ì¸ ì„ íƒ â†’ ì´í›„ Architectê°€ ìë™ ë¼ìš°íŒ…")

st.warning(
    "ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. "
    "ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
)

# --- 3. API í‚¤ ë° RAG ì—”ì§„ ì„¤ì • ---
# ---------------------------------------
# 1. API í‚¤ ì„¤ì •
# ---------------------------------------
try:
API_KEY = st.secrets["GOOGLE_API_KEY"]
if not API_KEY:
         raise ValueError("API Key is empty.")
        raise ValueError("API Key is empty.")
genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. {e}")
st.stop()

# --- [RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜] (ê¸°ì¡´ ìœ ì§€) ---
# (embed_text, load_and_embed_data, find_similar_items, _parse_precedent_block í•¨ìˆ˜ ìœ ì§€ - ìƒëµ)
# ---------------------------------------
# 2. ì„ë² ë”© / RAG ìœ í‹¸ (ê¸°ì¡´ ìœ ì§€)
# ---------------------------------------
EMBEDDING_MODEL_NAME = "models/text-embedding-004"
def embed_text(text, task_type="retrieval_document"): ...
@st.cache_data
def load_and_embed_data(file_path, separator_regex=None): ...
def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.50): ...
def _parse_precedent_block(text: str) -> dict: ...

# (ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ìœ ì§€)
def _is_menu_input(s: str) -> bool: ...
def _is_final_report(txt: str) -> bool: ...
def _query_title(prompt_text: str) -> str: ...
def update_active_module(response_text): ...

# [â˜…ì‹ ì„¤â˜…] íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
def process_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼(TXT, PDF)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œë‹¤."""
    text = ""
    try:
        if uploaded_file.type == "text/plain":
            # TXT íŒŒì¼ ì½ê¸°
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8", errors="ignore"))
            text = stringio.read()
        elif uploaded_file.type == "application/pdf":
            # PDF íŒŒì¼ ì²˜ë¦¬ (pdfminer ì‚¬ìš©)
            # ì£¼ì˜: ì´ê²ƒì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë©°, ì´ë¯¸ì§€ ê¸°ë°˜ PDFëŠ” ì‹¤íŒ¨í•¨.
            bytes_data = uploaded_file.read()
            text = extract_text(io.BytesIO(bytes_data))
            
            if not text.strip():
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ (ì´ë¯¸ì§€ ê¸°ë°˜ PDF ë“±) - MSP ì…€ë§ í¬ì¸íŠ¸
                return "[íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: PDFì— ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ê¸°ë°˜ PDF(ìŠ¤ìº”ë³¸ ë“±)ëŠ” í˜„ì¬ ë°ëª¨ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (MSP ë²„ì „ì—ì„œ ê³ ì„±ëŠ¥ OCR ì—”ì§„ìœ¼ë¡œ ì§€ì› ì˜ˆì •)]"
        else:
            return "[íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (TXT, PDFë§Œ ê°€ëŠ¥)]"
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì•ˆì •ì„± í™•ë³´)
        MAX_LEN = 50000
        if len(text) > MAX_LEN:
            text = text[:MAX_LEN] + f"\n\n[...ë‚´ìš© ìƒëµë¨ (ìµœëŒ€ ê¸¸ì´ {MAX_LEN}ì ì´ˆê³¼)...]"
        
        return text

def embed_text(text: str, task_type: str = "retrieval_document"):
    clean_text = text.replace("\n", " ").strip()
    if not clean_text: return None
    try:
        result = genai.embed_content(model=EMBEDDING_MODEL_NAME, content=clean_text, task_type=task_type)
        return result["embedding"]
except Exception as e:
        return f"[íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}]"
        print(f"[Embedding error] {e}"); return None

@st.cache_data(show_spinner=True)
def load_and_embed_data(file_path: str, separator_regex: str = None):
    if not os.path.exists(file_path): return [], []
    try:
        with open(file_path, "r", encoding="utf-8") as f: content = f.read()
    except Exception: return [], []
    if not content.strip(): return [], []

    data_items, embeddings = [], []
    if file_path.endswith(".jsonl"):
        for line in content.strip().split("\n"):
            if not line.strip(): continue
            try: obj = json.loads(line.strip())
            except json.JSONDecodeError: continue
            txt = obj.get("rag_index") or obj.get("summary") or ""
            if not txt: continue
            emb = embed_text(txt, task_type="retrieval_document")
            if emb: data_items.append(obj); embeddings.append(emb)
    elif separator_regex:
        parts = re.split(separator_regex, content)
        for p in parts:
            p = p.strip()
            if not p: continue
            emb = embed_text(p, task_type="retrieval_document")
            if emb: data_items.append({"rag_index": p, "raw_text": p}); embeddings.append(emb)
    print(f"[RAG] Loaded {len(data_items)} items from {file_path}")
    return data_items, embeddings

# --- 4. ì‹œìŠ¤í…œ í”„ë¼ì„ ìœ ì „ì ë¡œë“œ ë° ì´ˆê¸°í™” ---
# ... (system_prompt.txt ë¡œë“œ ë° ëª¨ë¸/RAG ì´ˆê¸°í™” ë¡œì§ ìœ ì§€ - ìƒëµ) ...
def find_similar_items(query_text, items, embeddings, top_k=3, threshold=0.5):
    if not items or not embeddings: return []
    q_emb = embed_text(query_text, task_type="retrieval_query")
    if q_emb is None: return []
    sims = np.dot(np.array(embeddings), np.array(q_emb))
    idxs = np.argsort(sims)[::-1][:top_k]
    results = []
    for i in idxs:
        score = float(sims[i])
        if score < threshold: continue
        item = items[i].copy()
        item["similarity"] = score
        results.append(item)
    return results

# --- 5. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
# ... (ìƒëµ) ...
# ---------------------------------------
# 3. ê°ì¢… ìœ í‹¸ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
# ---------------------------------------
def _is_menu_input(s: str) -> bool:
    return bool(re.fullmatch(r"^\s*\d{1,2}(?:-\d{1,2})?\s*$", s))

def _is_final_report(txt: str) -> bool:
    return "ì „ëµ ë¸Œë¦¬í•‘ ë³´ê³ ì„œ" in txt

def _query_title(prompt_text: str) -> str:
    return prompt_text[:67] + "..." if len(prompt_text) > 70 else prompt_text

def update_active_module(response_text: str):
    m = re.search(r"'(.*?)' ëª¨ë“ˆì„ (?:ìµœì¢… )?í™œì„±í™”í•©ë‹ˆë‹¤", response_text)
    if m:
        st.session_state.active_module = m.group(1).strip()
    elif "Phase 0" in response_text and not st.session_state.get("active_module"):
        st.session_state.active_module = "Phase 0 (ë„ë©”ì¸ ì„ íƒ)"

# ---------------------------------------
# 4. ì‹œìŠ¤í…œ í”„ë¼ì„ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
# ---------------------------------------
try:
    with open("system_prompt.txt", "r", encoding="utf-8") as f:
        SYSTEM_INSTRUCTION = f.read()
    if len(SYSTEM_INSTRUCTION) < 100:
        raise ValueError("System prompt is too short.")
except (FileNotFoundError, ValueError) as e:
    st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: system_prompt.txt ë¡œë“œ ì‹¤íŒ¨. {e}")
    st.stop()

# --- 6. ëŒ€í™” ì¶œë ¥ ---
# ... (ìƒëµ) ...
# ---------------------------------------
# 5. Phase 0 â€” ë„ë©”ì¸ ì„ íƒ UI (â˜…ìˆ˜ì •ë¨â˜…)
# ---------------------------------------
# [â˜…ìˆ˜ì •ë¨â˜…] "ì„ íƒ ì•ˆ í•¨" ì˜µì…˜ ì¶”ê°€
DEFAULT_OPTION = "ì„ íƒ ì•ˆ í•¨ (ìë™ íŒë‹¨)"
domain_options = [
    DEFAULT_OPTION,
    "í˜•ì‚¬",
    "ë¯¼ì‚¬",
    "ê°€ì‚¬/ì´í˜¼",
    "íŒŒì‚°Â·íšŒìƒ",
    "í–‰ì •/ì¡°ì„¸",
    "íšŒì‚¬Â·M&A",
    "ì˜ë£Œ/ì‚°ì¬",
    "IPÂ·ì €ì‘ê¶Œ",
    "ê¸°íƒ€(í˜¼í•©)",
]

# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ë° ì €ì¥ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
def stream_and_store_response(chat_session, prompt_to_send, spinner_text="Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
    # ... (í•¨ìˆ˜ ë‚´ìš© ìœ ì§€ - ìƒëµ) ...
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
if "selected_domain" not in st.session_state:
    st.session_state.selected_domain = DEFAULT_OPTION

st.subheader("Phase 0 â€” ì‚¬ê±´ ë„ë©”ì¸ ì„ íƒ")

# ë¼ë””ì˜¤ ë²„íŠ¼ ìƒì„±
selected_domain = st.radio(
    "í˜„ì¬ ì‚¬ê±´ì´ ì†í•œ ì£¼ ë„ë©”ì¸ì„ ì„ íƒí•˜ì„¸ìš”. (ì„ íƒ ì•ˆ í•¨ ì‹œ ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.)",
    domain_options,
    # í˜„ì¬ ì„¸ì…˜ ìƒíƒœì— ë”°ë¼ ì¸ë±ìŠ¤ ì„¤ì •
    index=domain_options.index(st.session_state.selected_domain),
    horizontal=True,
)

# ì„ íƒëœ ë„ë©”ì¸ ì—…ë°ì´íŠ¸
st.session_state.selected_domain = selected_domain
st.info(f"í˜„ì¬ ë„ë©”ì¸ ì„¤ì •: **{selected_domain}**")

# ---------------------------------------
# 6. ëª¨ë¸ & ì„¸ì…˜ ì´ˆê¸°í™”
# ---------------------------------------
if "model" not in st.session_state:
    try:
        # [â˜…ìˆ˜ì •ë¨â˜…] ëª¨ë¸ëª… ì˜¤ë¥˜ ìˆ˜ì •: 'gemini-2.5-flash' -> 'gemini-1.5-flash-latest'
        st.session_state.model = genai.GenerativeModel(
            "models/gemini-2.5-flash",
            system_instruction=SYSTEM_INSTRUCTION,
        )
        st.session_state.chat = st.session_state.model.start_chat(history=[])
    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜): {e}")
        st.stop()

    st.session_state.messages = []
    st.session_state.active_module = f"Phase 0 â€” {selected_domain}"

    # RAG ì½”í¼ìŠ¤ ì§€ì—° ë¡œë”© ì„¤ì •
    st.session_state.precedents = []
    st.session_state.p_embeddings = []
    st.session_state.statutes = []
    st.session_state.s_embeddings = []

    # ì´ˆê¸° ì¸ì‚¬/ë°°ì¹˜
    try:
        # [â˜…ìˆ˜ì •ë¨â˜…] ì´ˆê¸° í”„ë¡¬í”„íŠ¸ì— ë„ë©”ì¸ ì •ë³´ ì „ë‹¬ ë°©ì‹ ê°œì„ 
        domain_info = selected_domain
        if selected_domain == DEFAULT_OPTION:
            domain_info = "ë¯¸ì •ì˜ (ì‹œìŠ¤í…œ ìë™ íŒë‹¨ í•„ìš”)"

        init_prompt = (
            f"ì‹œìŠ¤í…œ ê°€ë™. í˜„ì¬ ì„¤ì •ëœ ë„ë©”ì¸: {domain_info}. "
            f"Phase 0ì—ì„œ ì‚¬ê±´ êµ¬ì¡°ë¥¼ ìŠ¤ìº”í•˜ê³ , ì´í›„ Phase 1~ë¥¼ ë™ì ìœ¼ë¡œ ë¼ìš°íŒ…í•˜ë¼. ë§Œì•½ ë„ë©”ì¸ì´ ë¯¸ì •ì˜ë¼ë©´, ì‚¬ìš©ìì˜ ì²« ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë„ë©”ì¸ì„ íŒë‹¨í•˜ê³  í™œì„±í™”í•˜ë¼."
        )
        resp = st.session_state.chat.send_message(init_prompt)
        init_text = resp.text
    except Exception as e:
        init_text = f"[ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}]"

# --- 7. ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (â˜…í•µì‹¬ ìˆ˜ì •: íŒŒì¼ ì—…ë¡œë“œ í†µí•© + ë¦´ë ˆì´â˜…) ---
    st.session_state.messages.append({"role": "Architect", "content": init_text})
    update_active_module(init_text)

# [â˜…í•µì‹¬ ìˆ˜ì • 1: Phase 2 ìƒíƒœ ê°ì§€â˜…]
# ì…ë ¥ ì²˜ë¦¬ ì „ì— Phase 2 ìƒíƒœë¥¼ ë¨¼ì € í™•ì¸í•˜ì—¬ UIë¥¼ ê²°ì •í•œë‹¤.
is_phase2_active = False
if st.session_state.get("messages"): # ì„¸ì…˜ ìƒíƒœ ì ‘ê·¼ ë°©ì‹ ê°œì„ 
    last_architect_msg = ""
    # ë§ˆì§€ë§‰ Architect ë©”ì‹œì§€ ì°¾ê¸°
    for msg in reversed(st.session_state.messages):
        if msg['role'] == 'Architect':
            last_architect_msg = re.sub('<[^<]+?>', '', msg['content']); break
# ---------------------------------------
# 7. ê³¼ê±° ë©”ì‹œì§€ ë Œë”ë§ (ìë™ ìŠ¤í¬ë¡¤ì€ Streamlit ê¸°ë³¸ ê¸°ëŠ¥)
# ---------------------------------------
for m in st.session_state.messages:
    role_name = "Client" if m["role"] == "user" else "Architect"
    avatar = "ğŸ‘¤" if m["role"] == "user" else "ğŸ›¡ï¸"
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(m["content"], unsafe_allow_html=True)

# ---------------------------------------
# 8. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜ (ê¸°ì¡´ ìœ ì§€ ë° ê°œì„ )
# ---------------------------------------
def stream_and_store_response(chat_session, prompt_to_send: str,
                              spinner_text: str = "Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘..."):
    full_response = ""
    start_time = time.time()
    with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
        placeholder = st.empty()
        try:
            with st.spinner(spinner_text):
                stream = chat_session.send_message(prompt_to_send, stream=True)
                for chunk in stream:
                    # ì‘ë‹µ ìœ íš¨ì„± ê²€ì‚¬ ê°•í™”
                    if not getattr(chunk, "parts", None) or not getattr(chunk, "text", None):
                        # ì‘ë‹µì´ ì—†ê±°ë‚˜ ì•ˆì „ í•„í„°ì— ë§‰í˜”ì„ ê²½ìš° ì²˜ë¦¬
                        if not full_response: # ì²« ì‘ë‹µì´ ë§‰í˜”ì„ ê²½ìš°
                             full_response = "[ì‹œìŠ¤í…œ ê²½ê³ : ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë¨.]"
                        placeholder.error(full_response)
                        break
                    full_response += chunk.text
                    placeholder.markdown(full_response + "â–Œ", unsafe_allow_html=True)
                placeholder.markdown(full_response, unsafe_allow_html=True)
        except Exception as e:
            full_response = f"[ì¹˜ëª…ì  ì˜¤ë¥˜: {e}]"
            placeholder.error(full_response)

    # ì´ì „ ë©”ì‹œì§€ê°€ Phase 2 ë°ì´í„° ìš”ì²­ì´ì—ˆëŠ”ì§€ í™•ì¸ (í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€)
    if "Phase 2:" in last_architect_msg and ("ë°ì´í„°ë¥¼ ì§€ê¸ˆ ì‹œìŠ¤í…œì— ì…ë ¥í•˜ì‹­ì‹œì˜¤" in last_architect_msg or "ì—”ì§„'ì„ ê°€ë™í•˜ì—¬" in last_architect_msg):
        is_phase2_active = True

# [â˜…í•µì‹¬ ìˆ˜ì • 2: ì¡°ê±´ë¶€ íŒŒì¼ ì—…ë¡œë“œ UI í‘œì‹œâ˜…]
uploaded_file = None
input_text = None # ì²˜ë¦¬í•  ìµœì¢… í…ìŠ¤íŠ¸

if is_phase2_active:
    st.info("ğŸ“‚ Phase 2 í™œì„±í™”: ë¶„ì„í•  ì¦ê±° ë°ì´í„°(TXT ë˜ëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ PDF)ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, ì•„ë˜ ì±„íŒ…ì°½ì— í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•˜ì‹­ì‹œì˜¤.")
    # keyë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ì—…ë¡œë” ìƒíƒœë¥¼ ëª…í™•íˆ ê´€ë¦¬
    uploaded_file = st.file_uploader("ì¦ê±° íŒŒì¼ ì—…ë¡œë“œ", type=['txt', 'pdf'], key="phase2_uploader")

# ë©”ì¸ ì…ë ¥ ë£¨í”„
chat_prompt = st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤.")

# [â˜…í•µì‹¬ ìˆ˜ì • 3: ì…ë ¥ ì†ŒìŠ¤ ê²°ì •â˜…]

# 1. íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìœ¼ë©´ íŒŒì¼ ë‚´ìš©ì„ ìš°ì„  ì‚¬ìš© (Streamlitì€ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ìœ¼ë¡œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰í•¨)
if uploaded_file is not None:
    with st.spinner("ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ì¤‘... í…ìŠ¤íŠ¸ ì¶”ì¶œ..."):
        input_text = process_uploaded_file(uploaded_file)
    # íŒŒì¼ ì²˜ë¦¬ í›„ì—ëŠ” input_textê°€ ì±„ì›Œì§„ ìƒíƒœë¡œ ì•„ë˜ ë¡œì§ì„ ì§„í–‰í•¨.

# 2. ì±„íŒ…ì°½ì— ì…ë ¥ì´ ìˆê³ , íŒŒì¼ ì—…ë¡œë“œê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°
elif chat_prompt:
    input_text = chat_prompt

# ì²˜ë¦¬í•  ì…ë ¥ì´ ìˆì„ ê²½ìš° ì‹¤í–‰ (Prompt ë³€ìˆ˜ëª… í†µì¼)
prompt = input_text

if prompt:
    # ì‚¬ìš©ì ì…ë ¥ í‘œì‹œ (ìš”ì•½í•´ì„œ í‘œì‹œ)
    display_text = prompt
    if len(display_text) > 500:
        display_text = display_text[:500] + f"...(ë‚´ìš© ìƒëµë¨: ì´ {len(prompt)}ì)..."
        
    st.session_state.messages.append({"role": "user", "content": f"<div class='fadein'>{display_text}</div>"})
    st.session_state.messages.append({"role": "Architect", "content": full_response})
    update_active_module(full_response)
    end_time = time.time()
    print(f"[LLM] ì‘ë‹µ ì‹œê°„: {end_time - start_time:.2f}s")
    return full_response

# ---------------------------------------
# 9. ë©”ì¸ ì…ë ¥ ë£¨í”„ + Dual RAG
# ---------------------------------------
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤. (ì‚¬ì‹¤ê´€ê³„/ì¦ê±°/ì§ˆë¬¸ ë“± ììœ  ì…ë ¥)"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡/í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(f"<div class='fadein'>{display_text}</div>", unsafe_allow_html=True)
        st.markdown(prompt, unsafe_allow_html=True)

    # Phase ìƒíƒœ í™•ì¸
    is_data_ingestion_phase = "Phase 2" in (st.session_state.active_module or "")

    # RAG ì½”í¼ìŠ¤ ì—†ìœ¼ë©´ ìµœì´ˆ 1íšŒ ë¡œë”©
    if (not st.session_state.statutes) and (not st.session_state.precedents):
        with st.spinner("ë¶„ì„ ì—”ì§„(Dual RAG) ì´ˆê¸°í™” ì¤‘... (ìµœì´ˆ 1íšŒ)"):
            # ë²•ë ¹ TXT
            s_data, s_emb = load_and_embed_data(
                "statutes_data.txt",
                r"\s*---END OF STATUTE---\s*",
            )
            st.session_state.statutes = s_data
            st.session_state.s_embeddings = s_emb

            # íŒë¡€ JSONL â†’ ì—†ìœ¼ë©´ TXT í´ë°±
            p_data, p_emb = load_and_embed_data("precedents_data.jsonl")
            if not p_data:
                # (ê²½ê³  ë©”ì‹œì§€ ìƒëµ)
                p_data, p_emb = load_and_embed_data(
                    "precedents_data.txt",
                    r"\s*---END OF PRECEDENT---\s*",
                )
            st.session_state.precedents = p_data
            st.session_state.p_embeddings = p_emb

    # --- RAG ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½ ---
    rag_context = ""
    similar_precedents = []

    if not _is_menu_input(prompt) and not is_data_ingestion_phase:
        # [â˜…ìˆ˜ì •ë¨â˜…] ë„ë©”ì¸ ì •ë³´ í™œìš© ê°œì„ 
        current_domain = st.session_state.selected_domain
        if current_domain == DEFAULT_OPTION:
            current_domain = "ë¯¸ì •ì˜ (ìë™ íŒë‹¨ ì¤‘)"

        contextual_query = (
            f"í˜„ì¬ í™œì„±í™”ëœ ëª¨ë“ˆ: {st.session_state.active_module}. "
            f"ì„ íƒëœ ë„ë©”ì¸: {current_domain}. "
            f"ì‚¬ìš©ì ì§ˆë¬¸/ì‚¬ì‹¤ê´€ê³„: {prompt}"
        )

        with st.spinner("ì‹¤ì‹œê°„ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... (Dual RAG: ë²•ë ¹/íŒë¡€)"):
            # ë²•ë ¹ ê²€ìƒ‰ (Threshold 0.75 ìœ ì§€)
            if st.session_state.statutes:
                s_hits = find_similar_items(
                    contextual_query,
                    st.session_state.statutes,
                    st.session_state.s_embeddings,
                    top_k=3,
                    threshold=0.75,
                )
                if s_hits:
                    s_texts = [
                        f"[ìœ ì‚¬ë„: {hit['similarity']:.2f}]\n"
                        f"{hit.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n"
                        for hit in s_hits
                    ]
                    rag_context += (
                        "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹ ë°ì´í„°]\n" +
                        "\n".join(s_texts)
                    )

            # íŒë¡€ ê²€ìƒ‰ (Threshold 0.75 ìœ ì§€)
            if st.session_state.precedents:
                similar_precedents = find_similar_items(
                    contextual_query,
                    st.session_state.precedents,
                    st.session_state.p_embeddings,
                    top_k=5,
                    threshold=0.75,
                )
                if similar_precedents:
                    p_texts = [
                        f"[ìœ ì‚¬ë„: {hit['similarity']:.2f}]\n"
                        f"{hit.get('rag_index', 'ë‚´ìš© ì—†ìŒ')}\n---\n"
                        for hit in similar_precedents
                    ]
                    rag_context += (
                        "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" +
                        "\n".join(p_texts)
                    )

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # [â˜…ìˆ˜ì •ë¨â˜…] ë„ë©”ì¸ ì •ë³´ ì „ë‹¬ ë°©ì‹ ê°œì„ 
    current_domain = st.session_state.selected_domain
    if current_domain == DEFAULT_OPTION:
        current_domain = "ë¯¸ì •ì˜ (ì‹œìŠ¤í…œ ìë™ íŒë‹¨ í•„ìš”)"

    final_prompt = (
        f"[í˜„ì¬ ì„¤ì •ëœ ë„ë©”ì¸] {current_domain}\n"
        f"[ì‚¬ìš©ì ì›ë¬¸ ì…ë ¥]\n{prompt}\n"
        f"{rag_context}"
    )
    
    # ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„±
    current_response = stream_and_store_response(
        st.session_state.chat,
        final_prompt,
    )

    # íŒë¡€ ì¹´ë“œ ì‹œê°í™” (ê¸°ì¡´ ìœ ì§€)
    clean_response = re.sub("<[^<]+?>", "", current_response)

    if _is_final_report(clean_response) and similar_precedents:
        q_title = _query_title(prompt)
        st.markdown(
            f"**ğŸ“š ì‹¤ì‹œê°„ íŒë¡€ ì „ë¬¸ ë¶„ì„ (P-RAG ê²°ê³¼)**\n\n"
            f"* ê²€ìƒ‰ ì¿¼ë¦¬: `[{q_title}]`\n"
        )

        for case_data in similar_precedents[:3]:
            sim_pct = int(round(case_data["similarity"] * 100))

            title = case_data.get("title", "ì œëª© ì—†ìŒ")
            case_no = case_data.get("case_no", case_data.get("id", ""))
            court = case_data.get("court", "")
            date = case_data.get("date", "")
            url = case_data.get("url")
            full_text = case_data.get("full_text", case_data.get("raw_text"))

            label = f"íŒë¡€ [{title}]"
            if court and case_no:
                label += f" â€” {court} {case_no}"

            summary = case_data.get("rag_index", "ìš”ì•½ ë‚´ìš© ì—†ìŒ")
            if len(summary) > 200:
                summary = summary[:197] + "..."

            link_md = f"[ğŸ”— ì›ë¬¸ ë§í¬ ë³´ê¸°]({url})" if url else ""

            md = (
                f"* **{label}**\n"
                f"  - ì„ ê³ : {date} | ìœ ì‚¬ë„: {sim_pct}% | {link_md}\n"
                f"  - ë‚´ìš© ìš”ì•½ (RAG Index): {summary}"
            )
            st.markdown(md)

            if full_text:
                with st.expander("ğŸ“„ íŒë¡€ ì „ë¬¸ ë³´ê¸°"):
                    st.text(full_text)

    # (ì´í•˜ Contextual RAG ì‹¤í–‰, ë¦´ë ˆì´ ë©”ì»¤ë‹ˆì¦˜, íŒë¡€ ì‹œê°í™” ë¡œì§ì€ ì´ì „ ë²„ì „(7.6)ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
    # ... (ì—¬ê¸°ì— ì´ì „ ë²„ì „ì˜ RAG ì‹¤í–‰, ì‘ë‹µ ìƒì„±, ë¦´ë ˆì´ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ì–´ë¼) ...
    elif _is_final_report(clean_response) and not similar_precedents and not _is_menu_input(prompt):
        st.info(
            "â„¹ï¸ ë¶„ì„ê³¼ ê´€ë ¨ëœ ìœ ì‚¬ íŒë¡€ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "(ì„ê³„ê°’ 0.75)"
        )
