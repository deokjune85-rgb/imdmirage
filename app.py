import streamlit as st
import google.generativeai as genai
import os
import numpy as np # RAG ì—”ì§„ì„ ìœ„í•œ ë²¡í„° ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- 1. ì‹œìŠ¤í…œ ì„¤ì • (The Vault & Mirage Protocol) ---
st.set_page_config(page_title="ARCHITECT 7.0", page_icon="ğŸ›¡ï¸", layout="centered")

# CSS í•´í‚¹ (ì‹ ê¸°ë£¨ í”„ë¡œí† ì½œ)
hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}
            .stDeployButton {visibility: hidden;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# --- 2. íƒ€ì´í‹€ ë° ê²½ê³  (í™©ì œì˜ êµë¦¬) ---
st.title("ARCHITECT 7.0 [Simulation Engine]")
st.error("ë³´ì•ˆ ê²½ê³ : ë³¸ ì‹œìŠ¤í…œì€ ê²©ë¦¬ëœ ì‚¬ì„¤ í™˜ê²½(The Vault)ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ëŠ” ê¸°ë°€ë¡œ ì·¨ê¸‰ë˜ë©° ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- 3. API í‚¤ ë° ëª¨ë¸ ì„¤ì • (The Engine & EPE/KB) ---
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    if not API_KEY:
        raise ValueError("API Key is empty.")
    genai.configure(api_key=API_KEY)
except (KeyError, ValueError) as e:
    st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: ì—”ì§„ ì—°ê²° ì‹¤íŒ¨. (API Key ëˆ„ë½ ë˜ëŠ” ë¹„ì–´ìˆìŒ): {e}")
    st.stop()

# --- [ì‘ì „ëª…: íŠ¸ë¡œì´ ëª©ë§ˆ] ê²Œë¦´ë¼ RAG ì—”ì§„ í•¨ìˆ˜ ì •ì˜ ---
EMBEDDING_MODEL_NAME = "models/text-embedding-004" # êµ¬ê¸€ ì„ë² ë”© ëª¨ë¸

# í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
def embed_text(text, task_type="retrieval_document"):
    try:
        # í…ìŠ¤íŠ¸ ì •ì œ (ì¤„ë°”ê¿ˆ ì œê±° ë“±)
        clean_text = text.replace('\n', ' ').strip()
        if not clean_text:
            return None
            
        result = genai.embed_content(
            model=EMBEDDING_MODEL_NAME,
            content=clean_text,
            task_type=task_type)
        return result['embedding']
    except Exception as e:
        print(f"Embedding error: {e}") # ì½˜ì†” ë¡œê·¸ ê¸°ë¡
        return None

# íŒë¡€ ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”© í•¨ìˆ˜ (st.cache_dataë¡œ ìºì‹±í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”)
@st.cache_data
def load_and_embed_precedents(file_path='precedents_data.txt'):
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}") # ì½˜ì†” ë¡œê·¸ ê¸°ë¡
        return [], []
    
    # íŒŒì¼ ì½ê¸° ë° íŒë¡€ ë¶„í• 
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}") # ì½˜ì†” ë¡œê·¸ ê¸°ë¡
        return [], []
    
    precedents = content.split('---END OF PRECEDENT---')
    precedents = [p.strip() for p in precedents if p.strip()]
    
    # ê° íŒë¡€ ì„ë² ë”© (ì‹œê°„ ì†Œìš”)
    embeddings = []
    valid_precedents = []
    for precedent in precedents:
        embedding = embed_text(precedent)
        if embedding:
            embeddings.append(embedding)
            valid_precedents.append(precedent)
    
    print(f"Successfully loaded and embedded {len(valid_precedents)} precedents.") # ì½˜ì†” ë¡œê·¸ ê¸°ë¡
    return valid_precedents, embeddings

# ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ í•¨ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
def find_similar_precedents(query_text, precedents, embeddings, top_k=3):
    if not embeddings or not precedents:
        return []

    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = embed_text(query_text, task_type="search_query")
    if query_embedding is None:
        return []
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (NumPy ì‚¬ìš©)
    # Googleì˜ text-embedding-004ëŠ” ì •ê·œí™”ëœ ë²¡í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ë‚´ì (Dot product)ì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì„.
    embeddings_np = np.array(embeddings)
    query_embedding_np = np.array(query_embedding)
    
    similarities = np.dot(embeddings_np, query_embedding_np)
    
    # ìƒìœ„ Kê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
    top_k_indices = np.argsort(similarities)[::-1][:top_k]
    
    # ê²°ê³¼ ë°˜í™˜ (ë³´ê³ ì„œ ì‚½ì…ìš©)
    results = []
    for idx in top_k_indices:
        # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì œì™¸ (ì„ê³„ê°’ 0.6 ì„¤ì •)
        if similarities[idx] > 0.6: 
            results.append(f"[ìœ ì‚¬ íŒë¡€ ë°œê²¬ (ìœ ì‚¬ë„: {similarities[idx]:.2f})]\n{precedents[idx]}\n---\n")
    
    return results
# ------------------------------------------------------------


# ëª¨ë¸ ì„¤ì •: 'í”„ë¼ì„ ê²Œë†ˆ' ì£¼ì… (EPE/KB)
# ì¤‘ìš”: ì—¬ê¸°ì— ë„¤ë†ˆì˜ 'í”„ë¼ì„ ê²Œë†ˆ' ì „ë¬¸ì„ ë„£ì–´ì•¼ í•œë‹¤. (3ë‹¨ê³„ì˜ RAG ì§€ì¹¨ì´ í¬í•¨ëœ ë²„ì „ìœ¼ë¡œ!)
SYSTEM_INSTRUCTION = """
(ì—¬ê¸°ì— í”„ë¼ì„ ê²Œë†ˆ ì „ë¬¸ ì‚½ì… - 3ë‹¨ê³„ì˜ RAG ì§€ì¹¨ í¬í•¨ í™•ì¸!)
"""

# ëª¨ë¸ ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë“œ
if "model" not in st.session_state:
    try:
        # 1. ëª¨ë¸ ë¡œë“œ
        st.session_state.model = genai.GenerativeModel('models/gemini-2.5-flash',
                                                       system_instruction=SYSTEM_INSTRUCTION)
        
        # 2. [íŠ¸ë¡œì´ ëª©ë§ˆ] íŒë¡€ ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”© (ìºì‹œ ì‚¬ìš©)
        # ì•± ì‹œì‘ ì‹œ ìµœì´ˆ 1íšŒ ì‹¤í–‰ë¨ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        with st.spinner("íŒë¡€ ë¶„ì„ ì—”ì§„(RAG) ì´ˆê¸°í™” ì¤‘... ë°ì´í„° ì„ë² ë”© ì‹¤í–‰... (ìµœì´ˆ ì‹¤í–‰ ì‹œ)"):
            p, e = load_and_embed_precedents()
            st.session_state.precedents = p
            st.session_state.embeddings = e
            if not p:
                st.warning("ê²½ê³ : íŒë¡€ ë°ì´í„°(precedents_data.txt)ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ë°ì´í„° ì„ë² ë”© ì˜¤ë¥˜): {e}")
        st.stop()

# --- 4. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ë° ìë™ ì‹œì‘ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat" not in st.session_state:
    if "model" in st.session_state:
        try:
            st.session_state.chat = st.session_state.model.start_chat(history=[])
            
            # ì‹œìŠ¤í…œ ì´ˆê¸° ë©”ì‹œì§€(Phase 0) ê°•ì œ ìƒì„±
            initial_prompt = "ì‹œìŠ¤í…œ ê°€ë™. 'ë™ì  ë¼ìš°íŒ… í”„ë¡œí† ì½œ'ì„ ì‹¤í–‰í•˜ì—¬ Phase 0ë¥¼ ì‹œì‘í•˜ë¼."
            response = st.session_state.chat.send_message(initial_prompt)
            if response and response.text:
                st.session_state.messages.append({"role": "Architect", "content": response.text})

        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ (API í†µì‹  ì˜¤ë¥˜): {e}")
    else:
        st.error("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: ì—”ì§„ ì½”ì–´ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    role_name = message["role"]
    avatar = "ğŸ›¡ï¸"
    if role_name == "user":
        role_name = "Client"
        avatar = "ğŸ‘¤"
        
    with st.chat_message(role_name, avatar=avatar):
        st.markdown(message["content"])

# --- 5. ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ìƒì„± (RAG í†µí•©) ---
if prompt := st.chat_input("ì‹œë®¬ë ˆì´ì…˜ ë³€ìˆ˜ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤."):
    if "chat" not in st.session_state:
        st.error("ì˜¤ë¥˜: ì‹œë®¬ë ˆì´ì…˜ ì„¸ì…˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

    # ì‚¬ìš©ì ì…ë ¥ í‘œì‹œ ë° ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("Client", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    # [íŠ¸ë¡œì´ ëª©ë§ˆ] RAG ì‹¤í–‰ ë° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    rag_context = ""
    # íŒë¡€ ë°ì´í„°ê°€ ë¡œë“œë˜ì–´ ìˆë‹¤ë©´ RAG ì‹¤í–‰
    if ("precedents" in st.session_state and st.session_state.precedents):
            with st.spinner("ì‹¤ì‹œê°„ íŒë¡€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘... ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰(RAG)..."):
                # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰
                similar_precedents = find_similar_precedents(prompt, 
                                                                st.session_state.precedents, 
                                                                st.session_state.embeddings)
                if similar_precedents:
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‹œìŠ¤í…œì´ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
                    rag_context = "\n\n[ì‹œìŠ¤í…œ ì°¸ì¡°: ê²€ìƒ‰ëœ ìœ ì‚¬ íŒë¡€ ë°ì´í„°]\n" + "\n".join(similar_precedents)

    # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‚¬ìš©ì ì…ë ¥ + RAG ì»¨í…ìŠ¤íŠ¸)
    # ì‹œìŠ¤í…œì€ ì‚¬ìš©ì ì…ë ¥ê³¼ ê²€ìƒ‰ëœ íŒë¡€ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•¨
    final_prompt = f"{prompt}\n{rag_context}"

    # ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± (API í˜¸ì¶œ)
    with st.spinner("Architect ì‹œìŠ¤í…œ ì—°ì‚° ì¤‘... ë³€ìˆ˜ ë¶„ì„ ë° ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰..."):
        try:
            # ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© (final_prompt ì‚¬ìš©)
            response_stream = st.session_state.chat.send_message(final_prompt, stream=True)
            
            # ì‹œìŠ¤í…œ ì‘ë‹µ í‘œì‹œ ë° ì €ì¥
            with st.chat_message("Architect", avatar="ğŸ›¡ï¸"):
                response_placeholder = st.empty()
                full_response = ""
                for chunk in response_stream:
                    if hasattr(chunk, 'text') and chunk.text:
                        full_response += chunk.text
                        response_placeholder.markdown(full_response + "â–Œ")
                response_placeholder.markdown(full_response)

            st.session_state.messages.append({"role": "Architect", "content": full_response})
        
        except Exception as e:
            error_msg = f"ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜ ë°œìƒ. ì‹œìŠ¤í…œ ë¡œê·¸ í™•ì¸ í•„ìš”: {e}"
            st.error(error_msg)
            st.session_state.messages.append({"role": "Architect", "content": error_msg})
